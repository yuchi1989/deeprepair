python3 cifar100_repair_bias.py --net_type resnet --dataset cifar100 --depth 50 --batch_size 256 --lr 0.1 --expname cifar100_resnet_bias --epochs 60 --beta 1.0 --cutmix_prob 0 --pretrained ./runs/cifar100_resnet_2/model_best.pth.tar --expid 0 --first 35 --second 0 --third 98 --lam 0
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
=> creating model 'resnet'
True
=> loading checkpoint './runs/cifar100_resnet_2/model_best.pth.tar'
=> loaded checkpoint './runs/cifar100_resnet_2/model_best.pth.tar'
the number of model parameters: 521716
* Epoch: [0/60]	 Top 1-err 10.487  Top 5-err 0.580	 Train Loss 0.322
* Epoch: [0/60]	 Top 1-err 37.360  Top 5-err 11.720	 Test Loss 1.718
* Epoch: [0/60]	 Top 1-err 37.360  Top 5-err 11.720	 Test Loss 1.718
62.64
0.6264
loss: 1.7178067726135253
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
* Epoch: [1/60]	 Top 1-err 12.926  Top 5-err 0.865	 Train Loss 0.395
* Epoch: [1/60]	 Top 1-err 38.060  Top 5-err 12.450	 Test Loss 1.770
* Epoch: [1/60]	 Top 1-err 38.060  Top 5-err 12.450	 Test Loss 1.770
61.94
0.6194
loss: 1.7696700563430787
(35, 0, 98) triplet: 0.115
(35, 0): 0.0
(35, 98): 0.115
* Epoch: [2/60]	 Top 1-err 12.626  Top 5-err 0.950	 Train Loss 0.391
* Epoch: [2/60]	 Top 1-err 39.350  Top 5-err 12.460	 Test Loss 1.854
* Epoch: [2/60]	 Top 1-err 39.350  Top 5-err 12.460	 Test Loss 1.854
60.65
0.6065
loss: 1.8540788040161134
(35, 0, 98) triplet: 0.325
(35, 0): 0.0
(35, 98): 0.325
* Epoch: [3/60]	 Top 1-err 12.376  Top 5-err 0.890	 Train Loss 0.386
* Epoch: [3/60]	 Top 1-err 38.770  Top 5-err 13.710	 Test Loss 1.917
* Epoch: [3/60]	 Top 1-err 38.770  Top 5-err 13.710	 Test Loss 1.917
61.23
0.6123
loss: 1.9173201606750487
(35, 0, 98) triplet: 0.22
(35, 0): 0.0
(35, 98): 0.22
* Epoch: [4/60]	 Top 1-err 12.612  Top 5-err 0.878	 Train Loss 0.387
* Epoch: [4/60]	 Top 1-err 39.800  Top 5-err 13.720	 Test Loss 1.940
* Epoch: [4/60]	 Top 1-err 39.800  Top 5-err 13.720	 Test Loss 1.940
60.2
0.602
loss: 1.9398508762359619
(35, 0, 98) triplet: 0.23500000000000001
(35, 0): 0.0
(35, 98): 0.23500000000000001
* Epoch: [5/60]	 Top 1-err 12.631  Top 5-err 0.852	 Train Loss 0.388
* Epoch: [5/60]	 Top 1-err 36.060  Top 5-err 12.090	 Test Loss 1.627
* Epoch: [5/60]	 Top 1-err 36.060  Top 5-err 12.090	 Test Loss 1.627
63.94
0.6394
loss: 1.6265457412719726
(35, 0, 98) triplet: 0.115
(35, 0): 0.0
(35, 98): 0.115
* Epoch: [6/60]	 Top 1-err 12.140  Top 5-err 0.929	 Train Loss 0.377
* Epoch: [6/60]	 Top 1-err 37.100  Top 5-err 11.230	 Test Loss 1.704
* Epoch: [6/60]	 Top 1-err 37.100  Top 5-err 11.230	 Test Loss 1.704
62.9
0.629
loss: 1.7039454233169555
(35, 0, 98) triplet: 0.175
(35, 0): 0.0
(35, 98): 0.175
* Epoch: [7/60]	 Top 1-err 12.212  Top 5-err 0.901	 Train Loss 0.379
* Epoch: [7/60]	 Top 1-err 37.160  Top 5-err 11.910	 Test Loss 1.672
* Epoch: [7/60]	 Top 1-err 37.160  Top 5-err 11.910	 Test Loss 1.672
62.84
0.6284
loss: 1.6724099813461304
(35, 0, 98) triplet: 0.21
(35, 0): 0.0
(35, 98): 0.21
* Epoch: [8/60]	 Top 1-err 12.221  Top 5-err 0.808	 Train Loss 0.380
* Epoch: [8/60]	 Top 1-err 38.250  Top 5-err 12.150	 Test Loss 1.893
* Epoch: [8/60]	 Top 1-err 38.250  Top 5-err 12.150	 Test Loss 1.893
61.75
0.6175
loss: 1.89301580657959
(35, 0, 98) triplet: 0.235
(35, 0): 0.005
(35, 98): 0.24
* Epoch: [9/60]	 Top 1-err 12.187  Top 5-err 0.788	 Train Loss 0.382
* Epoch: [9/60]	 Top 1-err 37.160  Top 5-err 12.430	 Test Loss 1.806
* Epoch: [9/60]	 Top 1-err 37.160  Top 5-err 12.430	 Test Loss 1.806
62.84
0.6284
loss: 1.8057672746658324
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [10/60]	 Top 1-err 11.796  Top 5-err 0.784	 Train Loss 0.367
* Epoch: [10/60]	 Top 1-err 35.900  Top 5-err 11.700	 Test Loss 1.683
* Epoch: [10/60]	 Top 1-err 35.900  Top 5-err 11.700	 Test Loss 1.683
64.1
0.641
loss: 1.6832355693817138
(35, 0, 98) triplet: 0.20500000000000002
(35, 0): 0.0
(35, 98): 0.20500000000000002
* Epoch: [11/60]	 Top 1-err 12.440  Top 5-err 0.912	 Train Loss 0.388
* Epoch: [11/60]	 Top 1-err 38.120  Top 5-err 12.440	 Test Loss 1.814
* Epoch: [11/60]	 Top 1-err 38.120  Top 5-err 12.440	 Test Loss 1.814
61.88
0.6188
loss: 1.8136935234069824
(35, 0, 98) triplet: 0.165
(35, 0): 0.0
(35, 98): 0.165
* Epoch: [12/60]	 Top 1-err 12.284  Top 5-err 0.824	 Train Loss 0.377
* Epoch: [12/60]	 Top 1-err 37.850  Top 5-err 11.950	 Test Loss 1.824
* Epoch: [12/60]	 Top 1-err 37.850  Top 5-err 11.950	 Test Loss 1.824
62.15
0.6215
loss: 1.823589014816284
(35, 0, 98) triplet: 0.25
(35, 0): 0.0
(35, 98): 0.25
* Epoch: [13/60]	 Top 1-err 12.255  Top 5-err 0.839	 Train Loss 0.383
* Epoch: [13/60]	 Top 1-err 36.460  Top 5-err 11.580	 Test Loss 1.721
* Epoch: [13/60]	 Top 1-err 36.460  Top 5-err 11.580	 Test Loss 1.721
63.54
0.6354
loss: 1.720647021484375
(35, 0, 98) triplet: 0.25
(35, 0): 0.0
(35, 98): 0.25
not enough sample
* Epoch: [14/60]	 Top 1-err 12.323  Top 5-err 0.890	 Train Loss 0.379
* Epoch: [14/60]	 Top 1-err 38.890  Top 5-err 12.870	 Test Loss 1.897
* Epoch: [14/60]	 Top 1-err 38.890  Top 5-err 12.870	 Test Loss 1.897
61.11
0.6111
loss: 1.897137730693817
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [15/60]	 Top 1-err 12.068  Top 5-err 0.820	 Train Loss 0.375
* Epoch: [15/60]	 Top 1-err 38.470  Top 5-err 12.700	 Test Loss 1.792
* Epoch: [15/60]	 Top 1-err 38.470  Top 5-err 12.700	 Test Loss 1.792
61.53
0.6153
loss: 1.7918343856811523
(35, 0, 98) triplet: 0.24
(35, 0): 0.0
(35, 98): 0.24
* Epoch: [16/60]	 Top 1-err 11.619  Top 5-err 0.763	 Train Loss 0.365
* Epoch: [16/60]	 Top 1-err 37.780  Top 5-err 11.490	 Test Loss 1.751
* Epoch: [16/60]	 Top 1-err 37.780  Top 5-err 11.490	 Test Loss 1.751
62.22
0.6222
loss: 1.7506037406921386
(35, 0, 98) triplet: 0.2
(35, 0): 0.0
(35, 98): 0.2
* Epoch: [17/60]	 Top 1-err 11.947  Top 5-err 0.890	 Train Loss 0.373
* Epoch: [17/60]	 Top 1-err 38.170  Top 5-err 12.610	 Test Loss 1.830
* Epoch: [17/60]	 Top 1-err 38.170  Top 5-err 12.610	 Test Loss 1.830
61.83
0.6183
loss: 1.829530125427246
(35, 0, 98) triplet: 0.22
(35, 0): 0.0
(35, 98): 0.22
* Epoch: [18/60]	 Top 1-err 12.136  Top 5-err 0.856	 Train Loss 0.376
* Epoch: [18/60]	 Top 1-err 37.650  Top 5-err 12.830	 Test Loss 1.736
* Epoch: [18/60]	 Top 1-err 37.650  Top 5-err 12.830	 Test Loss 1.736
62.35
0.6235
loss: 1.7361341899871827
(35, 0, 98) triplet: 0.175
(35, 0): 0.0
(35, 98): 0.175
* Epoch: [19/60]	 Top 1-err 11.668  Top 5-err 0.759	 Train Loss 0.363
* Epoch: [19/60]	 Top 1-err 37.010  Top 5-err 12.000	 Test Loss 1.779
* Epoch: [19/60]	 Top 1-err 37.010  Top 5-err 12.000	 Test Loss 1.779
62.99
0.6299
loss: 1.7794131225585939
(35, 0, 98) triplet: 0.26
(35, 0): 0.005
(35, 98): 0.265
* Epoch: [20/60]	 Top 1-err 12.168  Top 5-err 0.837	 Train Loss 0.380
* Epoch: [20/60]	 Top 1-err 36.830  Top 5-err 11.540	 Test Loss 1.673
* Epoch: [20/60]	 Top 1-err 36.830  Top 5-err 11.540	 Test Loss 1.673
63.17
0.6317
loss: 1.672998635864258
(35, 0, 98) triplet: 0.175
(35, 0): 0.0
(35, 98): 0.175
not enough sample
* Epoch: [21/60]	 Top 1-err 11.936  Top 5-err 0.829	 Train Loss 0.374
* Epoch: [21/60]	 Top 1-err 37.740  Top 5-err 12.260	 Test Loss 1.815
* Epoch: [21/60]	 Top 1-err 37.740  Top 5-err 12.260	 Test Loss 1.815
62.26
0.6226
loss: 1.8146925010681152
(35, 0, 98) triplet: 0.22999999999999998
(35, 0): 0.0
(35, 98): 0.22999999999999998
* Epoch: [22/60]	 Top 1-err 12.034  Top 5-err 0.825	 Train Loss 0.374
* Epoch: [22/60]	 Top 1-err 36.000  Top 5-err 11.140	 Test Loss 1.723
* Epoch: [22/60]	 Top 1-err 36.000  Top 5-err 11.140	 Test Loss 1.723
64.0
0.64
loss: 1.7231857063293456
(35, 0, 98) triplet: 0.22
(35, 0): 0.0
(35, 98): 0.22
* Epoch: [23/60]	 Top 1-err 11.692  Top 5-err 0.763	 Train Loss 0.362
* Epoch: [23/60]	 Top 1-err 35.900  Top 5-err 11.220	 Test Loss 1.630
* Epoch: [23/60]	 Top 1-err 35.900  Top 5-err 11.220	 Test Loss 1.630
64.1
0.641
loss: 1.629964704322815
(35, 0, 98) triplet: 0.18
(35, 0): 0.0
(35, 98): 0.18
* Epoch: [24/60]	 Top 1-err 11.683  Top 5-err 0.723	 Train Loss 0.363
* Epoch: [24/60]	 Top 1-err 36.830  Top 5-err 12.320	 Test Loss 1.693
* Epoch: [24/60]	 Top 1-err 36.830  Top 5-err 12.320	 Test Loss 1.693
63.17
0.6317
loss: 1.6929889080047607
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [25/60]	 Top 1-err 11.798  Top 5-err 0.814	 Train Loss 0.366
* Epoch: [25/60]	 Top 1-err 37.830  Top 5-err 12.200	 Test Loss 1.845
* Epoch: [25/60]	 Top 1-err 37.830  Top 5-err 12.200	 Test Loss 1.845
62.17
0.6217
loss: 1.8452202716827393
(35, 0, 98) triplet: 0.22
(35, 0): 0.0
(35, 98): 0.22
* Epoch: [26/60]	 Top 1-err 11.445  Top 5-err 0.688	 Train Loss 0.359
* Epoch: [26/60]	 Top 1-err 35.730  Top 5-err 11.350	 Test Loss 1.586
* Epoch: [26/60]	 Top 1-err 35.730  Top 5-err 11.350	 Test Loss 1.586
64.27
0.6427
loss: 1.5861041900634765
(35, 0, 98) triplet: 0.19
(35, 0): 0.0
(35, 98): 0.19
* Epoch: [27/60]	 Top 1-err 12.057  Top 5-err 0.861	 Train Loss 0.372
* Epoch: [27/60]	 Top 1-err 39.340  Top 5-err 12.960	 Test Loss 1.959
* Epoch: [27/60]	 Top 1-err 39.340  Top 5-err 12.960	 Test Loss 1.959
60.66
0.6066
loss: 1.9588051351070404
(35, 0, 98) triplet: 0.26
(35, 0): 0.0
(35, 98): 0.26
* Epoch: [28/60]	 Top 1-err 11.742  Top 5-err 0.782	 Train Loss 0.365
* Epoch: [28/60]	 Top 1-err 35.550  Top 5-err 11.430	 Test Loss 1.637
* Epoch: [28/60]	 Top 1-err 35.550  Top 5-err 11.430	 Test Loss 1.637
64.45
0.6445
loss: 1.6367153903961182
(35, 0, 98) triplet: 0.185
(35, 0): 0.0
(35, 98): 0.185
* Epoch: [29/60]	 Top 1-err 11.815  Top 5-err 0.740	 Train Loss 0.364
* Epoch: [29/60]	 Top 1-err 36.310  Top 5-err 12.010	 Test Loss 1.708
* Epoch: [29/60]	 Top 1-err 36.310  Top 5-err 12.010	 Test Loss 1.708
63.69
0.6369
loss: 1.708214867401123
(35, 0, 98) triplet: 0.1
(35, 0): 0.0
(35, 98): 0.1
* Epoch: [30/60]	 Top 1-err 5.672  Top 5-err 0.225	 Train Loss 0.200
* Epoch: [30/60]	 Top 1-err 29.780  Top 5-err 7.970	 Test Loss 1.287
* Epoch: [30/60]	 Top 1-err 29.780  Top 5-err 7.970	 Test Loss 1.287
70.22
0.7022
loss: 1.2872528385162354
(35, 0, 98) triplet: 0.175
(35, 0): 0.0
(35, 98): 0.175
* Epoch: [31/60]	 Top 1-err 3.410  Top 5-err 0.079	 Train Loss 0.140
* Epoch: [31/60]	 Top 1-err 29.420  Top 5-err 7.980	 Test Loss 1.290
* Epoch: [31/60]	 Top 1-err 29.420  Top 5-err 7.980	 Test Loss 1.290
70.58
0.7058
loss: 1.2895241539001465
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [32/60]	 Top 1-err 2.713  Top 5-err 0.051	 Train Loss 0.120
* Epoch: [32/60]	 Top 1-err 29.130  Top 5-err 7.910	 Test Loss 1.295
* Epoch: [32/60]	 Top 1-err 29.130  Top 5-err 7.910	 Test Loss 1.295
70.87
0.7087
loss: 1.2948959129333497
(35, 0, 98) triplet: 0.125
(35, 0): 0.0
(35, 98): 0.125
* Epoch: [33/60]	 Top 1-err 2.416  Top 5-err 0.059	 Train Loss 0.111
* Epoch: [33/60]	 Top 1-err 29.160  Top 5-err 7.870	 Test Loss 1.300
* Epoch: [33/60]	 Top 1-err 29.160  Top 5-err 7.870	 Test Loss 1.300
70.84
0.7084
loss: 1.3002909436225891
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [34/60]	 Top 1-err 2.257  Top 5-err 0.036	 Train Loss 0.103
* Epoch: [34/60]	 Top 1-err 29.280  Top 5-err 7.870	 Test Loss 1.314
* Epoch: [34/60]	 Top 1-err 29.280  Top 5-err 7.870	 Test Loss 1.314
70.72
0.7072
loss: 1.3137689567565918
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [35/60]	 Top 1-err 2.021  Top 5-err 0.032	 Train Loss 0.098
* Epoch: [35/60]	 Top 1-err 28.770  Top 5-err 7.770	 Test Loss 1.315
* Epoch: [35/60]	 Top 1-err 28.770  Top 5-err 7.770	 Test Loss 1.315
71.23
0.7123
loss: 1.314504751968384
(35, 0, 98) triplet: 0.15000000000000002
(35, 0): 0.0
(35, 98): 0.15000000000000002
* Epoch: [36/60]	 Top 1-err 1.742  Top 5-err 0.030	 Train Loss 0.090
* Epoch: [36/60]	 Top 1-err 28.810  Top 5-err 7.990	 Test Loss 1.311
* Epoch: [36/60]	 Top 1-err 28.810  Top 5-err 7.990	 Test Loss 1.311
71.19
0.7119
loss: 1.3108929659366608
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
* Epoch: [37/60]	 Top 1-err 1.681  Top 5-err 0.017	 Train Loss 0.086
* Epoch: [37/60]	 Top 1-err 28.960  Top 5-err 7.970	 Test Loss 1.325
* Epoch: [37/60]	 Top 1-err 28.960  Top 5-err 7.970	 Test Loss 1.325
71.04
0.7104
loss: 1.325419229698181
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
not enough sample
* Epoch: [38/60]	 Top 1-err 1.643  Top 5-err 0.026	 Train Loss 0.086
* Epoch: [38/60]	 Top 1-err 28.670  Top 5-err 7.960	 Test Loss 1.337
* Epoch: [38/60]	 Top 1-err 28.670  Top 5-err 7.960	 Test Loss 1.337
71.33
0.7133
loss: 1.3371819192886352
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [39/60]	 Top 1-err 1.575  Top 5-err 0.017	 Train Loss 0.081
* Epoch: [39/60]	 Top 1-err 28.980  Top 5-err 7.950	 Test Loss 1.335
* Epoch: [39/60]	 Top 1-err 28.980  Top 5-err 7.950	 Test Loss 1.335
71.02
0.7102
loss: 1.334663863182068
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
not enough sample
* Epoch: [40/60]	 Top 1-err 1.481  Top 5-err 0.023	 Train Loss 0.079
* Epoch: [40/60]	 Top 1-err 29.070  Top 5-err 8.070	 Test Loss 1.342
* Epoch: [40/60]	 Top 1-err 29.070  Top 5-err 8.070	 Test Loss 1.342
70.93
0.7093
loss: 1.3420660518646241
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [41/60]	 Top 1-err 1.290  Top 5-err 0.013	 Train Loss 0.074
* Epoch: [41/60]	 Top 1-err 28.900  Top 5-err 8.120	 Test Loss 1.353
* Epoch: [41/60]	 Top 1-err 28.900  Top 5-err 8.120	 Test Loss 1.353
71.1
0.711
loss: 1.3528986081123353
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [42/60]	 Top 1-err 1.256  Top 5-err 0.013	 Train Loss 0.073
* Epoch: [42/60]	 Top 1-err 29.080  Top 5-err 8.110	 Test Loss 1.355
* Epoch: [42/60]	 Top 1-err 29.080  Top 5-err 8.110	 Test Loss 1.355
70.92
0.7092
loss: 1.3547603797912597
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [43/60]	 Top 1-err 1.162  Top 5-err 0.025	 Train Loss 0.071
* Epoch: [43/60]	 Top 1-err 29.040  Top 5-err 8.110	 Test Loss 1.366
* Epoch: [43/60]	 Top 1-err 29.040  Top 5-err 8.110	 Test Loss 1.366
70.96
0.7096
loss: 1.3661147018432618
(35, 0, 98) triplet: 0.15000000000000002
(35, 0): 0.0
(35, 98): 0.15000000000000002
* Epoch: [44/60]	 Top 1-err 1.190  Top 5-err 0.011	 Train Loss 0.070
* Epoch: [44/60]	 Top 1-err 29.110  Top 5-err 8.240	 Test Loss 1.361
* Epoch: [44/60]	 Top 1-err 29.110  Top 5-err 8.240	 Test Loss 1.361
70.89
0.7089
loss: 1.361393817138672
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [45/60]	 Top 1-err 1.058  Top 5-err 0.015	 Train Loss 0.065
* Epoch: [45/60]	 Top 1-err 28.990  Top 5-err 8.280	 Test Loss 1.366
Current best accuracy (top-1 and 5 error): 28.99 8.28
saving best model...
* Epoch: [45/60]	 Top 1-err 28.990  Top 5-err 8.280	 Test Loss 1.366
71.01
0.7101
loss: 1.366347488975525
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [46/60]	 Top 1-err 1.020  Top 5-err 0.009	 Train Loss 0.064
* Epoch: [46/60]	 Top 1-err 29.060  Top 5-err 8.130	 Test Loss 1.366
Current best accuracy (top-1 and 5 error): 28.99 8.28
* Epoch: [46/60]	 Top 1-err 29.060  Top 5-err 8.130	 Test Loss 1.366
70.94
0.7094
loss: 1.3655153121948242
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
not enough sample
* Epoch: [47/60]	 Top 1-err 0.960  Top 5-err 0.008	 Train Loss 0.064
* Epoch: [47/60]	 Top 1-err 29.120  Top 5-err 8.170	 Test Loss 1.363
Current best accuracy (top-1 and 5 error): 28.99 8.28
* Epoch: [47/60]	 Top 1-err 29.120  Top 5-err 8.170	 Test Loss 1.363
70.88
0.7088
loss: 1.3627191104888916
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [48/60]	 Top 1-err 0.909  Top 5-err 0.008	 Train Loss 0.063
* Epoch: [48/60]	 Top 1-err 28.870  Top 5-err 8.200	 Test Loss 1.362
Current best accuracy (top-1 and 5 error): 28.87 8.2
saving best model...
* Epoch: [48/60]	 Top 1-err 28.870  Top 5-err 8.200	 Test Loss 1.362
71.13
0.7113
loss: 1.36240943069458
(35, 0, 98) triplet: 0.17
(35, 0): 0.0
(35, 98): 0.17
* Epoch: [49/60]	 Top 1-err 0.935  Top 5-err 0.009	 Train Loss 0.063
* Epoch: [49/60]	 Top 1-err 29.050  Top 5-err 8.210	 Test Loss 1.365
Current best accuracy (top-1 and 5 error): 28.87 8.2
* Epoch: [49/60]	 Top 1-err 29.050  Top 5-err 8.210	 Test Loss 1.365
70.95
0.7095
loss: 1.3654362419128419
(35, 0, 98) triplet: 0.165
(35, 0): 0.0
(35, 98): 0.165
* Epoch: [50/60]	 Top 1-err 0.975  Top 5-err 0.009	 Train Loss 0.062
* Epoch: [50/60]	 Top 1-err 28.780  Top 5-err 8.220	 Test Loss 1.365
Current best accuracy (top-1 and 5 error): 28.78 8.22
saving best model...
* Epoch: [50/60]	 Top 1-err 28.780  Top 5-err 8.220	 Test Loss 1.365
71.22
0.7122
loss: 1.3648398246765137
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [51/60]	 Top 1-err 0.967  Top 5-err 0.011	 Train Loss 0.062
* Epoch: [51/60]	 Top 1-err 28.960  Top 5-err 8.090	 Test Loss 1.361
Current best accuracy (top-1 and 5 error): 28.78 8.22
* Epoch: [51/60]	 Top 1-err 28.960  Top 5-err 8.090	 Test Loss 1.361
71.04
0.7104
loss: 1.3614515171051025
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.0
(35, 98): 0.16499999999999998
* Epoch: [52/60]	 Top 1-err 0.960  Top 5-err 0.011	 Train Loss 0.061
* Epoch: [52/60]	 Top 1-err 28.820  Top 5-err 8.070	 Test Loss 1.367
Current best accuracy (top-1 and 5 error): 28.78 8.22
* Epoch: [52/60]	 Top 1-err 28.820  Top 5-err 8.070	 Test Loss 1.367
71.18
0.7118
loss: 1.3670194375038147
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.0
(35, 98): 0.16999999999999998
* Epoch: [53/60]	 Top 1-err 0.886  Top 5-err 0.002	 Train Loss 0.061
* Epoch: [53/60]	 Top 1-err 28.840  Top 5-err 8.120	 Test Loss 1.368
Current best accuracy (top-1 and 5 error): 28.78 8.22
* Epoch: [53/60]	 Top 1-err 28.840  Top 5-err 8.120	 Test Loss 1.368
71.16
0.7116
loss: 1.3681247245788575
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [54/60]	 Top 1-err 0.984  Top 5-err 0.004	 Train Loss 0.062
* Epoch: [54/60]	 Top 1-err 28.950  Top 5-err 8.100	 Test Loss 1.361
Current best accuracy (top-1 and 5 error): 28.78 8.22
* Epoch: [54/60]	 Top 1-err 28.950  Top 5-err 8.100	 Test Loss 1.361
71.05
0.7105
loss: 1.3613193578720093
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [55/60]	 Top 1-err 0.967  Top 5-err 0.009	 Train Loss 0.062
* Epoch: [55/60]	 Top 1-err 28.950  Top 5-err 8.180	 Test Loss 1.367
Current best accuracy (top-1 and 5 error): 28.78 8.22
* Epoch: [55/60]	 Top 1-err 28.950  Top 5-err 8.180	 Test Loss 1.367
71.05
0.7105
loss: 1.36665694732666
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [56/60]	 Top 1-err 0.960  Top 5-err 0.009	 Train Loss 0.061
* Epoch: [56/60]	 Top 1-err 28.940  Top 5-err 8.150	 Test Loss 1.367
Current best accuracy (top-1 and 5 error): 28.78 8.22
* Epoch: [56/60]	 Top 1-err 28.940  Top 5-err 8.150	 Test Loss 1.367
71.06
0.7106
loss: 1.3666586936950684
(35, 0, 98) triplet: 0.17
(35, 0): 0.0
(35, 98): 0.17
* Epoch: [57/60]	 Top 1-err 0.878  Top 5-err 0.013	 Train Loss 0.061
* Epoch: [57/60]	 Top 1-err 28.900  Top 5-err 8.120	 Test Loss 1.365
Current best accuracy (top-1 and 5 error): 28.78 8.22
* Epoch: [57/60]	 Top 1-err 28.900  Top 5-err 8.120	 Test Loss 1.365
71.1
0.711
loss: 1.3652049701690674
(35, 0, 98) triplet: 0.165
(35, 0): 0.0
(35, 98): 0.165
not enough sample
* Epoch: [58/60]	 Top 1-err 0.914  Top 5-err 0.013	 Train Loss 0.061
* Epoch: [58/60]	 Top 1-err 29.000  Top 5-err 8.150	 Test Loss 1.370
Current best accuracy (top-1 and 5 error): 28.78 8.22
* Epoch: [58/60]	 Top 1-err 29.000  Top 5-err 8.150	 Test Loss 1.370
71.0
0.71
loss: 1.3698989654541016
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.0
(35, 98): 0.16999999999999998
* Epoch: [59/60]	 Top 1-err 0.909  Top 5-err 0.004	 Train Loss 0.060
* Epoch: [59/60]	 Top 1-err 28.920  Top 5-err 8.170	 Test Loss 1.367
Current best accuracy (top-1 and 5 error): 28.78 8.22
* Epoch: [59/60]	 Top 1-err 28.920  Top 5-err 8.170	 Test Loss 1.367
71.08
0.7108
loss: 1.3670729167938231
(35, 0, 98) triplet: 0.165
(35, 0): 0.0
(35, 98): 0.165
Best accuracy (top-1 and 5 error): 28.78 8.22
=> loading checkpoint 'runs/cifar100_resnet_bias/model_best.pth.tar'
* Epoch: [-1/60]	 Top 1-err 28.780  Top 5-err 8.220	 Test Loss 1.365
71.22
0.7122
loss: 1.3648398245811462
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
python3 cifar100_repair_bias.py --net_type resnet --dataset cifar100 --depth 50 --batch_size 256 --lr 0.1 --expname cifar100_resnet_bias --epochs 60 --beta 1.0 --cutmix_prob 0 --pretrained ./runs/cifar100_resnet_2/model_best.pth.tar --expid 0 --first 35 --second 0 --third 98 --lam 0.5
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
=> creating model 'resnet'
True
=> loading checkpoint './runs/cifar100_resnet_2/model_best.pth.tar'
=> loaded checkpoint './runs/cifar100_resnet_2/model_best.pth.tar'
the number of model parameters: 521716
* Epoch: [0/60]	 Top 1-err 25.831  Top 5-err 4.360	 Train Loss 0.964
* Epoch: [0/60]	 Top 1-err 42.050  Top 5-err 14.830	 Test Loss 1.787
* Epoch: [0/60]	 Top 1-err 42.050  Top 5-err 14.830	 Test Loss 1.787
57.95
0.5795
loss: 1.7874274391174316
(35, 0, 98) triplet: 0.255
(35, 0): 0.005
(35, 98): 0.26
* Epoch: [1/60]	 Top 1-err 23.330  Top 5-err 3.411	 Train Loss 0.838
* Epoch: [1/60]	 Top 1-err 45.280  Top 5-err 16.750	 Test Loss 2.030
* Epoch: [1/60]	 Top 1-err 45.280  Top 5-err 16.750	 Test Loss 2.030
54.72
0.5472
loss: 2.0304635932922364
(35, 0, 98) triplet: 0.335
(35, 0): 0.0
(35, 98): 0.335
* Epoch: [2/60]	 Top 1-err 22.422  Top 5-err 3.189	 Train Loss 0.804
* Epoch: [2/60]	 Top 1-err 42.890  Top 5-err 15.010	 Test Loss 1.862
* Epoch: [2/60]	 Top 1-err 42.890  Top 5-err 15.010	 Test Loss 1.862
57.11
0.5711
loss: 1.8623763835906983
(35, 0, 98) triplet: 0.32
(35, 0): 0.005
(35, 98): 0.325
* Epoch: [3/60]	 Top 1-err 21.662  Top 5-err 2.830	 Train Loss 0.764
* Epoch: [3/60]	 Top 1-err 41.620  Top 5-err 14.280	 Test Loss 1.847
* Epoch: [3/60]	 Top 1-err 41.620  Top 5-err 14.280	 Test Loss 1.847
58.38
0.5838
loss: 1.8468338886260987
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [4/60]	 Top 1-err 20.931  Top 5-err 2.641	 Train Loss 0.746
* Epoch: [4/60]	 Top 1-err 40.880  Top 5-err 14.030	 Test Loss 1.788
* Epoch: [4/60]	 Top 1-err 40.880  Top 5-err 14.030	 Test Loss 1.788
59.12
0.5912
loss: 1.7879937358856202
(35, 0, 98) triplet: 0.275
(35, 0): 0.015
(35, 98): 0.29000000000000004
* Epoch: [5/60]	 Top 1-err 20.822  Top 5-err 2.648	 Train Loss 0.740
* Epoch: [5/60]	 Top 1-err 39.440  Top 5-err 13.570	 Test Loss 1.616
* Epoch: [5/60]	 Top 1-err 39.440  Top 5-err 13.570	 Test Loss 1.616
60.56
0.6056
loss: 1.6155229927062988
(35, 0, 98) triplet: 0.11499999999999999
(35, 0): 0.005
(35, 98): 0.12
* Epoch: [6/60]	 Top 1-err 19.949  Top 5-err 2.526	 Train Loss 0.704
* Epoch: [6/60]	 Top 1-err 39.140  Top 5-err 11.930	 Test Loss 1.622
* Epoch: [6/60]	 Top 1-err 39.140  Top 5-err 11.930	 Test Loss 1.622
60.86
0.6086
loss: 1.6223644752502442
(35, 0, 98) triplet: 0.185
(35, 0): 0.0
(35, 98): 0.185
* Epoch: [7/60]	 Top 1-err 19.399  Top 5-err 2.278	 Train Loss 0.688
* Epoch: [7/60]	 Top 1-err 39.000  Top 5-err 12.610	 Test Loss 1.685
* Epoch: [7/60]	 Top 1-err 39.000  Top 5-err 12.610	 Test Loss 1.685
61.0
0.61
loss: 1.684723033809662
(35, 0, 98) triplet: 0.33
(35, 0): 0.0
(35, 98): 0.33
* Epoch: [8/60]	 Top 1-err 19.146  Top 5-err 2.295	 Train Loss 0.678
* Epoch: [8/60]	 Top 1-err 38.700  Top 5-err 12.460	 Test Loss 1.634
* Epoch: [8/60]	 Top 1-err 38.700  Top 5-err 12.460	 Test Loss 1.634
61.3
0.613
loss: 1.6338381996154785
(35, 0, 98) triplet: 0.22999999999999998
(35, 0): 0.005
(35, 98): 0.235
* Epoch: [9/60]	 Top 1-err 18.950  Top 5-err 2.144	 Train Loss 0.670
* Epoch: [9/60]	 Top 1-err 45.250  Top 5-err 17.480	 Test Loss 1.967
* Epoch: [9/60]	 Top 1-err 45.250  Top 5-err 17.480	 Test Loss 1.967
54.75
0.5475
loss: 1.967117206954956
(35, 0, 98) triplet: 0.27
(35, 0): 0.01
(35, 98): 0.28
* Epoch: [10/60]	 Top 1-err 19.618  Top 5-err 2.427	 Train Loss 0.692
* Epoch: [10/60]	 Top 1-err 39.360  Top 5-err 13.460	 Test Loss 1.679
* Epoch: [10/60]	 Top 1-err 39.360  Top 5-err 13.460	 Test Loss 1.679
60.64
0.6064
loss: 1.6786271213531494
(35, 0, 98) triplet: 0.185
(35, 0): 0.01
(35, 98): 0.195
* Epoch: [11/60]	 Top 1-err 18.226  Top 5-err 2.112	 Train Loss 0.645
* Epoch: [11/60]	 Top 1-err 37.070  Top 5-err 12.080	 Test Loss 1.636
* Epoch: [11/60]	 Top 1-err 37.070  Top 5-err 12.080	 Test Loss 1.636
62.93
0.6293
loss: 1.6361756631851196
(35, 0, 98) triplet: 0.145
(35, 0): 0.005
(35, 98): 0.15
* Epoch: [12/60]	 Top 1-err 17.669  Top 5-err 1.900	 Train Loss 0.619
* Epoch: [12/60]	 Top 1-err 38.550  Top 5-err 12.120	 Test Loss 1.628
* Epoch: [12/60]	 Top 1-err 38.550  Top 5-err 12.120	 Test Loss 1.628
61.45
0.6145
loss: 1.6277010013580322
(35, 0, 98) triplet: 0.235
(35, 0): 0.01
(35, 98): 0.245
* Epoch: [13/60]	 Top 1-err 18.013  Top 5-err 1.997	 Train Loss 0.633
* Epoch: [13/60]	 Top 1-err 41.020  Top 5-err 13.480	 Test Loss 1.803
* Epoch: [13/60]	 Top 1-err 41.020  Top 5-err 13.480	 Test Loss 1.803
58.98
0.5898
loss: 1.8029086891174317
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
not enough sample
* Epoch: [14/60]	 Top 1-err 18.427  Top 5-err 2.099	 Train Loss 0.656
* Epoch: [14/60]	 Top 1-err 37.780  Top 5-err 12.590	 Test Loss 1.688
* Epoch: [14/60]	 Top 1-err 37.780  Top 5-err 12.590	 Test Loss 1.688
62.22
0.6222
loss: 1.6877649737358094
(35, 0, 98) triplet: 0.13
(35, 0): 0.005
(35, 98): 0.135
* Epoch: [15/60]	 Top 1-err 20.416  Top 5-err 2.637	 Train Loss 0.728
* Epoch: [15/60]	 Top 1-err 37.830  Top 5-err 12.050	 Test Loss 1.596
* Epoch: [15/60]	 Top 1-err 37.830  Top 5-err 12.050	 Test Loss 1.596
62.17
0.6217
loss: 1.5959159717559814
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.015
(35, 98): 0.18
* Epoch: [16/60]	 Top 1-err 18.442  Top 5-err 2.076	 Train Loss 0.661
* Epoch: [16/60]	 Top 1-err 39.770  Top 5-err 13.440	 Test Loss 1.735
* Epoch: [16/60]	 Top 1-err 39.770  Top 5-err 13.440	 Test Loss 1.735
60.23
0.6023
loss: 1.7352240631103515
(35, 0, 98) triplet: 0.28
(35, 0): 0.005
(35, 98): 0.28500000000000003
* Epoch: [17/60]	 Top 1-err 18.748  Top 5-err 2.236	 Train Loss 0.663
* Epoch: [17/60]	 Top 1-err 38.710  Top 5-err 12.650	 Test Loss 1.622
* Epoch: [17/60]	 Top 1-err 38.710  Top 5-err 12.650	 Test Loss 1.622
61.29
0.6129
loss: 1.6215836265563965
(35, 0, 98) triplet: 0.22999999999999998
(35, 0): 0.0
(35, 98): 0.22999999999999998
* Epoch: [18/60]	 Top 1-err 18.432  Top 5-err 2.002	 Train Loss 0.642
* Epoch: [18/60]	 Top 1-err 42.030  Top 5-err 15.230	 Test Loss 1.869
* Epoch: [18/60]	 Top 1-err 42.030  Top 5-err 15.230	 Test Loss 1.869
57.97
0.5797
loss: 1.8689007587432862
(35, 0, 98) triplet: 0.225
(35, 0): 0.005
(35, 98): 0.23
* Epoch: [19/60]	 Top 1-err 18.054  Top 5-err 2.065	 Train Loss 0.643
* Epoch: [19/60]	 Top 1-err 35.950  Top 5-err 11.330	 Test Loss 1.508
* Epoch: [19/60]	 Top 1-err 35.950  Top 5-err 11.330	 Test Loss 1.508
64.05
0.6405
loss: 1.5083493602752684
(35, 0, 98) triplet: 0.165
(35, 0): 0.0
(35, 98): 0.165
* Epoch: [20/60]	 Top 1-err 17.818  Top 5-err 2.019	 Train Loss 0.626
* Epoch: [20/60]	 Top 1-err 36.790  Top 5-err 11.630	 Test Loss 1.516
* Epoch: [20/60]	 Top 1-err 36.790  Top 5-err 11.630	 Test Loss 1.516
63.21
0.6321
loss: 1.515553727531433
(35, 0, 98) triplet: 0.235
(35, 0): 0.0
(35, 98): 0.235
not enough sample
* Epoch: [21/60]	 Top 1-err 17.002  Top 5-err 1.789	 Train Loss 0.605
* Epoch: [21/60]	 Top 1-err 36.740  Top 5-err 11.540	 Test Loss 1.580
* Epoch: [21/60]	 Top 1-err 36.740  Top 5-err 11.540	 Test Loss 1.580
63.26
0.6326
loss: 1.5801317081451416
(35, 0, 98) triplet: 0.12
(35, 0): 0.0
(35, 98): 0.12
* Epoch: [22/60]	 Top 1-err 16.755  Top 5-err 1.711	 Train Loss 0.587
* Epoch: [22/60]	 Top 1-err 37.300  Top 5-err 12.010	 Test Loss 1.603
* Epoch: [22/60]	 Top 1-err 37.300  Top 5-err 12.010	 Test Loss 1.603
62.7
0.627
loss: 1.6033717102050782
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [23/60]	 Top 1-err 17.533  Top 5-err 1.832	 Train Loss 0.611
* Epoch: [23/60]	 Top 1-err 38.200  Top 5-err 12.000	 Test Loss 1.658
* Epoch: [23/60]	 Top 1-err 38.200  Top 5-err 12.000	 Test Loss 1.658
61.8
0.618
loss: 1.6584913583755494
(35, 0, 98) triplet: 0.25
(35, 0): 0.01
(35, 98): 0.26
* Epoch: [24/60]	 Top 1-err 16.921  Top 5-err 1.762	 Train Loss 0.593
* Epoch: [24/60]	 Top 1-err 36.070  Top 5-err 11.270	 Test Loss 1.550
* Epoch: [24/60]	 Top 1-err 36.070  Top 5-err 11.270	 Test Loss 1.550
63.93
0.6393
loss: 1.549656611061096
(35, 0, 98) triplet: 0.13
(35, 0): 0.0
(35, 98): 0.13
* Epoch: [25/60]	 Top 1-err 16.147  Top 5-err 1.572	 Train Loss 0.566
* Epoch: [25/60]	 Top 1-err 38.000  Top 5-err 12.220	 Test Loss 1.655
* Epoch: [25/60]	 Top 1-err 38.000  Top 5-err 12.220	 Test Loss 1.655
62.0
0.62
loss: 1.6548345865249634
(35, 0, 98) triplet: 0.195
(35, 0): 0.0
(35, 98): 0.195
* Epoch: [26/60]	 Top 1-err 16.375  Top 5-err 1.736	 Train Loss 0.579
* Epoch: [26/60]	 Top 1-err 36.800  Top 5-err 12.060	 Test Loss 1.599
* Epoch: [26/60]	 Top 1-err 36.800  Top 5-err 12.060	 Test Loss 1.599
63.2
0.632
loss: 1.5989039733886719
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [27/60]	 Top 1-err 16.549  Top 5-err 1.657	 Train Loss 0.570
* Epoch: [27/60]	 Top 1-err 37.170  Top 5-err 12.090	 Test Loss 1.666
* Epoch: [27/60]	 Top 1-err 37.170  Top 5-err 12.090	 Test Loss 1.666
62.83
0.6283
loss: 1.6664313919067384
(35, 0, 98) triplet: 0.22499999999999998
(35, 0): 0.0
(35, 98): 0.22499999999999998
* Epoch: [28/60]	 Top 1-err 17.917  Top 5-err 2.082	 Train Loss 0.633
* Epoch: [28/60]	 Top 1-err 42.660  Top 5-err 15.090	 Test Loss 2.083
* Epoch: [28/60]	 Top 1-err 42.660  Top 5-err 15.090	 Test Loss 2.083
57.34
0.5734
loss: 2.083119962501526
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
* Epoch: [29/60]	 Top 1-err 18.646  Top 5-err 2.161	 Train Loss 0.650
* Epoch: [29/60]	 Top 1-err 35.880  Top 5-err 11.420	 Test Loss 1.510
* Epoch: [29/60]	 Top 1-err 35.880  Top 5-err 11.420	 Test Loss 1.510
64.12
0.6412
loss: 1.51012449798584
(35, 0, 98) triplet: 0.13
(35, 0): 0.0
(35, 98): 0.13
* Epoch: [30/60]	 Top 1-err 10.221  Top 5-err 0.693	 Train Loss 0.371
* Epoch: [30/60]	 Top 1-err 29.810  Top 5-err 8.210	 Test Loss 1.198
* Epoch: [30/60]	 Top 1-err 29.810  Top 5-err 8.210	 Test Loss 1.198
70.19
0.7019
loss: 1.1975466094970704
(35, 0, 98) triplet: 0.13
(35, 0): 0.0
(35, 98): 0.13
* Epoch: [31/60]	 Top 1-err 7.524  Top 5-err 0.421	 Train Loss 0.287
* Epoch: [31/60]	 Top 1-err 29.320  Top 5-err 8.100	 Test Loss 1.195
* Epoch: [31/60]	 Top 1-err 29.320  Top 5-err 8.100	 Test Loss 1.195
70.68
0.7068
loss: 1.19542887840271
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [32/60]	 Top 1-err 6.732  Top 5-err 0.334	 Train Loss 0.257
* Epoch: [32/60]	 Top 1-err 29.110  Top 5-err 7.990	 Test Loss 1.203
* Epoch: [32/60]	 Top 1-err 29.110  Top 5-err 7.990	 Test Loss 1.203
70.89
0.7089
loss: 1.2033367532730102
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [33/60]	 Top 1-err 5.924  Top 5-err 0.300	 Train Loss 0.233
* Epoch: [33/60]	 Top 1-err 29.130  Top 5-err 7.910	 Test Loss 1.202
* Epoch: [33/60]	 Top 1-err 29.130  Top 5-err 7.910	 Test Loss 1.202
70.87
0.7087
loss: 1.202481782913208
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
* Epoch: [34/60]	 Top 1-err 5.484  Top 5-err 0.312	 Train Loss 0.218
* Epoch: [34/60]	 Top 1-err 29.160  Top 5-err 7.910	 Test Loss 1.213
* Epoch: [34/60]	 Top 1-err 29.160  Top 5-err 7.910	 Test Loss 1.213
70.84
0.7084
loss: 1.2134072076797486
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [35/60]	 Top 1-err 5.183  Top 5-err 0.242	 Train Loss 0.205
* Epoch: [35/60]	 Top 1-err 28.700  Top 5-err 7.910	 Test Loss 1.213
* Epoch: [35/60]	 Top 1-err 28.700  Top 5-err 7.910	 Test Loss 1.213
71.3
0.713
loss: 1.2133371208190917
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
* Epoch: [36/60]	 Top 1-err 4.826  Top 5-err 0.232	 Train Loss 0.195
* Epoch: [36/60]	 Top 1-err 28.700  Top 5-err 7.890	 Test Loss 1.215
* Epoch: [36/60]	 Top 1-err 28.700  Top 5-err 7.890	 Test Loss 1.215
71.3
0.713
loss: 1.2147910157203674
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [37/60]	 Top 1-err 4.539  Top 5-err 0.159	 Train Loss 0.183
* Epoch: [37/60]	 Top 1-err 28.740  Top 5-err 8.050	 Test Loss 1.233
* Epoch: [37/60]	 Top 1-err 28.740  Top 5-err 8.050	 Test Loss 1.233
71.26
0.7126
loss: 1.2325534780502319
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
not enough sample
* Epoch: [38/60]	 Top 1-err 4.494  Top 5-err 0.189	 Train Loss 0.179
* Epoch: [38/60]	 Top 1-err 28.890  Top 5-err 8.040	 Test Loss 1.235
* Epoch: [38/60]	 Top 1-err 28.890  Top 5-err 8.040	 Test Loss 1.235
71.11
0.7111
loss: 1.234861496925354
(35, 0, 98) triplet: 0.15000000000000002
(35, 0): 0.0
(35, 98): 0.15000000000000002
* Epoch: [39/60]	 Top 1-err 4.154  Top 5-err 0.147	 Train Loss 0.171
* Epoch: [39/60]	 Top 1-err 28.850  Top 5-err 7.970	 Test Loss 1.243
* Epoch: [39/60]	 Top 1-err 28.850  Top 5-err 7.970	 Test Loss 1.243
71.15
0.7115
loss: 1.2427139610290527
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
not enough sample
* Epoch: [40/60]	 Top 1-err 4.144  Top 5-err 0.155	 Train Loss 0.166
* Epoch: [40/60]	 Top 1-err 29.010  Top 5-err 8.010	 Test Loss 1.255
* Epoch: [40/60]	 Top 1-err 29.010  Top 5-err 8.010	 Test Loss 1.255
70.99
0.7099
loss: 1.2547710107803345
(35, 0, 98) triplet: 0.125
(35, 0): 0.0
(35, 98): 0.125
* Epoch: [41/60]	 Top 1-err 3.861  Top 5-err 0.128	 Train Loss 0.159
* Epoch: [41/60]	 Top 1-err 28.910  Top 5-err 8.080	 Test Loss 1.265
* Epoch: [41/60]	 Top 1-err 28.910  Top 5-err 8.080	 Test Loss 1.265
71.09
0.7109
loss: 1.2648020793914796
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
* Epoch: [42/60]	 Top 1-err 3.697  Top 5-err 0.117	 Train Loss 0.153
* Epoch: [42/60]	 Top 1-err 28.780  Top 5-err 7.880	 Test Loss 1.260
* Epoch: [42/60]	 Top 1-err 28.780  Top 5-err 7.880	 Test Loss 1.260
71.22
0.7122
loss: 1.2602301770925521
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
* Epoch: [43/60]	 Top 1-err 3.513  Top 5-err 0.134	 Train Loss 0.149
* Epoch: [43/60]	 Top 1-err 29.020  Top 5-err 8.230	 Test Loss 1.270
* Epoch: [43/60]	 Top 1-err 29.020  Top 5-err 8.230	 Test Loss 1.270
70.98
0.7098
loss: 1.2696173240661621
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [44/60]	 Top 1-err 3.538  Top 5-err 0.096	 Train Loss 0.146
* Epoch: [44/60]	 Top 1-err 29.130  Top 5-err 7.950	 Test Loss 1.272
* Epoch: [44/60]	 Top 1-err 29.130  Top 5-err 7.950	 Test Loss 1.272
70.87
0.7087
loss: 1.2717379709243775
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
* Epoch: [45/60]	 Top 1-err 3.213  Top 5-err 0.094	 Train Loss 0.138
* Epoch: [45/60]	 Top 1-err 28.960  Top 5-err 8.060	 Test Loss 1.270
Current best accuracy (top-1 and 5 error): 28.96 8.06
saving best model...
* Epoch: [45/60]	 Top 1-err 28.960  Top 5-err 8.060	 Test Loss 1.270
71.04
0.7104
loss: 1.2704164827346802
(35, 0, 98) triplet: 0.125
(35, 0): 0.0
(35, 98): 0.125
* Epoch: [46/60]	 Top 1-err 3.083  Top 5-err 0.085	 Train Loss 0.137
* Epoch: [46/60]	 Top 1-err 28.880  Top 5-err 8.050	 Test Loss 1.271
Current best accuracy (top-1 and 5 error): 28.88 8.05
saving best model...
* Epoch: [46/60]	 Top 1-err 28.880  Top 5-err 8.050	 Test Loss 1.271
71.12
0.7112
loss: 1.2712711639404297
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
not enough sample
* Epoch: [47/60]	 Top 1-err 3.107  Top 5-err 0.085	 Train Loss 0.134
* Epoch: [47/60]	 Top 1-err 28.830  Top 5-err 8.020	 Test Loss 1.273
Current best accuracy (top-1 and 5 error): 28.83 8.02
saving best model...
* Epoch: [47/60]	 Top 1-err 28.830  Top 5-err 8.020	 Test Loss 1.273
71.17
0.7117
loss: 1.2730698099136353
(35, 0, 98) triplet: 0.125
(35, 0): 0.0
(35, 98): 0.125
* Epoch: [48/60]	 Top 1-err 2.986  Top 5-err 0.089	 Train Loss 0.133
* Epoch: [48/60]	 Top 1-err 28.880  Top 5-err 8.030	 Test Loss 1.268
Current best accuracy (top-1 and 5 error): 28.83 8.02
* Epoch: [48/60]	 Top 1-err 28.880  Top 5-err 8.030	 Test Loss 1.268
71.12
0.7112
loss: 1.2682101865768434
(35, 0, 98) triplet: 0.125
(35, 0): 0.0
(35, 98): 0.125
* Epoch: [49/60]	 Top 1-err 3.009  Top 5-err 0.087	 Train Loss 0.132
* Epoch: [49/60]	 Top 1-err 28.710  Top 5-err 7.950	 Test Loss 1.270
Current best accuracy (top-1 and 5 error): 28.71 7.95
saving best model...
* Epoch: [49/60]	 Top 1-err 28.710  Top 5-err 7.950	 Test Loss 1.270
71.29
0.7129
loss: 1.270285806274414
(35, 0, 98) triplet: 0.13
(35, 0): 0.0
(35, 98): 0.13
* Epoch: [50/60]	 Top 1-err 2.969  Top 5-err 0.085	 Train Loss 0.131
* Epoch: [50/60]	 Top 1-err 28.870  Top 5-err 8.060	 Test Loss 1.271
Current best accuracy (top-1 and 5 error): 28.71 7.95
* Epoch: [50/60]	 Top 1-err 28.870  Top 5-err 8.060	 Test Loss 1.271
71.13
0.7113
loss: 1.2707746604919434
(35, 0, 98) triplet: 0.12
(35, 0): 0.0
(35, 98): 0.12
* Epoch: [51/60]	 Top 1-err 3.022  Top 5-err 0.077	 Train Loss 0.132
* Epoch: [51/60]	 Top 1-err 28.880  Top 5-err 8.050	 Test Loss 1.270
Current best accuracy (top-1 and 5 error): 28.71 7.95
* Epoch: [51/60]	 Top 1-err 28.880  Top 5-err 8.050	 Test Loss 1.270
71.12
0.7112
loss: 1.2700769010543824
(35, 0, 98) triplet: 0.13
(35, 0): 0.0
(35, 98): 0.13
* Epoch: [52/60]	 Top 1-err 2.818  Top 5-err 0.076	 Train Loss 0.129
* Epoch: [52/60]	 Top 1-err 28.830  Top 5-err 7.980	 Test Loss 1.274
Current best accuracy (top-1 and 5 error): 28.71 7.95
* Epoch: [52/60]	 Top 1-err 28.830  Top 5-err 7.980	 Test Loss 1.274
71.17
0.7117
loss: 1.2742747242927552
(35, 0, 98) triplet: 0.12
(35, 0): 0.0
(35, 98): 0.12
* Epoch: [53/60]	 Top 1-err 2.905  Top 5-err 0.081	 Train Loss 0.130
* Epoch: [53/60]	 Top 1-err 28.690  Top 5-err 7.970	 Test Loss 1.275
Current best accuracy (top-1 and 5 error): 28.69 7.97
saving best model...
* Epoch: [53/60]	 Top 1-err 28.690  Top 5-err 7.970	 Test Loss 1.275
71.31
0.7131
loss: 1.2748479196548461
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
* Epoch: [54/60]	 Top 1-err 2.918  Top 5-err 0.076	 Train Loss 0.131
* Epoch: [54/60]	 Top 1-err 28.770  Top 5-err 8.030	 Test Loss 1.271
Current best accuracy (top-1 and 5 error): 28.69 7.97
* Epoch: [54/60]	 Top 1-err 28.770  Top 5-err 8.030	 Test Loss 1.271
71.23
0.7123
loss: 1.2712710123062134
(35, 0, 98) triplet: 0.12
(35, 0): 0.0
(35, 98): 0.12
* Epoch: [55/60]	 Top 1-err 2.799  Top 5-err 0.076	 Train Loss 0.129
* Epoch: [55/60]	 Top 1-err 28.840  Top 5-err 8.130	 Test Loss 1.276
Current best accuracy (top-1 and 5 error): 28.69 7.97
* Epoch: [55/60]	 Top 1-err 28.840  Top 5-err 8.130	 Test Loss 1.276
71.16
0.7116
loss: 1.2763439311981202
(35, 0, 98) triplet: 0.12
(35, 0): 0.0
(35, 98): 0.12
* Epoch: [56/60]	 Top 1-err 2.920  Top 5-err 0.074	 Train Loss 0.128
* Epoch: [56/60]	 Top 1-err 28.880  Top 5-err 8.020	 Test Loss 1.273
Current best accuracy (top-1 and 5 error): 28.69 7.97
* Epoch: [56/60]	 Top 1-err 28.880  Top 5-err 8.020	 Test Loss 1.273
71.12
0.7112
loss: 1.273103867149353
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [57/60]	 Top 1-err 2.864  Top 5-err 0.077	 Train Loss 0.129
* Epoch: [57/60]	 Top 1-err 28.910  Top 5-err 7.990	 Test Loss 1.276
Current best accuracy (top-1 and 5 error): 28.69 7.97
* Epoch: [57/60]	 Top 1-err 28.910  Top 5-err 7.990	 Test Loss 1.276
71.09
0.7109
loss: 1.2755263484954833
(35, 0, 98) triplet: 0.13
(35, 0): 0.0
(35, 98): 0.13
not enough sample
* Epoch: [58/60]	 Top 1-err 2.809  Top 5-err 0.076	 Train Loss 0.128
* Epoch: [58/60]	 Top 1-err 28.780  Top 5-err 8.110	 Test Loss 1.274
Current best accuracy (top-1 and 5 error): 28.69 7.97
* Epoch: [58/60]	 Top 1-err 28.780  Top 5-err 8.110	 Test Loss 1.274
71.22
0.7122
loss: 1.2742108337402345
(35, 0, 98) triplet: 0.13
(35, 0): 0.0
(35, 98): 0.13
* Epoch: [59/60]	 Top 1-err 2.839  Top 5-err 0.089	 Train Loss 0.127
* Epoch: [59/60]	 Top 1-err 28.790  Top 5-err 8.090	 Test Loss 1.276
Current best accuracy (top-1 and 5 error): 28.69 7.97
* Epoch: [59/60]	 Top 1-err 28.790  Top 5-err 8.090	 Test Loss 1.276
71.21
0.7121
loss: 1.2760017024993897
(35, 0, 98) triplet: 0.125
(35, 0): 0.0
(35, 98): 0.125
Best accuracy (top-1 and 5 error): 28.69 7.97
=> loading checkpoint 'runs/cifar100_resnet_bias/model_best.pth.tar'
* Epoch: [-1/60]	 Top 1-err 28.690  Top 5-err 7.970	 Test Loss 1.275
71.31
0.7131
loss: 1.274847934627533
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
python3 cifar100_repair_bias.py --net_type resnet --dataset cifar100 --depth 50 --batch_size 256 --lr 0.1 --expname cifar100_resnet_bias --epochs 60 --beta 1.0 --cutmix_prob 0 --pretrained ./runs/cifar100_resnet_2/model_best.pth.tar --expid 0 --first 35 --second 0 --third 98 --lam 1
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
=> creating model 'resnet'
True
=> loading checkpoint './runs/cifar100_resnet_2/model_best.pth.tar'
=> loaded checkpoint './runs/cifar100_resnet_2/model_best.pth.tar'
the number of model parameters: 521716
* Epoch: [0/60]	 Top 1-err 37.027  Top 5-err 10.893	 Train Loss 1.472
* Epoch: [0/60]	 Top 1-err 45.270  Top 5-err 16.860	 Test Loss 1.828
* Epoch: [0/60]	 Top 1-err 45.270  Top 5-err 16.860	 Test Loss 1.828
54.73
0.5473
loss: 1.8275189636230469
(35, 0, 98) triplet: 0.13
(35, 0): 0.04
(35, 98): 0.17
* Epoch: [1/60]	 Top 1-err 29.911  Top 5-err 6.273	 Train Loss 1.078
* Epoch: [1/60]	 Top 1-err 43.310  Top 5-err 16.070	 Test Loss 1.772
* Epoch: [1/60]	 Top 1-err 43.310  Top 5-err 16.070	 Test Loss 1.772
56.69
0.5669
loss: 1.772088419342041
(35, 0, 98) triplet: 0.05000000000000002
(35, 0): 0.135
(35, 98): 0.08499999999999999
* Epoch: [2/60]	 Top 1-err 29.182  Top 5-err 5.691	 Train Loss 1.054
* Epoch: [2/60]	 Top 1-err 54.670  Top 5-err 24.900	 Test Loss 2.614
* Epoch: [2/60]	 Top 1-err 54.670  Top 5-err 24.900	 Test Loss 2.614
45.33
0.4533
loss: 2.61357299118042
(35, 0, 98) triplet: 0.195
(35, 0): 0.195
(35, 98): 0.0
* Epoch: [3/60]	 Top 1-err 27.274  Top 5-err 5.104	 Train Loss 0.983
* Epoch: [3/60]	 Top 1-err 42.630  Top 5-err 15.040	 Test Loss 1.816
* Epoch: [3/60]	 Top 1-err 42.630  Top 5-err 15.040	 Test Loss 1.816
57.37
0.5737
loss: 1.8160737022399902
(35, 0, 98) triplet: 0.015
(35, 0): 0.045
(35, 98): 0.06
* Epoch: [4/60]	 Top 1-err 25.444  Top 5-err 4.074	 Train Loss 0.889
* Epoch: [4/60]	 Top 1-err 42.150  Top 5-err 15.270	 Test Loss 1.872
* Epoch: [4/60]	 Top 1-err 42.150  Top 5-err 15.270	 Test Loss 1.872
57.85
0.5785
loss: 1.8716700096130372
(35, 0, 98) triplet: 0.17
(35, 0): 0.0
(35, 98): 0.17
* Epoch: [5/60]	 Top 1-err 24.662  Top 5-err 3.765	 Train Loss 0.861
* Epoch: [5/60]	 Top 1-err 38.830  Top 5-err 13.160	 Test Loss 1.509
* Epoch: [5/60]	 Top 1-err 38.830  Top 5-err 13.160	 Test Loss 1.509
61.17
0.6117
loss: 1.5091398286819457
(35, 0, 98) triplet: 0.030000000000000013
(35, 0): 0.06999999999999999
(35, 98): 0.1
* Epoch: [6/60]	 Top 1-err 22.954  Top 5-err 3.175	 Train Loss 0.805
* Epoch: [6/60]	 Top 1-err 40.110  Top 5-err 12.900	 Test Loss 1.677
* Epoch: [6/60]	 Top 1-err 40.110  Top 5-err 12.900	 Test Loss 1.677
59.89
0.5989
loss: 1.677475059890747
(35, 0, 98) triplet: 0.29500000000000004
(35, 0): 0.005
(35, 98): 0.30000000000000004
* Epoch: [7/60]	 Top 1-err 22.240  Top 5-err 2.867	 Train Loss 0.773
* Epoch: [7/60]	 Top 1-err 38.350  Top 5-err 11.600	 Test Loss 1.520
* Epoch: [7/60]	 Top 1-err 38.350  Top 5-err 11.600	 Test Loss 1.520
61.65
0.6165
loss: 1.5198683528900145
(35, 0, 98) triplet: 0.17
(35, 0): 0.025
(35, 98): 0.195
* Epoch: [8/60]	 Top 1-err 20.735  Top 5-err 2.448	 Train Loss 0.713
* Epoch: [8/60]	 Top 1-err 39.190  Top 5-err 13.150	 Test Loss 1.671
* Epoch: [8/60]	 Top 1-err 39.190  Top 5-err 13.150	 Test Loss 1.671
60.81
0.6081
loss: 1.6710764654159547
(35, 0, 98) triplet: 0.10500000000000001
(35, 0): 0.060000000000000005
(35, 98): 0.165
* Epoch: [9/60]	 Top 1-err 21.065  Top 5-err 2.558	 Train Loss 0.723
* Epoch: [9/60]	 Top 1-err 37.950  Top 5-err 12.290	 Test Loss 1.582
* Epoch: [9/60]	 Top 1-err 37.950  Top 5-err 12.290	 Test Loss 1.582
62.05
0.6205
loss: 1.5815708148956298
(35, 0, 98) triplet: 0.12
(35, 0): 0.18
(35, 98): 0.06
* Epoch: [10/60]	 Top 1-err 21.471  Top 5-err 2.684	 Train Loss 0.733
* Epoch: [10/60]	 Top 1-err 39.910  Top 5-err 13.870	 Test Loss 1.684
* Epoch: [10/60]	 Top 1-err 39.910  Top 5-err 13.870	 Test Loss 1.684
60.09
0.6009
loss: 1.6836459880828858
(35, 0, 98) triplet: 0.15
(35, 0): 0.030000000000000002
(35, 98): 0.18
* Epoch: [11/60]	 Top 1-err 21.417  Top 5-err 2.807	 Train Loss 0.734
* Epoch: [11/60]	 Top 1-err 38.770  Top 5-err 12.500	 Test Loss 1.626
* Epoch: [11/60]	 Top 1-err 38.770  Top 5-err 12.500	 Test Loss 1.626
61.23
0.6123
loss: 1.6263881916046143
(35, 0, 98) triplet: 0.01999999999999999
(35, 0): 0.07
(35, 98): 0.09
* Epoch: [12/60]	 Top 1-err 21.186  Top 5-err 2.628	 Train Loss 0.738
* Epoch: [12/60]	 Top 1-err 35.710  Top 5-err 11.310	 Test Loss 1.491
* Epoch: [12/60]	 Top 1-err 35.710  Top 5-err 11.310	 Test Loss 1.491
64.29
0.6429
loss: 1.4905587076187135
(35, 0, 98) triplet: 0.09
(35, 0): 0.155
(35, 98): 0.065
* Epoch: [13/60]	 Top 1-err 20.842  Top 5-err 2.720	 Train Loss 0.718
* Epoch: [13/60]	 Top 1-err 42.310  Top 5-err 15.350	 Test Loss 1.957
* Epoch: [13/60]	 Top 1-err 42.310  Top 5-err 15.350	 Test Loss 1.957
57.69
0.5769
loss: 1.9573538188934325
(35, 0, 98) triplet: 0.215
(35, 0): 0.24
(35, 98): 0.025
not enough sample
* Epoch: [14/60]	 Top 1-err 20.722  Top 5-err 2.503	 Train Loss 0.708
* Epoch: [14/60]	 Top 1-err 42.030  Top 5-err 14.840	 Test Loss 1.969
* Epoch: [14/60]	 Top 1-err 42.030  Top 5-err 14.840	 Test Loss 1.969
57.97
0.5797
loss: 1.9687863582611085
(35, 0, 98) triplet: 0.29
(35, 0): 0.0
(35, 98): 0.29
* Epoch: [15/60]	 Top 1-err 22.367  Top 5-err 3.011	 Train Loss 0.778
* Epoch: [15/60]	 Top 1-err 38.770  Top 5-err 13.120	 Test Loss 1.628
* Epoch: [15/60]	 Top 1-err 38.770  Top 5-err 13.120	 Test Loss 1.628
61.23
0.6123
loss: 1.6283351873397827
(35, 0, 98) triplet: 0.15
(35, 0): 0.19
(35, 98): 0.04
* Epoch: [16/60]	 Top 1-err 19.305  Top 5-err 2.129	 Train Loss 0.649
* Epoch: [16/60]	 Top 1-err 36.340  Top 5-err 10.650	 Test Loss 1.480
* Epoch: [16/60]	 Top 1-err 36.340  Top 5-err 10.650	 Test Loss 1.480
63.66
0.6366
loss: 1.4798542919158935
(35, 0, 98) triplet: 0.18
(35, 0): 0.015
(35, 98): 0.195
* Epoch: [17/60]	 Top 1-err 19.299  Top 5-err 2.123	 Train Loss 0.649
* Epoch: [17/60]	 Top 1-err 36.510  Top 5-err 11.700	 Test Loss 1.522
* Epoch: [17/60]	 Top 1-err 36.510  Top 5-err 11.700	 Test Loss 1.522
63.49
0.6349
loss: 1.5224554138183595
(35, 0, 98) triplet: 0.02500000000000001
(35, 0): 0.065
(35, 98): 0.09000000000000001
* Epoch: [18/60]	 Top 1-err 20.263  Top 5-err 2.348	 Train Loss 0.685
* Epoch: [18/60]	 Top 1-err 37.880  Top 5-err 12.700	 Test Loss 1.581
* Epoch: [18/60]	 Top 1-err 37.880  Top 5-err 12.700	 Test Loss 1.581
62.12
0.6212
loss: 1.5810465803146363
(35, 0, 98) triplet: 0.024999999999999994
(35, 0): 0.095
(35, 98): 0.07
* Epoch: [19/60]	 Top 1-err 19.717  Top 5-err 2.142	 Train Loss 0.662
* Epoch: [19/60]	 Top 1-err 37.570  Top 5-err 12.380	 Test Loss 1.661
* Epoch: [19/60]	 Top 1-err 37.570  Top 5-err 12.380	 Test Loss 1.661
62.43
0.6243
loss: 1.6607559789657593
(35, 0, 98) triplet: 0.07500000000000001
(35, 0): 0.025
(35, 98): 0.1
* Epoch: [20/60]	 Top 1-err 21.028  Top 5-err 2.524	 Train Loss 0.713
* Epoch: [20/60]	 Top 1-err 38.720  Top 5-err 12.960	 Test Loss 1.596
* Epoch: [20/60]	 Top 1-err 38.720  Top 5-err 12.960	 Test Loss 1.596
61.28
0.6128
loss: 1.5964129405975342
(35, 0, 98) triplet: 0.06
(35, 0): 0.045
(35, 98): 0.105
not enough sample
* Epoch: [21/60]	 Top 1-err 21.226  Top 5-err 2.747	 Train Loss 0.728
* Epoch: [21/60]	 Top 1-err 36.590  Top 5-err 11.330	 Test Loss 1.480
* Epoch: [21/60]	 Top 1-err 36.590  Top 5-err 11.330	 Test Loss 1.480
63.41
0.6341
loss: 1.4796827669143677
(35, 0, 98) triplet: 0.17
(35, 0): 0.03
(35, 98): 0.2
* Epoch: [22/60]	 Top 1-err 18.513  Top 5-err 1.879	 Train Loss 0.623
* Epoch: [22/60]	 Top 1-err 35.610  Top 5-err 11.170	 Test Loss 1.491
* Epoch: [22/60]	 Top 1-err 35.610  Top 5-err 11.170	 Test Loss 1.491
64.39
0.6439
loss: 1.4910931037902833
(35, 0, 98) triplet: 0.125
(35, 0): 0.085
(35, 98): 0.21000000000000002
* Epoch: [23/60]	 Top 1-err 20.529  Top 5-err 2.550	 Train Loss 0.701
* Epoch: [23/60]	 Top 1-err 39.070  Top 5-err 12.860	 Test Loss 1.588
* Epoch: [23/60]	 Top 1-err 39.070  Top 5-err 12.860	 Test Loss 1.588
60.93
0.6093
loss: 1.5881014715194701
(35, 0, 98) triplet: 0.265
(35, 0): 0.0
(35, 98): 0.265
* Epoch: [24/60]	 Top 1-err 20.038  Top 5-err 2.291	 Train Loss 0.694
* Epoch: [24/60]	 Top 1-err 39.890  Top 5-err 14.220	 Test Loss 1.805
* Epoch: [24/60]	 Top 1-err 39.890  Top 5-err 14.220	 Test Loss 1.805
60.11
0.6011
loss: 1.8047522109985352
(35, 0, 98) triplet: 0.06
(35, 0): 0.08
(35, 98): 0.02
* Epoch: [25/60]	 Top 1-err 19.023  Top 5-err 2.070	 Train Loss 0.651
* Epoch: [25/60]	 Top 1-err 38.140  Top 5-err 12.250	 Test Loss 1.625
* Epoch: [25/60]	 Top 1-err 38.140  Top 5-err 12.250	 Test Loss 1.625
61.86
0.6186
loss: 1.6252987480163574
(35, 0, 98) triplet: 0.0050000000000000044
(35, 0): 0.115
(35, 98): 0.11
* Epoch: [26/60]	 Top 1-err 18.428  Top 5-err 1.879	 Train Loss 0.620
* Epoch: [26/60]	 Top 1-err 38.980  Top 5-err 12.200	 Test Loss 1.616
* Epoch: [26/60]	 Top 1-err 38.980  Top 5-err 12.200	 Test Loss 1.616
61.02
0.6102
loss: 1.6163207878112793
(35, 0, 98) triplet: 0.2
(35, 0): 0.015
(35, 98): 0.215
* Epoch: [27/60]	 Top 1-err 18.034  Top 5-err 1.777	 Train Loss 0.599
* Epoch: [27/60]	 Top 1-err 37.840  Top 5-err 12.880	 Test Loss 1.673
* Epoch: [27/60]	 Top 1-err 37.840  Top 5-err 12.880	 Test Loss 1.673
62.16
0.6216
loss: 1.6730158173561096
(35, 0, 98) triplet: 0.19999999999999998
(35, 0): 0.025
(35, 98): 0.22499999999999998
* Epoch: [28/60]	 Top 1-err 18.289  Top 5-err 1.796	 Train Loss 0.608
* Epoch: [28/60]	 Top 1-err 39.040  Top 5-err 13.240	 Test Loss 1.725
* Epoch: [28/60]	 Top 1-err 39.040  Top 5-err 13.240	 Test Loss 1.725
60.96
0.6096
loss: 1.725429220199585
(35, 0, 98) triplet: 0.175
(35, 0): 0.015
(35, 98): 0.19
* Epoch: [29/60]	 Top 1-err 18.663  Top 5-err 2.017	 Train Loss 0.622
* Epoch: [29/60]	 Top 1-err 39.800  Top 5-err 13.150	 Test Loss 1.655
* Epoch: [29/60]	 Top 1-err 39.800  Top 5-err 13.150	 Test Loss 1.655
60.2
0.602
loss: 1.6554161026000977
(35, 0, 98) triplet: 0.07
(35, 0): 0.06
(35, 98): 0.13
* Epoch: [30/60]	 Top 1-err 11.842  Top 5-err 0.897	 Train Loss 0.427
* Epoch: [30/60]	 Top 1-err 29.780  Top 5-err 8.520	 Test Loss 1.197
* Epoch: [30/60]	 Top 1-err 29.780  Top 5-err 8.520	 Test Loss 1.197
70.22
0.7022
loss: 1.197429869556427
(35, 0, 98) triplet: 0.14999999999999997
(35, 0): 0.01
(35, 98): 0.15999999999999998
* Epoch: [31/60]	 Top 1-err 8.504  Top 5-err 0.501	 Train Loss 0.337
* Epoch: [31/60]	 Top 1-err 29.580  Top 5-err 8.070	 Test Loss 1.189
* Epoch: [31/60]	 Top 1-err 29.580  Top 5-err 8.070	 Test Loss 1.189
70.42
0.7042
loss: 1.1886563467979432
(35, 0, 98) triplet: 0.13499999999999998
(35, 0): 0.01
(35, 98): 0.145
* Epoch: [32/60]	 Top 1-err 7.565  Top 5-err 0.408	 Train Loss 0.313
* Epoch: [32/60]	 Top 1-err 29.170  Top 5-err 8.360	 Test Loss 1.203
* Epoch: [32/60]	 Top 1-err 29.170  Top 5-err 8.360	 Test Loss 1.203
70.83
0.7083
loss: 1.2031010887145996
(35, 0, 98) triplet: 0.115
(35, 0): 0.01
(35, 98): 0.125
* Epoch: [33/60]	 Top 1-err 6.889  Top 5-err 0.336	 Train Loss 0.289
* Epoch: [33/60]	 Top 1-err 29.170  Top 5-err 8.200	 Test Loss 1.201
* Epoch: [33/60]	 Top 1-err 29.170  Top 5-err 8.200	 Test Loss 1.201
70.83
0.7083
loss: 1.201148132610321
(35, 0, 98) triplet: 0.08
(35, 0): 0.02
(35, 98): 0.1
* Epoch: [34/60]	 Top 1-err 6.175  Top 5-err 0.331	 Train Loss 0.273
* Epoch: [34/60]	 Top 1-err 29.070  Top 5-err 8.040	 Test Loss 1.219
* Epoch: [34/60]	 Top 1-err 29.070  Top 5-err 8.040	 Test Loss 1.219
70.93
0.7093
loss: 1.2187754934310913
(35, 0, 98) triplet: 0.135
(35, 0): 0.015
(35, 98): 0.15
* Epoch: [35/60]	 Top 1-err 5.827  Top 5-err 0.238	 Train Loss 0.263
* Epoch: [35/60]	 Top 1-err 28.860  Top 5-err 8.210	 Test Loss 1.216
* Epoch: [35/60]	 Top 1-err 28.860  Top 5-err 8.210	 Test Loss 1.216
71.14
0.7114
loss: 1.2156526071548461
(35, 0, 98) triplet: 0.07999999999999999
(35, 0): 0.02
(35, 98): 0.09999999999999999
* Epoch: [36/60]	 Top 1-err 5.412  Top 5-err 0.244	 Train Loss 0.251
* Epoch: [36/60]	 Top 1-err 28.970  Top 5-err 8.180	 Test Loss 1.226
* Epoch: [36/60]	 Top 1-err 28.970  Top 5-err 8.180	 Test Loss 1.226
71.03
0.7103
loss: 1.2256737689971924
(35, 0, 98) triplet: 0.095
(35, 0): 0.034999999999999996
(35, 98): 0.13
* Epoch: [37/60]	 Top 1-err 5.410  Top 5-err 0.217	 Train Loss 0.247
* Epoch: [37/60]	 Top 1-err 28.880  Top 5-err 8.160	 Test Loss 1.233
* Epoch: [37/60]	 Top 1-err 28.880  Top 5-err 8.160	 Test Loss 1.233
71.12
0.7112
loss: 1.2330131525993346
(35, 0, 98) triplet: 0.115
(35, 0): 0.02
(35, 98): 0.135
not enough sample
* Epoch: [38/60]	 Top 1-err 4.896  Top 5-err 0.212	 Train Loss 0.233
* Epoch: [38/60]	 Top 1-err 29.070  Top 5-err 8.300	 Test Loss 1.236
* Epoch: [38/60]	 Top 1-err 29.070  Top 5-err 8.300	 Test Loss 1.236
70.93
0.7093
loss: 1.2364554462432862
(35, 0, 98) triplet: 0.145
(35, 0): 0.01
(35, 98): 0.155
* Epoch: [39/60]	 Top 1-err 4.836  Top 5-err 0.191	 Train Loss 0.233
* Epoch: [39/60]	 Top 1-err 29.140  Top 5-err 8.290	 Test Loss 1.249
* Epoch: [39/60]	 Top 1-err 29.140  Top 5-err 8.290	 Test Loss 1.249
70.86
0.7086
loss: 1.2487275886535645
(35, 0, 98) triplet: 0.09
(35, 0): 0.04
(35, 98): 0.13
not enough sample
* Epoch: [40/60]	 Top 1-err 4.694  Top 5-err 0.170	 Train Loss 0.225
* Epoch: [40/60]	 Top 1-err 29.000  Top 5-err 8.190	 Test Loss 1.257
* Epoch: [40/60]	 Top 1-err 29.000  Top 5-err 8.190	 Test Loss 1.257
71.0
0.71
loss: 1.2566985710144043
(35, 0, 98) triplet: 0.13
(35, 0): 0.005
(35, 98): 0.135
* Epoch: [41/60]	 Top 1-err 4.437  Top 5-err 0.138	 Train Loss 0.217
* Epoch: [41/60]	 Top 1-err 29.220  Top 5-err 8.210	 Test Loss 1.268
* Epoch: [41/60]	 Top 1-err 29.220  Top 5-err 8.210	 Test Loss 1.268
70.78
0.7078
loss: 1.2675560848236085
(35, 0, 98) triplet: 0.13
(35, 0): 0.005
(35, 98): 0.135
* Epoch: [42/60]	 Top 1-err 4.256  Top 5-err 0.136	 Train Loss 0.213
* Epoch: [42/60]	 Top 1-err 29.240  Top 5-err 8.410	 Test Loss 1.279
* Epoch: [42/60]	 Top 1-err 29.240  Top 5-err 8.410	 Test Loss 1.279
70.76
0.7076
loss: 1.278896910905838
(35, 0, 98) triplet: 0.195
(35, 0): 0.0
(35, 98): 0.195
* Epoch: [43/60]	 Top 1-err 4.063  Top 5-err 0.155	 Train Loss 0.206
* Epoch: [43/60]	 Top 1-err 28.830  Top 5-err 8.270	 Test Loss 1.280
* Epoch: [43/60]	 Top 1-err 28.830  Top 5-err 8.270	 Test Loss 1.280
71.17
0.7117
loss: 1.27957931060791
(35, 0, 98) triplet: 0.165
(35, 0): 0.0
(35, 98): 0.165
* Epoch: [44/60]	 Top 1-err 3.950  Top 5-err 0.128	 Train Loss 0.210
* Epoch: [44/60]	 Top 1-err 28.800  Top 5-err 8.330	 Test Loss 1.282
* Epoch: [44/60]	 Top 1-err 28.800  Top 5-err 8.330	 Test Loss 1.282
71.2
0.712
loss: 1.2821824851989747
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
* Epoch: [45/60]	 Top 1-err 3.529  Top 5-err 0.113	 Train Loss 0.187
* Epoch: [45/60]	 Top 1-err 28.780  Top 5-err 8.240	 Test Loss 1.275
Current best accuracy (top-1 and 5 error): 28.78 8.24
saving best model...
* Epoch: [45/60]	 Top 1-err 28.780  Top 5-err 8.240	 Test Loss 1.275
71.22
0.7122
loss: 1.2753278619766235
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
* Epoch: [46/60]	 Top 1-err 3.423  Top 5-err 0.115	 Train Loss 0.184
* Epoch: [46/60]	 Top 1-err 28.860  Top 5-err 8.180	 Test Loss 1.276
Current best accuracy (top-1 and 5 error): 28.78 8.24
* Epoch: [46/60]	 Top 1-err 28.860  Top 5-err 8.180	 Test Loss 1.276
71.14
0.7114
loss: 1.276147201538086
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
not enough sample
* Epoch: [47/60]	 Top 1-err 3.379  Top 5-err 0.091	 Train Loss 0.182
* Epoch: [47/60]	 Top 1-err 28.920  Top 5-err 8.260	 Test Loss 1.273
Current best accuracy (top-1 and 5 error): 28.78 8.24
* Epoch: [47/60]	 Top 1-err 28.920  Top 5-err 8.260	 Test Loss 1.273
71.08
0.7108
loss: 1.2731476222991944
(35, 0, 98) triplet: 0.15000000000000002
(35, 0): 0.0
(35, 98): 0.15000000000000002
* Epoch: [48/60]	 Top 1-err 3.366  Top 5-err 0.113	 Train Loss 0.179
* Epoch: [48/60]	 Top 1-err 28.840  Top 5-err 8.240	 Test Loss 1.271
Current best accuracy (top-1 and 5 error): 28.78 8.24
* Epoch: [48/60]	 Top 1-err 28.840  Top 5-err 8.240	 Test Loss 1.271
71.16
0.7116
loss: 1.2711348207473754
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [49/60]	 Top 1-err 3.183  Top 5-err 0.098	 Train Loss 0.177
* Epoch: [49/60]	 Top 1-err 28.800  Top 5-err 8.100	 Test Loss 1.271
Current best accuracy (top-1 and 5 error): 28.78 8.24
* Epoch: [49/60]	 Top 1-err 28.800  Top 5-err 8.100	 Test Loss 1.271
71.2
0.712
loss: 1.270504961013794
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [50/60]	 Top 1-err 3.207  Top 5-err 0.115	 Train Loss 0.178
* Epoch: [50/60]	 Top 1-err 28.850  Top 5-err 8.210	 Test Loss 1.274
Current best accuracy (top-1 and 5 error): 28.78 8.24
* Epoch: [50/60]	 Top 1-err 28.850  Top 5-err 8.210	 Test Loss 1.274
71.15
0.7115
loss: 1.2738567842483521
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [51/60]	 Top 1-err 3.232  Top 5-err 0.100	 Train Loss 0.176
* Epoch: [51/60]	 Top 1-err 28.820  Top 5-err 8.070	 Test Loss 1.274
Current best accuracy (top-1 and 5 error): 28.78 8.24
* Epoch: [51/60]	 Top 1-err 28.820  Top 5-err 8.070	 Test Loss 1.274
71.18
0.7118
loss: 1.2738672065734864
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [52/60]	 Top 1-err 3.109  Top 5-err 0.094	 Train Loss 0.173
* Epoch: [52/60]	 Top 1-err 28.780  Top 5-err 8.170	 Test Loss 1.275
Current best accuracy (top-1 and 5 error): 28.78 8.17
saving best model...
* Epoch: [52/60]	 Top 1-err 28.780  Top 5-err 8.170	 Test Loss 1.275
71.22
0.7122
loss: 1.2746325048446656
(35, 0, 98) triplet: 0.18
(35, 0): 0.0
(35, 98): 0.18
* Epoch: [53/60]	 Top 1-err 3.132  Top 5-err 0.096	 Train Loss 0.174
* Epoch: [53/60]	 Top 1-err 28.900  Top 5-err 8.130	 Test Loss 1.278
Current best accuracy (top-1 and 5 error): 28.78 8.17
* Epoch: [53/60]	 Top 1-err 28.900  Top 5-err 8.130	 Test Loss 1.278
71.1
0.711
loss: 1.2782574584960937
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [54/60]	 Top 1-err 3.111  Top 5-err 0.110	 Train Loss 0.174
* Epoch: [54/60]	 Top 1-err 28.840  Top 5-err 8.100	 Test Loss 1.272
Current best accuracy (top-1 and 5 error): 28.78 8.17
* Epoch: [54/60]	 Top 1-err 28.840  Top 5-err 8.100	 Test Loss 1.272
71.16
0.7116
loss: 1.2716117880821227
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
* Epoch: [55/60]	 Top 1-err 3.253  Top 5-err 0.102	 Train Loss 0.173
* Epoch: [55/60]	 Top 1-err 28.860  Top 5-err 8.200	 Test Loss 1.279
Current best accuracy (top-1 and 5 error): 28.78 8.17
* Epoch: [55/60]	 Top 1-err 28.860  Top 5-err 8.200	 Test Loss 1.279
71.14
0.7114
loss: 1.2786662998199463
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [56/60]	 Top 1-err 3.081  Top 5-err 0.091	 Train Loss 0.170
* Epoch: [56/60]	 Top 1-err 28.850  Top 5-err 8.120	 Test Loss 1.275
Current best accuracy (top-1 and 5 error): 28.78 8.17
* Epoch: [56/60]	 Top 1-err 28.850  Top 5-err 8.120	 Test Loss 1.275
71.15
0.7115
loss: 1.2752130367279053
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [57/60]	 Top 1-err 3.049  Top 5-err 0.089	 Train Loss 0.169
* Epoch: [57/60]	 Top 1-err 28.780  Top 5-err 8.050	 Test Loss 1.279
Current best accuracy (top-1 and 5 error): 28.78 8.05
saving best model...
* Epoch: [57/60]	 Top 1-err 28.780  Top 5-err 8.050	 Test Loss 1.279
71.22
0.7122
loss: 1.2790385709762573
(35, 0, 98) triplet: 0.13999999999999999
(35, 0): 0.0
(35, 98): 0.13999999999999999
not enough sample
* Epoch: [58/60]	 Top 1-err 3.206  Top 5-err 0.091	 Train Loss 0.171
* Epoch: [58/60]	 Top 1-err 28.950  Top 5-err 8.090	 Test Loss 1.279
Current best accuracy (top-1 and 5 error): 28.78 8.05
* Epoch: [58/60]	 Top 1-err 28.950  Top 5-err 8.090	 Test Loss 1.279
71.05
0.7105
loss: 1.2794349235534668
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [59/60]	 Top 1-err 3.026  Top 5-err 0.104	 Train Loss 0.168
* Epoch: [59/60]	 Top 1-err 28.970  Top 5-err 8.170	 Test Loss 1.280
Current best accuracy (top-1 and 5 error): 28.78 8.05
* Epoch: [59/60]	 Top 1-err 28.970  Top 5-err 8.170	 Test Loss 1.280
71.03
0.7103
loss: 1.2799887495040894
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
Best accuracy (top-1 and 5 error): 28.78 8.05
=> loading checkpoint 'runs/cifar100_resnet_bias/model_best.pth.tar'
* Epoch: [-1/60]	 Top 1-err 28.780  Top 5-err 8.050	 Test Loss 1.279
71.22
0.7122
loss: 1.2790385667800903
(35, 0, 98) triplet: 0.13999999999999999
(35, 0): 0.0
(35, 98): 0.13999999999999999
python3 cifar100_repair_bias.py --net_type resnet --dataset cifar100 --depth 50 --batch_size 256 --lr 0.1 --expname cifar100_resnet_bias --epochs 60 --beta 1.0 --cutmix_prob 0 --pretrained ./runs/cifar100_resnet_2/model_best.pth.tar --expid 0 --first 35 --second 0 --third 98 --lam 2
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
=> creating model 'resnet'
True
=> loading checkpoint './runs/cifar100_resnet_2/model_best.pth.tar'
=> loaded checkpoint './runs/cifar100_resnet_2/model_best.pth.tar'
the number of model parameters: 521716
* Epoch: [0/60]	 Top 1-err 56.728  Top 5-err 26.609	 Train Loss 2.423
* Epoch: [0/60]	 Top 1-err 56.050  Top 5-err 25.170	 Test Loss 2.273
* Epoch: [0/60]	 Top 1-err 56.050  Top 5-err 25.170	 Test Loss 2.273
43.95
0.4395
loss: 2.2730559761047364
(35, 0, 98) triplet: 0.11000000000000001
(35, 0): 0.17500000000000002
(35, 98): 0.065
* Epoch: [1/60]	 Top 1-err 43.504  Top 5-err 14.482	 Train Loss 1.677
* Epoch: [1/60]	 Top 1-err 49.040  Top 5-err 20.360	 Test Loss 1.986
* Epoch: [1/60]	 Top 1-err 49.040  Top 5-err 20.360	 Test Loss 1.986
50.96
0.5096
loss: 1.986207502746582
(35, 0, 98) triplet: 0.115
(35, 0): 0.085
(35, 98): 0.2
* Epoch: [2/60]	 Top 1-err 40.015  Top 5-err 12.204	 Train Loss 1.522
* Epoch: [2/60]	 Top 1-err 47.550  Top 5-err 18.940	 Test Loss 1.831
* Epoch: [2/60]	 Top 1-err 47.550  Top 5-err 18.940	 Test Loss 1.831
52.45
0.5245
loss: 1.8307662326812744
(35, 0, 98) triplet: 0.125
(35, 0): 0.46
(35, 98): 0.335
* Epoch: [3/60]	 Top 1-err 37.010  Top 5-err 9.949	 Train Loss 1.377
* Epoch: [3/60]	 Top 1-err 59.070  Top 5-err 28.250	 Test Loss 2.948
* Epoch: [3/60]	 Top 1-err 59.070  Top 5-err 28.250	 Test Loss 2.948
40.93
0.4093
loss: 2.9483322525024414
(35, 0, 98) triplet: 0.205
(35, 0): 0.21
(35, 98): 0.005
* Epoch: [4/60]	 Top 1-err 37.221  Top 5-err 9.764	 Train Loss 1.373
* Epoch: [4/60]	 Top 1-err 46.520  Top 5-err 17.740	 Test Loss 1.919
* Epoch: [4/60]	 Top 1-err 46.520  Top 5-err 17.740	 Test Loss 1.919
53.48
0.5348
loss: 1.9186849731445312
(35, 0, 98) triplet: 0.10999999999999999
(35, 0): 0.07
(35, 98): 0.18
* Epoch: [5/60]	 Top 1-err 35.731  Top 5-err 9.227	 Train Loss 1.321
* Epoch: [5/60]	 Top 1-err 45.400  Top 5-err 17.190	 Test Loss 1.720
* Epoch: [5/60]	 Top 1-err 45.400  Top 5-err 17.190	 Test Loss 1.720
54.6
0.546
loss: 1.7204454858779907
(35, 0, 98) triplet: 0.020000000000000018
(35, 0): 0.175
(35, 98): 0.195
* Epoch: [6/60]	 Top 1-err 34.105  Top 5-err 8.328	 Train Loss 1.249
* Epoch: [6/60]	 Top 1-err 47.430  Top 5-err 18.330	 Test Loss 1.855
* Epoch: [6/60]	 Top 1-err 47.430  Top 5-err 18.330	 Test Loss 1.855
52.57
0.5257
loss: 1.8549630031585693
(35, 0, 98) triplet: 0.28500000000000003
(35, 0): 0.01
(35, 98): 0.29500000000000004
* Epoch: [7/60]	 Top 1-err 33.313  Top 5-err 7.752	 Train Loss 1.214
* Epoch: [7/60]	 Top 1-err 46.500  Top 5-err 18.270	 Test Loss 1.818
* Epoch: [7/60]	 Top 1-err 46.500  Top 5-err 18.270	 Test Loss 1.818
53.5
0.535
loss: 1.8179998106002808
(35, 0, 98) triplet: 0.16
(35, 0): 0.23
(35, 98): 0.07
* Epoch: [8/60]	 Top 1-err 32.605  Top 5-err 7.552	 Train Loss 1.190
* Epoch: [8/60]	 Top 1-err 39.720  Top 5-err 13.370	 Test Loss 1.456
* Epoch: [8/60]	 Top 1-err 39.720  Top 5-err 13.370	 Test Loss 1.456
60.28
0.6028
loss: 1.456347901725769
(35, 0, 98) triplet: 0.135
(35, 0): 0.23
(35, 98): 0.095
* Epoch: [9/60]	 Top 1-err 30.557  Top 5-err 6.294	 Train Loss 1.095
* Epoch: [9/60]	 Top 1-err 41.930  Top 5-err 14.740	 Test Loss 1.680
* Epoch: [9/60]	 Top 1-err 41.930  Top 5-err 14.740	 Test Loss 1.680
58.07
0.5807
loss: 1.6802496307373047
(35, 0, 98) triplet: 0.11
(35, 0): 0.11
(35, 98): 0.22
* Epoch: [10/60]	 Top 1-err 30.861  Top 5-err 6.343	 Train Loss 1.108
* Epoch: [10/60]	 Top 1-err 40.620  Top 5-err 13.460	 Test Loss 1.573
* Epoch: [10/60]	 Top 1-err 40.620  Top 5-err 13.460	 Test Loss 1.573
59.38
0.5938
loss: 1.5729596637725831
(35, 0, 98) triplet: 0.035
(35, 0): 0.1
(35, 98): 0.135
* Epoch: [11/60]	 Top 1-err 31.747  Top 5-err 6.902	 Train Loss 1.141
* Epoch: [11/60]	 Top 1-err 39.440  Top 5-err 13.730	 Test Loss 1.560
* Epoch: [11/60]	 Top 1-err 39.440  Top 5-err 13.730	 Test Loss 1.560
60.56
0.6056
loss: 1.5599633081436157
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.285
(35, 98): 0.12000000000000001
* Epoch: [12/60]	 Top 1-err 30.824  Top 5-err 6.522	 Train Loss 1.100
* Epoch: [12/60]	 Top 1-err 42.340  Top 5-err 14.220	 Test Loss 1.587
* Epoch: [12/60]	 Top 1-err 42.340  Top 5-err 14.220	 Test Loss 1.587
57.66
0.5766
loss: 1.5865744407653808
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.09000000000000001
(35, 98): 0.26
* Epoch: [13/60]	 Top 1-err 30.986  Top 5-err 6.711	 Train Loss 1.111
* Epoch: [13/60]	 Top 1-err 39.530  Top 5-err 13.140	 Test Loss 1.559
* Epoch: [13/60]	 Top 1-err 39.530  Top 5-err 13.140	 Test Loss 1.559
60.47
0.6047
loss: 1.5586390979766847
(35, 0, 98) triplet: 0.15500000000000003
(35, 0): 0.07
(35, 98): 0.22500000000000003
not enough sample
* Epoch: [14/60]	 Top 1-err 30.457  Top 5-err 6.284	 Train Loss 1.088
* Epoch: [14/60]	 Top 1-err 42.810  Top 5-err 15.080	 Test Loss 1.801
* Epoch: [14/60]	 Top 1-err 42.810  Top 5-err 15.080	 Test Loss 1.801
57.19
0.5719
loss: 1.801439978313446
(35, 0, 98) triplet: 0.015000000000000013
(35, 0): 0.145
(35, 98): 0.16
* Epoch: [15/60]	 Top 1-err 29.246  Top 5-err 5.950	 Train Loss 1.063
* Epoch: [15/60]	 Top 1-err 39.920  Top 5-err 13.210	 Test Loss 1.545
* Epoch: [15/60]	 Top 1-err 39.920  Top 5-err 13.210	 Test Loss 1.545
60.08
0.6008
loss: 1.5445150848388671
(35, 0, 98) triplet: 0.185
(35, 0): 0.13
(35, 98): 0.315
* Epoch: [16/60]	 Top 1-err 29.127  Top 5-err 5.790	 Train Loss 1.049
* Epoch: [16/60]	 Top 1-err 41.680  Top 5-err 15.070	 Test Loss 1.706
* Epoch: [16/60]	 Top 1-err 41.680  Top 5-err 15.070	 Test Loss 1.706
58.32
0.5832
loss: 1.7063496208190918
(35, 0, 98) triplet: 0.05499999999999999
(35, 0): 0.155
(35, 98): 0.1
* Epoch: [17/60]	 Top 1-err 29.598  Top 5-err 6.092	 Train Loss 1.052
* Epoch: [17/60]	 Top 1-err 40.510  Top 5-err 14.050	 Test Loss 1.634
* Epoch: [17/60]	 Top 1-err 40.510  Top 5-err 14.050	 Test Loss 1.634
59.49
0.5949
loss: 1.6344505453109741
(35, 0, 98) triplet: 0.08
(35, 0): 0.195
(35, 98): 0.115
* Epoch: [18/60]	 Top 1-err 30.421  Top 5-err 6.173	 Train Loss 1.084
* Epoch: [18/60]	 Top 1-err 41.150  Top 5-err 14.280	 Test Loss 1.601
* Epoch: [18/60]	 Top 1-err 41.150  Top 5-err 14.280	 Test Loss 1.601
58.85
0.5885
loss: 1.6006694101333618
(35, 0, 98) triplet: 0.11499999999999999
(35, 0): 0.24
(35, 98): 0.125
* Epoch: [19/60]	 Top 1-err 30.030  Top 5-err 6.058	 Train Loss 1.080
* Epoch: [19/60]	 Top 1-err 41.830  Top 5-err 15.550	 Test Loss 1.809
* Epoch: [19/60]	 Top 1-err 41.830  Top 5-err 15.550	 Test Loss 1.809
58.17
0.5817
loss: 1.8091268812179566
(35, 0, 98) triplet: 0.08499999999999999
(35, 0): 0.22999999999999998
(35, 98): 0.145
* Epoch: [20/60]	 Top 1-err 30.355  Top 5-err 6.413	 Train Loss 1.097
* Epoch: [20/60]	 Top 1-err 41.860  Top 5-err 14.300	 Test Loss 1.587
* Epoch: [20/60]	 Top 1-err 41.860  Top 5-err 14.300	 Test Loss 1.587
58.14
0.5814
loss: 1.5865849472999574
(35, 0, 98) triplet: 0.22499999999999998
(35, 0): 0.01
(35, 98): 0.235
not enough sample
* Epoch: [21/60]	 Top 1-err 29.989  Top 5-err 6.158	 Train Loss 1.076
* Epoch: [21/60]	 Top 1-err 39.160  Top 5-err 12.950	 Test Loss 1.574
* Epoch: [21/60]	 Top 1-err 39.160  Top 5-err 12.950	 Test Loss 1.574
60.84
0.6084
loss: 1.574370390510559
(35, 0, 98) triplet: 0.065
(35, 0): 0.09
(35, 98): 0.155
* Epoch: [22/60]	 Top 1-err 29.677  Top 5-err 5.835	 Train Loss 1.048
* Epoch: [22/60]	 Top 1-err 40.340  Top 5-err 13.500	 Test Loss 1.643
* Epoch: [22/60]	 Top 1-err 40.340  Top 5-err 13.500	 Test Loss 1.643
59.66
0.5966
loss: 1.6430109664916992
(35, 0, 98) triplet: 0.065
(35, 0): 0.315
(35, 98): 0.25
* Epoch: [23/60]	 Top 1-err 29.486  Top 5-err 5.659	 Train Loss 1.029
* Epoch: [23/60]	 Top 1-err 42.010  Top 5-err 15.150	 Test Loss 1.709
* Epoch: [23/60]	 Top 1-err 42.010  Top 5-err 15.150	 Test Loss 1.709
57.99
0.5799
loss: 1.7085360095977784
(35, 0, 98) triplet: 0.22500000000000003
(35, 0): 0.275
(35, 98): 0.05
* Epoch: [24/60]	 Top 1-err 29.584  Top 5-err 5.876	 Train Loss 1.054
* Epoch: [24/60]	 Top 1-err 41.320  Top 5-err 14.110	 Test Loss 1.625
* Epoch: [24/60]	 Top 1-err 41.320  Top 5-err 14.110	 Test Loss 1.625
58.68
0.5868
loss: 1.6254346391677856
(35, 0, 98) triplet: 0.19
(35, 0): 0.11499999999999999
(35, 98): 0.305
* Epoch: [25/60]	 Top 1-err 27.172  Top 5-err 4.581	 Train Loss 0.942
* Epoch: [25/60]	 Top 1-err 41.070  Top 5-err 14.160	 Test Loss 1.718
* Epoch: [25/60]	 Top 1-err 41.070  Top 5-err 14.160	 Test Loss 1.718
58.93
0.5893
loss: 1.7176113185882569
(35, 0, 98) triplet: 0.01999999999999999
(35, 0): 0.205
(35, 98): 0.185
* Epoch: [26/60]	 Top 1-err 27.476  Top 5-err 4.573	 Train Loss 0.948
* Epoch: [26/60]	 Top 1-err 38.230  Top 5-err 12.290	 Test Loss 1.477
* Epoch: [26/60]	 Top 1-err 38.230  Top 5-err 12.290	 Test Loss 1.477
61.77
0.6177
loss: 1.4766754825592041
(35, 0, 98) triplet: 0.12000000000000005
(35, 0): 0.42000000000000004
(35, 98): 0.3
* Epoch: [27/60]	 Top 1-err 26.913  Top 5-err 4.675	 Train Loss 0.935
* Epoch: [27/60]	 Top 1-err 38.580  Top 5-err 12.740	 Test Loss 1.496
* Epoch: [27/60]	 Top 1-err 38.580  Top 5-err 12.740	 Test Loss 1.496
61.42
0.6142
loss: 1.4962427820205688
(35, 0, 98) triplet: 0.015000000000000013
(35, 0): 0.34
(35, 98): 0.325
* Epoch: [28/60]	 Top 1-err 26.307  Top 5-err 4.350	 Train Loss 0.912
* Epoch: [28/60]	 Top 1-err 37.160  Top 5-err 11.610	 Test Loss 1.436
* Epoch: [28/60]	 Top 1-err 37.160  Top 5-err 11.610	 Test Loss 1.436
62.84
0.6284
loss: 1.4360049615859984
(35, 0, 98) triplet: 0.08500000000000002
(35, 0): 0.16999999999999998
(35, 98): 0.255
* Epoch: [29/60]	 Top 1-err 26.951  Top 5-err 4.490	 Train Loss 0.919
* Epoch: [29/60]	 Top 1-err 46.110  Top 5-err 17.670	 Test Loss 1.848
* Epoch: [29/60]	 Top 1-err 46.110  Top 5-err 17.670	 Test Loss 1.848
53.89
0.5389
loss: 1.8480313369750976
(35, 0, 98) triplet: 0.24000000000000002
(35, 0): 0.27
(35, 98): 0.03
* Epoch: [30/60]	 Top 1-err 22.055  Top 5-err 3.241	 Train Loss 0.777
* Epoch: [30/60]	 Top 1-err 31.670  Top 5-err 9.040	 Test Loss 1.180
* Epoch: [30/60]	 Top 1-err 31.670  Top 5-err 9.040	 Test Loss 1.180
68.33
0.6833
loss: 1.1799952701568603
(35, 0, 98) triplet: 0.06
(35, 0): 0.21
(35, 98): 0.15
* Epoch: [31/60]	 Top 1-err 18.325  Top 5-err 2.012	 Train Loss 0.651
* Epoch: [31/60]	 Top 1-err 30.830  Top 5-err 8.380	 Test Loss 1.154
* Epoch: [31/60]	 Top 1-err 30.830  Top 5-err 8.380	 Test Loss 1.154
69.17
0.6917
loss: 1.1535828324317932
(35, 0, 98) triplet: 0.0050000000000000044
(35, 0): 0.205
(35, 98): 0.19999999999999998
* Epoch: [32/60]	 Top 1-err 17.280  Top 5-err 1.738	 Train Loss 0.592
* Epoch: [32/60]	 Top 1-err 30.640  Top 5-err 8.150	 Test Loss 1.142
* Epoch: [32/60]	 Top 1-err 30.640  Top 5-err 8.150	 Test Loss 1.142
69.36
0.6936
loss: 1.1424647272109985
(35, 0, 98) triplet: 0.06
(35, 0): 0.195
(35, 98): 0.255
* Epoch: [33/60]	 Top 1-err 16.619  Top 5-err 1.526	 Train Loss 0.559
* Epoch: [33/60]	 Top 1-err 30.780  Top 5-err 8.190	 Test Loss 1.144
* Epoch: [33/60]	 Top 1-err 30.780  Top 5-err 8.190	 Test Loss 1.144
69.22
0.6922
loss: 1.1444048619270324
(35, 0, 98) triplet: 0.019999999999999962
(35, 0): 0.31
(35, 98): 0.29000000000000004
* Epoch: [34/60]	 Top 1-err 16.148  Top 5-err 1.456	 Train Loss 0.536
* Epoch: [34/60]	 Top 1-err 30.390  Top 5-err 8.050	 Test Loss 1.149
* Epoch: [34/60]	 Top 1-err 30.390  Top 5-err 8.050	 Test Loss 1.149
69.61
0.6961
loss: 1.149440456199646
(35, 0, 98) triplet: 0.05499999999999994
(35, 0): 0.28
(35, 98): 0.33499999999999996
* Epoch: [35/60]	 Top 1-err 15.703  Top 5-err 1.315	 Train Loss 0.517
* Epoch: [35/60]	 Top 1-err 29.840  Top 5-err 8.150	 Test Loss 1.151
* Epoch: [35/60]	 Top 1-err 29.840  Top 5-err 8.150	 Test Loss 1.151
70.16
0.7016
loss: 1.1510305875778197
(35, 0, 98) triplet: 0.07500000000000001
(35, 0): 0.195
(35, 98): 0.27
* Epoch: [36/60]	 Top 1-err 15.480  Top 5-err 1.258	 Train Loss 0.509
* Epoch: [36/60]	 Top 1-err 30.310  Top 5-err 7.970	 Test Loss 1.154
* Epoch: [36/60]	 Top 1-err 30.310  Top 5-err 7.970	 Test Loss 1.154
69.69
0.6969
loss: 1.1541303079605103
(35, 0, 98) triplet: 0.12000000000000002
(35, 0): 0.145
(35, 98): 0.265
* Epoch: [37/60]	 Top 1-err 15.334  Top 5-err 1.147	 Train Loss 0.492
* Epoch: [37/60]	 Top 1-err 30.270  Top 5-err 8.220	 Test Loss 1.158
* Epoch: [37/60]	 Top 1-err 30.270  Top 5-err 8.220	 Test Loss 1.158
69.73
0.6973
loss: 1.1579357958316803
(35, 0, 98) triplet: 0.03999999999999998
(35, 0): 0.315
(35, 98): 0.275
not enough sample
* Epoch: [38/60]	 Top 1-err 14.662  Top 5-err 1.133	 Train Loss 0.486
* Epoch: [38/60]	 Top 1-err 29.810  Top 5-err 8.000	 Test Loss 1.154
* Epoch: [38/60]	 Top 1-err 29.810  Top 5-err 8.000	 Test Loss 1.154
70.19
0.7019
loss: 1.1543715278625488
(35, 0, 98) triplet: 0.06499999999999997
(35, 0): 0.225
(35, 98): 0.29
* Epoch: [39/60]	 Top 1-err 14.218  Top 5-err 1.045	 Train Loss 0.470
* Epoch: [39/60]	 Top 1-err 29.880  Top 5-err 7.890	 Test Loss 1.159
* Epoch: [39/60]	 Top 1-err 29.880  Top 5-err 7.890	 Test Loss 1.159
70.12
0.7012
loss: 1.158884309387207
(35, 0, 98) triplet: 0.03999999999999998
(35, 0): 0.265
(35, 98): 0.305
not enough sample
* Epoch: [40/60]	 Top 1-err 14.116  Top 5-err 1.001	 Train Loss 0.457
* Epoch: [40/60]	 Top 1-err 30.240  Top 5-err 8.130	 Test Loss 1.174
* Epoch: [40/60]	 Top 1-err 30.240  Top 5-err 8.130	 Test Loss 1.174
69.76
0.6976
loss: 1.1739253509521483
(35, 0, 98) triplet: 0.014999999999999958
(35, 0): 0.29500000000000004
(35, 98): 0.31
* Epoch: [41/60]	 Top 1-err 13.536  Top 5-err 0.929	 Train Loss 0.447
* Epoch: [41/60]	 Top 1-err 30.100  Top 5-err 7.820	 Test Loss 1.169
* Epoch: [41/60]	 Top 1-err 30.100  Top 5-err 7.820	 Test Loss 1.169
69.9
0.699
loss: 1.1691112035751343
(35, 0, 98) triplet: 0.03500000000000003
(35, 0): 0.32
(35, 98): 0.35500000000000004
* Epoch: [42/60]	 Top 1-err 13.158  Top 5-err 0.880	 Train Loss 0.439
* Epoch: [42/60]	 Top 1-err 30.030  Top 5-err 8.310	 Test Loss 1.186
* Epoch: [42/60]	 Top 1-err 30.030  Top 5-err 8.310	 Test Loss 1.186
69.97
0.6997
loss: 1.185932686471939
(35, 0, 98) triplet: 0.23
(35, 0): 0.1
(35, 98): 0.33
* Epoch: [43/60]	 Top 1-err 13.228  Top 5-err 0.869	 Train Loss 0.431
* Epoch: [43/60]	 Top 1-err 29.940  Top 5-err 8.240	 Test Loss 1.186
* Epoch: [43/60]	 Top 1-err 29.940  Top 5-err 8.240	 Test Loss 1.186
70.06
0.7006
loss: 1.1858664831161498
(35, 0, 98) triplet: 0.155
(35, 0): 0.17
(35, 98): 0.325
* Epoch: [44/60]	 Top 1-err 13.119  Top 5-err 0.831	 Train Loss 0.425
* Epoch: [44/60]	 Top 1-err 30.130  Top 5-err 8.150	 Test Loss 1.203
* Epoch: [44/60]	 Top 1-err 30.130  Top 5-err 8.150	 Test Loss 1.203
69.87
0.6987
loss: 1.202621396636963
(35, 0, 98) triplet: 0.09
(35, 0): 0.32
(35, 98): 0.23
* Epoch: [45/60]	 Top 1-err 12.006  Top 5-err 0.757	 Train Loss 0.410
* Epoch: [45/60]	 Top 1-err 29.870  Top 5-err 8.120	 Test Loss 1.189
Current best accuracy (top-1 and 5 error): 29.87 8.12
saving best model...
* Epoch: [45/60]	 Top 1-err 29.870  Top 5-err 8.120	 Test Loss 1.189
70.13
0.7013
loss: 1.1894889818191527
(35, 0, 98) triplet: 0.004999999999999977
(35, 0): 0.23500000000000001
(35, 98): 0.24
* Epoch: [46/60]	 Top 1-err 12.017  Top 5-err 0.718	 Train Loss 0.404
* Epoch: [46/60]	 Top 1-err 29.900  Top 5-err 8.090	 Test Loss 1.186
Current best accuracy (top-1 and 5 error): 29.87 8.12
* Epoch: [46/60]	 Top 1-err 29.900  Top 5-err 8.090	 Test Loss 1.186
70.1
0.701
loss: 1.186166266822815
(35, 0, 98) triplet: 0.010000000000000009
(35, 0): 0.26
(35, 98): 0.27
not enough sample
* Epoch: [47/60]	 Top 1-err 11.870  Top 5-err 0.676	 Train Loss 0.399
* Epoch: [47/60]	 Top 1-err 29.910  Top 5-err 8.040	 Test Loss 1.185
Current best accuracy (top-1 and 5 error): 29.87 8.12
* Epoch: [47/60]	 Top 1-err 29.910  Top 5-err 8.040	 Test Loss 1.185
70.09
0.7009
loss: 1.1848627157211304
(35, 0, 98) triplet: 0.0
(35, 0): 0.28500000000000003
(35, 98): 0.28500000000000003
* Epoch: [48/60]	 Top 1-err 12.233  Top 5-err 0.727	 Train Loss 0.397
* Epoch: [48/60]	 Top 1-err 29.500  Top 5-err 8.070	 Test Loss 1.179
Current best accuracy (top-1 and 5 error): 29.5 8.07
saving best model...
* Epoch: [48/60]	 Top 1-err 29.500  Top 5-err 8.070	 Test Loss 1.179
70.5
0.705
loss: 1.1788613136291504
(35, 0, 98) triplet: 0.07500000000000004
(35, 0): 0.19999999999999998
(35, 98): 0.275
* Epoch: [49/60]	 Top 1-err 11.479  Top 5-err 0.737	 Train Loss 0.396
* Epoch: [49/60]	 Top 1-err 29.630  Top 5-err 8.030	 Test Loss 1.178
Current best accuracy (top-1 and 5 error): 29.5 8.07
* Epoch: [49/60]	 Top 1-err 29.630  Top 5-err 8.030	 Test Loss 1.178
70.37
0.7037
loss: 1.1780459754943848
(35, 0, 98) triplet: 0.09000000000000002
(35, 0): 0.2
(35, 98): 0.29000000000000004
* Epoch: [50/60]	 Top 1-err 11.434  Top 5-err 0.699	 Train Loss 0.393
* Epoch: [50/60]	 Top 1-err 29.750  Top 5-err 7.990	 Test Loss 1.183
Current best accuracy (top-1 and 5 error): 29.5 8.07
* Epoch: [50/60]	 Top 1-err 29.750  Top 5-err 7.990	 Test Loss 1.183
70.25
0.7025
loss: 1.1830036706924438
(35, 0, 98) triplet: 0.13499999999999998
(35, 0): 0.17500000000000002
(35, 98): 0.31
* Epoch: [51/60]	 Top 1-err 11.356  Top 5-err 0.705	 Train Loss 0.400
* Epoch: [51/60]	 Top 1-err 29.550  Top 5-err 8.090	 Test Loss 1.181
Current best accuracy (top-1 and 5 error): 29.5 8.07
* Epoch: [51/60]	 Top 1-err 29.550  Top 5-err 8.090	 Test Loss 1.181
70.45
0.7045
loss: 1.1813468349456786
(35, 0, 98) triplet: 0.12
(35, 0): 0.175
(35, 98): 0.295
* Epoch: [52/60]	 Top 1-err 11.252  Top 5-err 0.710	 Train Loss 0.389
* Epoch: [52/60]	 Top 1-err 29.630  Top 5-err 7.860	 Test Loss 1.182
Current best accuracy (top-1 and 5 error): 29.5 8.07
* Epoch: [52/60]	 Top 1-err 29.630  Top 5-err 7.860	 Test Loss 1.182
70.37
0.7037
loss: 1.1817348413467408
(35, 0, 98) triplet: 0.12
(35, 0): 0.19
(35, 98): 0.31
* Epoch: [53/60]	 Top 1-err 10.944  Top 5-err 0.716	 Train Loss 0.389
* Epoch: [53/60]	 Top 1-err 29.290  Top 5-err 8.060	 Test Loss 1.183
Current best accuracy (top-1 and 5 error): 29.29 8.06
saving best model...
* Epoch: [53/60]	 Top 1-err 29.290  Top 5-err 8.060	 Test Loss 1.183
70.71
0.7071
loss: 1.1833234493255615
(35, 0, 98) triplet: 0.17000000000000004
(35, 0): 0.13
(35, 98): 0.30000000000000004
* Epoch: [54/60]	 Top 1-err 10.637  Top 5-err 0.706	 Train Loss 0.390
* Epoch: [54/60]	 Top 1-err 29.270  Top 5-err 8.030	 Test Loss 1.180
Current best accuracy (top-1 and 5 error): 29.27 8.03
saving best model...
* Epoch: [54/60]	 Top 1-err 29.270  Top 5-err 8.030	 Test Loss 1.180
70.73
0.7073
loss: 1.179884171295166
(35, 0, 98) triplet: 0.135
(35, 0): 0.15500000000000003
(35, 98): 0.29000000000000004
* Epoch: [55/60]	 Top 1-err 10.595  Top 5-err 0.659	 Train Loss 0.389
* Epoch: [55/60]	 Top 1-err 29.390  Top 5-err 8.050	 Test Loss 1.184
Current best accuracy (top-1 and 5 error): 29.27 8.03
* Epoch: [55/60]	 Top 1-err 29.390  Top 5-err 8.050	 Test Loss 1.184
70.61
0.7061
loss: 1.1836485048294068
(35, 0, 98) triplet: 0.17500000000000002
(35, 0): 0.095
(35, 98): 0.27
* Epoch: [56/60]	 Top 1-err 10.382  Top 5-err 0.674	 Train Loss 0.389
* Epoch: [56/60]	 Top 1-err 29.370  Top 5-err 7.950	 Test Loss 1.181
Current best accuracy (top-1 and 5 error): 29.27 8.03
* Epoch: [56/60]	 Top 1-err 29.370  Top 5-err 7.950	 Test Loss 1.181
70.63
0.7063
loss: 1.1813118648529053
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.14
(35, 98): 0.305
* Epoch: [57/60]	 Top 1-err 10.247  Top 5-err 0.706	 Train Loss 0.385
* Epoch: [57/60]	 Top 1-err 29.180  Top 5-err 8.010	 Test Loss 1.186
Current best accuracy (top-1 and 5 error): 29.18 8.01
saving best model...
* Epoch: [57/60]	 Top 1-err 29.180  Top 5-err 8.010	 Test Loss 1.186
70.82
0.7082
loss: 1.1860602010726928
(35, 0, 98) triplet: 0.21500000000000002
(35, 0): 0.045
(35, 98): 0.26
not enough sample
* Epoch: [58/60]	 Top 1-err 10.198  Top 5-err 0.689	 Train Loss 0.390
* Epoch: [58/60]	 Top 1-err 29.210  Top 5-err 7.940	 Test Loss 1.183
Current best accuracy (top-1 and 5 error): 29.18 8.01
* Epoch: [58/60]	 Top 1-err 29.210  Top 5-err 7.940	 Test Loss 1.183
70.79
0.7079
loss: 1.1833912115097045
(35, 0, 98) triplet: 0.21500000000000002
(35, 0): 0.060000000000000005
(35, 98): 0.275
* Epoch: [59/60]	 Top 1-err 10.323  Top 5-err 0.689	 Train Loss 0.384
* Epoch: [59/60]	 Top 1-err 29.420  Top 5-err 7.940	 Test Loss 1.186
Current best accuracy (top-1 and 5 error): 29.18 8.01
* Epoch: [59/60]	 Top 1-err 29.420  Top 5-err 7.940	 Test Loss 1.186
70.58
0.7058
loss: 1.1856159446716308
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.14
(35, 98): 0.305
Best accuracy (top-1 and 5 error): 29.18 8.01
=> loading checkpoint 'runs/cifar100_resnet_bias/model_best.pth.tar'
* Epoch: [-1/60]	 Top 1-err 29.180  Top 5-err 8.010	 Test Loss 1.186
70.82
0.7082
loss: 1.1860602033615113
(35, 0, 98) triplet: 0.21500000000000002
(35, 0): 0.045
(35, 98): 0.26
python3 cifar100_repair_bias.py --net_type resnet --dataset cifar100 --depth 50 --batch_size 256 --lr 0.1 --expname cifar100_resnet_bias --epochs 60 --beta 1.0 --cutmix_prob 1 --pretrained ./runs/cifar100_resnet_2/model_best.pth.tar --expid 0 --first 35 --second 0 --third 98 --lam 0
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
=> creating model 'resnet'
True
=> loading checkpoint './runs/cifar100_resnet_2/model_best.pth.tar'
=> loaded checkpoint './runs/cifar100_resnet_2/model_best.pth.tar'
the number of model parameters: 521716
* Epoch: [0/60]	 Top 1-err 53.784  Top 5-err 29.299	 Train Loss 2.611
* Epoch: [0/60]	 Top 1-err 36.040  Top 5-err 12.020	 Test Loss 1.332
* Epoch: [0/60]	 Top 1-err 36.040  Top 5-err 12.020	 Test Loss 1.332
63.96
0.6396
loss: 1.3324708437919617
(35, 0, 98) triplet: 0.195
(35, 0): 0.01
(35, 98): 0.20500000000000002
* Epoch: [1/60]	 Top 1-err 54.150  Top 5-err 30.170	 Train Loss 2.477
* Epoch: [1/60]	 Top 1-err 35.170  Top 5-err 11.250	 Test Loss 1.278
* Epoch: [1/60]	 Top 1-err 35.170  Top 5-err 11.250	 Test Loss 1.278
64.83
0.6483
loss: 1.277812872505188
(35, 0, 98) triplet: 0.20500000000000002
(35, 0): 0.01
(35, 98): 0.21500000000000002
* Epoch: [2/60]	 Top 1-err 50.334  Top 5-err 25.965	 Train Loss 2.358
* Epoch: [2/60]	 Top 1-err 34.320  Top 5-err 10.460	 Test Loss 1.241
* Epoch: [2/60]	 Top 1-err 34.320  Top 5-err 10.460	 Test Loss 1.241
65.68
0.6568
loss: 1.240634801673889
(35, 0, 98) triplet: 0.25
(35, 0): 0.0
(35, 98): 0.25
* Epoch: [3/60]	 Top 1-err 50.954  Top 5-err 26.420	 Train Loss 2.359
* Epoch: [3/60]	 Top 1-err 35.460  Top 5-err 11.220	 Test Loss 1.316
* Epoch: [3/60]	 Top 1-err 35.460  Top 5-err 11.220	 Test Loss 1.316
64.54
0.6454
loss: 1.3158764678955077
(35, 0, 98) triplet: 0.225
(35, 0): 0.0
(35, 98): 0.225
not enough sample
not enough sample
* Epoch: [4/60]	 Top 1-err 51.742  Top 5-err 26.468	 Train Loss 2.412
* Epoch: [4/60]	 Top 1-err 36.190  Top 5-err 11.620	 Test Loss 1.350
* Epoch: [4/60]	 Top 1-err 36.190  Top 5-err 11.620	 Test Loss 1.350
63.81
0.6381
loss: 1.3502549701690674
(35, 0, 98) triplet: 0.245
(35, 0): 0.0
(35, 98): 0.245
* Epoch: [5/60]	 Top 1-err 46.679  Top 5-err 22.529	 Train Loss 2.266
* Epoch: [5/60]	 Top 1-err 36.160  Top 5-err 11.600	 Test Loss 1.326
* Epoch: [5/60]	 Top 1-err 36.160  Top 5-err 11.600	 Test Loss 1.326
63.84
0.6384
loss: 1.3263843276977538
(35, 0, 98) triplet: 0.255
(35, 0): 0.005
(35, 98): 0.26
* Epoch: [6/60]	 Top 1-err 48.308  Top 5-err 23.759	 Train Loss 2.310
* Epoch: [6/60]	 Top 1-err 33.910  Top 5-err 10.210	 Test Loss 1.256
* Epoch: [6/60]	 Top 1-err 33.910  Top 5-err 10.210	 Test Loss 1.256
66.09
0.6609
loss: 1.2560701038360595
(35, 0, 98) triplet: 0.2
(35, 0): 0.0
(35, 98): 0.2
* Epoch: [7/60]	 Top 1-err 46.868  Top 5-err 23.457	 Train Loss 2.200
* Epoch: [7/60]	 Top 1-err 34.820  Top 5-err 11.480	 Test Loss 1.319
* Epoch: [7/60]	 Top 1-err 34.820  Top 5-err 11.480	 Test Loss 1.319
65.18
0.6518
loss: 1.3187829444885253
(35, 0, 98) triplet: 0.225
(35, 0): 0.0
(35, 98): 0.225
* Epoch: [8/60]	 Top 1-err 48.175  Top 5-err 24.173	 Train Loss 2.222
* Epoch: [8/60]	 Top 1-err 34.100  Top 5-err 10.660	 Test Loss 1.255
* Epoch: [8/60]	 Top 1-err 34.100  Top 5-err 10.660	 Test Loss 1.255
65.9
0.659
loss: 1.2554538192749023
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.0
(35, 98): 0.16499999999999998
* Epoch: [9/60]	 Top 1-err 48.997  Top 5-err 24.702	 Train Loss 2.285
* Epoch: [9/60]	 Top 1-err 34.410  Top 5-err 10.100	 Test Loss 1.242
* Epoch: [9/60]	 Top 1-err 34.410  Top 5-err 10.100	 Test Loss 1.242
65.59
0.6559
loss: 1.241850382232666
(35, 0, 98) triplet: 0.20500000000000002
(35, 0): 0.0
(35, 98): 0.20500000000000002
* Epoch: [10/60]	 Top 1-err 48.852  Top 5-err 24.224	 Train Loss 2.291
* Epoch: [10/60]	 Top 1-err 34.570  Top 5-err 10.600	 Test Loss 1.296
* Epoch: [10/60]	 Top 1-err 34.570  Top 5-err 10.600	 Test Loss 1.296
65.43
0.6543
loss: 1.2959075689315795
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.0
(35, 98): 0.16499999999999998
* Epoch: [11/60]	 Top 1-err 49.822  Top 5-err 24.377	 Train Loss 2.343
* Epoch: [11/60]	 Top 1-err 34.580  Top 5-err 10.610	 Test Loss 1.280
* Epoch: [11/60]	 Top 1-err 34.580  Top 5-err 10.610	 Test Loss 1.280
65.42
0.6542
loss: 1.2799382923126221
(35, 0, 98) triplet: 0.185
(35, 0): 0.0
(35, 98): 0.185
not enough sample
* Epoch: [12/60]	 Top 1-err 50.720  Top 5-err 25.854	 Train Loss 2.337
* Epoch: [12/60]	 Top 1-err 35.680  Top 5-err 11.560	 Test Loss 1.311
* Epoch: [12/60]	 Top 1-err 35.680  Top 5-err 11.560	 Test Loss 1.311
64.32
0.6432
loss: 1.3109086902618408
(35, 0, 98) triplet: 0.2
(35, 0): 0.0
(35, 98): 0.2
* Epoch: [13/60]	 Top 1-err 49.192  Top 5-err 24.054	 Train Loss 2.307
* Epoch: [13/60]	 Top 1-err 35.310  Top 5-err 11.220	 Test Loss 1.321
* Epoch: [13/60]	 Top 1-err 35.310  Top 5-err 11.220	 Test Loss 1.321
64.69
0.6469
loss: 1.3213845565795899
(35, 0, 98) triplet: 0.28500000000000003
(35, 0): 0.005
(35, 98): 0.29000000000000004
* Epoch: [14/60]	 Top 1-err 49.212  Top 5-err 24.116	 Train Loss 2.313
* Epoch: [14/60]	 Top 1-err 34.590  Top 5-err 10.650	 Test Loss 1.289
* Epoch: [14/60]	 Top 1-err 34.590  Top 5-err 10.650	 Test Loss 1.289
65.41
0.6541
loss: 1.2893037338256836
(35, 0, 98) triplet: 0.2
(35, 0): 0.005
(35, 98): 0.20500000000000002
* Epoch: [15/60]	 Top 1-err 53.026  Top 5-err 27.221	 Train Loss 2.367
* Epoch: [15/60]	 Top 1-err 36.580  Top 5-err 12.050	 Test Loss 1.374
* Epoch: [15/60]	 Top 1-err 36.580  Top 5-err 12.050	 Test Loss 1.374
63.42
0.6342
loss: 1.3742629350662232
(35, 0, 98) triplet: 0.175
(35, 0): 0.0
(35, 98): 0.175
* Epoch: [16/60]	 Top 1-err 48.410  Top 5-err 22.512	 Train Loss 2.287
* Epoch: [16/60]	 Top 1-err 34.360  Top 5-err 10.760	 Test Loss 1.305
* Epoch: [16/60]	 Top 1-err 34.360  Top 5-err 10.760	 Test Loss 1.305
65.64
0.6564
loss: 1.3047135956764222
(35, 0, 98) triplet: 0.21
(35, 0): 0.0
(35, 98): 0.21
not enough sample
* Epoch: [17/60]	 Top 1-err 49.668  Top 5-err 24.669	 Train Loss 2.254
* Epoch: [17/60]	 Top 1-err 36.150  Top 5-err 12.310	 Test Loss 1.353
* Epoch: [17/60]	 Top 1-err 36.150  Top 5-err 12.310	 Test Loss 1.353
63.85
0.6385
loss: 1.3533804756164551
(35, 0, 98) triplet: 0.245
(35, 0): 0.0
(35, 98): 0.245
* Epoch: [18/60]	 Top 1-err 50.727  Top 5-err 24.690	 Train Loss 2.299
* Epoch: [18/60]	 Top 1-err 35.090  Top 5-err 10.560	 Test Loss 1.311
* Epoch: [18/60]	 Top 1-err 35.090  Top 5-err 10.560	 Test Loss 1.311
64.91
0.6491
loss: 1.3112174520492554
(35, 0, 98) triplet: 0.165
(35, 0): 0.0
(35, 98): 0.165
* Epoch: [19/60]	 Top 1-err 49.050  Top 5-err 24.358	 Train Loss 2.227
* Epoch: [19/60]	 Top 1-err 34.720  Top 5-err 10.680	 Test Loss 1.314
* Epoch: [19/60]	 Top 1-err 34.720  Top 5-err 10.680	 Test Loss 1.314
65.28
0.6528
loss: 1.3139698266983033
(35, 0, 98) triplet: 0.26
(35, 0): 0.0
(35, 98): 0.26
* Epoch: [20/60]	 Top 1-err 45.967  Top 5-err 21.028	 Train Loss 2.169
* Epoch: [20/60]	 Top 1-err 33.230  Top 5-err 9.430	 Test Loss 1.201
* Epoch: [20/60]	 Top 1-err 33.230  Top 5-err 9.430	 Test Loss 1.201
66.77
0.6677
loss: 1.200689882850647
(35, 0, 98) triplet: 0.22999999999999998
(35, 0): 0.0
(35, 98): 0.22999999999999998
* Epoch: [21/60]	 Top 1-err 50.508  Top 5-err 25.015	 Train Loss 2.271
* Epoch: [21/60]	 Top 1-err 34.540  Top 5-err 10.850	 Test Loss 1.285
* Epoch: [21/60]	 Top 1-err 34.540  Top 5-err 10.850	 Test Loss 1.285
65.46
0.6546
loss: 1.284637099456787
(35, 0, 98) triplet: 0.185
(35, 0): 0.0
(35, 98): 0.185
not enough sample
* Epoch: [22/60]	 Top 1-err 45.663  Top 5-err 20.323	 Train Loss 2.217
* Epoch: [22/60]	 Top 1-err 36.000  Top 5-err 11.020	 Test Loss 1.309
* Epoch: [22/60]	 Top 1-err 36.000  Top 5-err 11.020	 Test Loss 1.309
64.0
0.64
loss: 1.3091368392944336
(35, 0, 98) triplet: 0.21000000000000002
(35, 0): 0.0
(35, 98): 0.21000000000000002
* Epoch: [23/60]	 Top 1-err 48.502  Top 5-err 22.960	 Train Loss 2.231
* Epoch: [23/60]	 Top 1-err 34.260  Top 5-err 10.240	 Test Loss 1.271
* Epoch: [23/60]	 Top 1-err 34.260  Top 5-err 10.240	 Test Loss 1.271
65.74
0.6574
loss: 1.270938039779663
(35, 0, 98) triplet: 0.185
(35, 0): 0.0
(35, 98): 0.185
* Epoch: [24/60]	 Top 1-err 48.413  Top 5-err 22.969	 Train Loss 2.254
* Epoch: [24/60]	 Top 1-err 34.430  Top 5-err 10.470	 Test Loss 1.263
* Epoch: [24/60]	 Top 1-err 34.430  Top 5-err 10.470	 Test Loss 1.263
65.57
0.6557
loss: 1.2626363639831544
(35, 0, 98) triplet: 0.29500000000000004
(35, 0): 0.0
(35, 98): 0.29500000000000004
* Epoch: [25/60]	 Top 1-err 47.833  Top 5-err 21.932	 Train Loss 2.257
* Epoch: [25/60]	 Top 1-err 38.370  Top 5-err 12.540	 Test Loss 1.419
* Epoch: [25/60]	 Top 1-err 38.370  Top 5-err 12.540	 Test Loss 1.419
61.63
0.6163
loss: 1.4187790807724
(35, 0, 98) triplet: 0.26
(35, 0): 0.0
(35, 98): 0.26
* Epoch: [26/60]	 Top 1-err 48.070  Top 5-err 21.649	 Train Loss 2.269
* Epoch: [26/60]	 Top 1-err 35.010  Top 5-err 10.620	 Test Loss 1.288
* Epoch: [26/60]	 Top 1-err 35.010  Top 5-err 10.620	 Test Loss 1.288
64.99
0.6499
loss: 1.2884168840408325
(35, 0, 98) triplet: 0.33499999999999996
(35, 0): 0.0
(35, 98): 0.33499999999999996
* Epoch: [27/60]	 Top 1-err 48.731  Top 5-err 22.779	 Train Loss 2.275
* Epoch: [27/60]	 Top 1-err 35.970  Top 5-err 11.640	 Test Loss 1.361
* Epoch: [27/60]	 Top 1-err 35.970  Top 5-err 11.640	 Test Loss 1.361
64.03
0.6403
loss: 1.3608404090881348
(35, 0, 98) triplet: 0.18
(35, 0): 0.0
(35, 98): 0.18
* Epoch: [28/60]	 Top 1-err 49.343  Top 5-err 23.965	 Train Loss 2.264
* Epoch: [28/60]	 Top 1-err 32.590  Top 5-err 9.710	 Test Loss 1.218
* Epoch: [28/60]	 Top 1-err 32.590  Top 5-err 9.710	 Test Loss 1.218
67.41
0.6741
loss: 1.2184991813659667
(35, 0, 98) triplet: 0.185
(35, 0): 0.0
(35, 98): 0.185
* Epoch: [29/60]	 Top 1-err 48.287  Top 5-err 22.291	 Train Loss 2.245
* Epoch: [29/60]	 Top 1-err 35.220  Top 5-err 11.260	 Test Loss 1.321
* Epoch: [29/60]	 Top 1-err 35.220  Top 5-err 11.260	 Test Loss 1.321
64.78
0.6478
loss: 1.3213952480316162
(35, 0, 98) triplet: 0.18
(35, 0): 0.0
(35, 98): 0.18
* Epoch: [30/60]	 Top 1-err 45.395  Top 5-err 20.812	 Train Loss 2.135
* Epoch: [30/60]	 Top 1-err 28.190  Top 5-err 7.140	 Test Loss 1.047
* Epoch: [30/60]	 Top 1-err 28.190  Top 5-err 7.140	 Test Loss 1.047
71.81
0.7181
loss: 1.0469912782669066
(35, 0, 98) triplet: 0.15000000000000002
(35, 0): 0.0
(35, 98): 0.15000000000000002
* Epoch: [31/60]	 Top 1-err 43.804  Top 5-err 20.264	 Train Loss 2.051
* Epoch: [31/60]	 Top 1-err 27.770  Top 5-err 7.430	 Test Loss 1.044
* Epoch: [31/60]	 Top 1-err 27.770  Top 5-err 7.430	 Test Loss 1.044
72.23
0.7223
loss: 1.0435288246154786
(35, 0, 98) triplet: 0.17
(35, 0): 0.0
(35, 98): 0.17
* Epoch: [32/60]	 Top 1-err 40.746  Top 5-err 17.575	 Train Loss 2.050
* Epoch: [32/60]	 Top 1-err 27.720  Top 5-err 7.170	 Test Loss 1.058
* Epoch: [32/60]	 Top 1-err 27.720  Top 5-err 7.170	 Test Loss 1.058
72.28
0.7228
loss: 1.0576865606307984
(35, 0, 98) triplet: 0.165
(35, 0): 0.0
(35, 98): 0.165
not enough sample
* Epoch: [33/60]	 Top 1-err 44.363  Top 5-err 19.498	 Train Loss 2.087
* Epoch: [33/60]	 Top 1-err 27.630  Top 5-err 7.430	 Test Loss 1.053
* Epoch: [33/60]	 Top 1-err 27.630  Top 5-err 7.430	 Test Loss 1.053
72.37
0.7237
loss: 1.052665799999237
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [34/60]	 Top 1-err 40.780  Top 5-err 17.512	 Train Loss 2.012
* Epoch: [34/60]	 Top 1-err 27.820  Top 5-err 7.500	 Test Loss 1.052
* Epoch: [34/60]	 Top 1-err 27.820  Top 5-err 7.500	 Test Loss 1.052
72.18
0.7218
loss: 1.0523095363616943
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [35/60]	 Top 1-err 43.938  Top 5-err 19.770	 Train Loss 2.080
* Epoch: [35/60]	 Top 1-err 27.540  Top 5-err 7.300	 Test Loss 1.038
* Epoch: [35/60]	 Top 1-err 27.540  Top 5-err 7.300	 Test Loss 1.038
72.46
0.7246
loss: 1.037500238800049
(35, 0, 98) triplet: 0.175
(35, 0): 0.0
(35, 98): 0.175
* Epoch: [36/60]	 Top 1-err 39.994  Top 5-err 17.008	 Train Loss 1.986
* Epoch: [36/60]	 Top 1-err 27.850  Top 5-err 7.320	 Test Loss 1.056
* Epoch: [36/60]	 Top 1-err 27.850  Top 5-err 7.320	 Test Loss 1.056
72.15
0.7215
loss: 1.0559936519622803
(35, 0, 98) triplet: 0.165
(35, 0): 0.0
(35, 98): 0.165
not enough sample
* Epoch: [37/60]	 Top 1-err 38.980  Top 5-err 16.462	 Train Loss 1.965
* Epoch: [37/60]	 Top 1-err 26.940  Top 5-err 7.060	 Test Loss 1.007
* Epoch: [37/60]	 Top 1-err 26.940  Top 5-err 7.060	 Test Loss 1.007
73.06
0.7306
loss: 1.0067560749053954
(35, 0, 98) triplet: 0.13
(35, 0): 0.0
(35, 98): 0.13
* Epoch: [38/60]	 Top 1-err 41.564  Top 5-err 18.017	 Train Loss 2.023
* Epoch: [38/60]	 Top 1-err 27.430  Top 5-err 7.190	 Test Loss 1.026
* Epoch: [38/60]	 Top 1-err 27.430  Top 5-err 7.190	 Test Loss 1.026
72.57
0.7257
loss: 1.026096866607666
(35, 0, 98) triplet: 0.175
(35, 0): 0.0
(35, 98): 0.175
* Epoch: [39/60]	 Top 1-err 43.646  Top 5-err 19.001	 Train Loss 2.077
* Epoch: [39/60]	 Top 1-err 27.410  Top 5-err 7.010	 Test Loss 1.033
* Epoch: [39/60]	 Top 1-err 27.410  Top 5-err 7.010	 Test Loss 1.033
72.59
0.7259
loss: 1.0334886966705323
(35, 0, 98) triplet: 0.165
(35, 0): 0.0
(35, 98): 0.165
not enough sample
* Epoch: [40/60]	 Top 1-err 42.066  Top 5-err 18.423	 Train Loss 2.040
* Epoch: [40/60]	 Top 1-err 27.900  Top 5-err 7.370	 Test Loss 1.057
* Epoch: [40/60]	 Top 1-err 27.900  Top 5-err 7.370	 Test Loss 1.057
72.1
0.721
loss: 1.057251096343994
(35, 0, 98) triplet: 0.18
(35, 0): 0.0
(35, 98): 0.18
* Epoch: [41/60]	 Top 1-err 41.266  Top 5-err 17.486	 Train Loss 2.018
* Epoch: [41/60]	 Top 1-err 27.030  Top 5-err 7.080	 Test Loss 1.009
* Epoch: [41/60]	 Top 1-err 27.030  Top 5-err 7.080	 Test Loss 1.009
72.97
0.7297
loss: 1.009318550491333
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [42/60]	 Top 1-err 40.351  Top 5-err 17.975	 Train Loss 1.981
* Epoch: [42/60]	 Top 1-err 27.420  Top 5-err 7.140	 Test Loss 1.029
* Epoch: [42/60]	 Top 1-err 27.420  Top 5-err 7.140	 Test Loss 1.029
72.58
0.7258
loss: 1.029376798057556
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [43/60]	 Top 1-err 40.553  Top 5-err 17.342	 Train Loss 1.983
* Epoch: [43/60]	 Top 1-err 27.370  Top 5-err 7.170	 Test Loss 1.028
* Epoch: [43/60]	 Top 1-err 27.370  Top 5-err 7.170	 Test Loss 1.028
72.63
0.7263
loss: 1.0281881080627442
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [44/60]	 Top 1-err 42.025  Top 5-err 18.742	 Train Loss 1.989
* Epoch: [44/60]	 Top 1-err 27.390  Top 5-err 7.350	 Test Loss 1.031
* Epoch: [44/60]	 Top 1-err 27.390  Top 5-err 7.350	 Test Loss 1.031
72.61
0.7261
loss: 1.0313370697021484
(35, 0, 98) triplet: 0.175
(35, 0): 0.0
(35, 98): 0.175
not enough sample
* Epoch: [45/60]	 Top 1-err 38.710  Top 5-err 16.487	 Train Loss 1.959
* Epoch: [45/60]	 Top 1-err 27.110  Top 5-err 7.210	 Test Loss 1.035
Current best accuracy (top-1 and 5 error): 27.11 7.21
saving best model...
* Epoch: [45/60]	 Top 1-err 27.110  Top 5-err 7.210	 Test Loss 1.035
72.89
0.7289
loss: 1.0353803382873534
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [46/60]	 Top 1-err 40.094  Top 5-err 17.713	 Train Loss 1.934
* Epoch: [46/60]	 Top 1-err 27.170  Top 5-err 7.110	 Test Loss 1.024
Current best accuracy (top-1 and 5 error): 27.11 7.21
* Epoch: [46/60]	 Top 1-err 27.170  Top 5-err 7.110	 Test Loss 1.024
72.83
0.7283
loss: 1.0235940418243408
(35, 0, 98) triplet: 0.145
(35, 0): 0.0
(35, 98): 0.145
* Epoch: [47/60]	 Top 1-err 42.939  Top 5-err 19.141	 Train Loss 2.017
* Epoch: [47/60]	 Top 1-err 26.980  Top 5-err 7.070	 Test Loss 1.014
Current best accuracy (top-1 and 5 error): 26.98 7.07
saving best model...
* Epoch: [47/60]	 Top 1-err 26.980  Top 5-err 7.070	 Test Loss 1.014
73.02
0.7302
loss: 1.013540325832367
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [48/60]	 Top 1-err 39.073  Top 5-err 16.664	 Train Loss 1.951
* Epoch: [48/60]	 Top 1-err 27.020  Top 5-err 7.080	 Test Loss 1.025
Current best accuracy (top-1 and 5 error): 26.98 7.07
* Epoch: [48/60]	 Top 1-err 27.020  Top 5-err 7.080	 Test Loss 1.025
72.98
0.7298
loss: 1.0245085611343383
(35, 0, 98) triplet: 0.165
(35, 0): 0.0
(35, 98): 0.165
* Epoch: [49/60]	 Top 1-err 40.538  Top 5-err 18.513	 Train Loss 1.917
* Epoch: [49/60]	 Top 1-err 26.970  Top 5-err 7.090	 Test Loss 1.016
Current best accuracy (top-1 and 5 error): 26.97 7.09
saving best model...
* Epoch: [49/60]	 Top 1-err 26.970  Top 5-err 7.090	 Test Loss 1.016
73.03
0.7303
loss: 1.0156333148956298
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
* Epoch: [50/60]	 Top 1-err 39.828  Top 5-err 16.949	 Train Loss 1.963
* Epoch: [50/60]	 Top 1-err 26.800  Top 5-err 6.970	 Test Loss 1.019
Current best accuracy (top-1 and 5 error): 26.8 6.97
saving best model...
* Epoch: [50/60]	 Top 1-err 26.800  Top 5-err 6.970	 Test Loss 1.019
73.2
0.732
loss: 1.0190973882675172
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [51/60]	 Top 1-err 43.523  Top 5-err 19.390	 Train Loss 2.042
* Epoch: [51/60]	 Top 1-err 26.990  Top 5-err 6.990	 Test Loss 1.024
Current best accuracy (top-1 and 5 error): 26.8 6.97
* Epoch: [51/60]	 Top 1-err 26.990  Top 5-err 6.990	 Test Loss 1.024
73.01
0.7301
loss: 1.0237347017288208
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [52/60]	 Top 1-err 40.062  Top 5-err 16.768	 Train Loss 1.987
* Epoch: [52/60]	 Top 1-err 26.790  Top 5-err 7.010	 Test Loss 1.016
Current best accuracy (top-1 and 5 error): 26.79 7.01
saving best model...
* Epoch: [52/60]	 Top 1-err 26.790  Top 5-err 7.010	 Test Loss 1.016
73.21
0.7321
loss: 1.0157935081481935
(35, 0, 98) triplet: 0.15000000000000002
(35, 0): 0.0
(35, 98): 0.15000000000000002
* Epoch: [53/60]	 Top 1-err 37.201  Top 5-err 14.751	 Train Loss 1.942
* Epoch: [53/60]	 Top 1-err 26.860  Top 5-err 6.950	 Test Loss 1.011
Current best accuracy (top-1 and 5 error): 26.79 7.01
* Epoch: [53/60]	 Top 1-err 26.860  Top 5-err 6.950	 Test Loss 1.011
73.14
0.7314
loss: 1.0110218561172486
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
not enough sample
* Epoch: [54/60]	 Top 1-err 39.569  Top 5-err 16.670	 Train Loss 1.987
* Epoch: [54/60]	 Top 1-err 26.990  Top 5-err 7.130	 Test Loss 1.044
Current best accuracy (top-1 and 5 error): 26.79 7.01
* Epoch: [54/60]	 Top 1-err 26.990  Top 5-err 7.130	 Test Loss 1.044
73.01
0.7301
loss: 1.0435832593917846
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [55/60]	 Top 1-err 39.921  Top 5-err 17.261	 Train Loss 1.939
* Epoch: [55/60]	 Top 1-err 26.720  Top 5-err 6.900	 Test Loss 1.002
Current best accuracy (top-1 and 5 error): 26.72 6.9
saving best model...
* Epoch: [55/60]	 Top 1-err 26.720  Top 5-err 6.900	 Test Loss 1.002
73.28
0.7328
loss: 1.0019364861488342
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
not enough sample
* Epoch: [56/60]	 Top 1-err 38.938  Top 5-err 15.937	 Train Loss 1.973
* Epoch: [56/60]	 Top 1-err 26.820  Top 5-err 7.050	 Test Loss 1.025
Current best accuracy (top-1 and 5 error): 26.72 6.9
* Epoch: [56/60]	 Top 1-err 26.820  Top 5-err 7.050	 Test Loss 1.025
73.18
0.7318
loss: 1.025296972846985
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [57/60]	 Top 1-err 40.531  Top 5-err 17.286	 Train Loss 1.993
* Epoch: [57/60]	 Top 1-err 26.480  Top 5-err 6.940	 Test Loss 1.000
Current best accuracy (top-1 and 5 error): 26.48 6.94
saving best model...
* Epoch: [57/60]	 Top 1-err 26.480  Top 5-err 6.940	 Test Loss 1.000
73.52
0.7352
loss: 0.9998837996482849
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
* Epoch: [58/60]	 Top 1-err 39.804  Top 5-err 16.968	 Train Loss 1.919
* Epoch: [58/60]	 Top 1-err 26.560  Top 5-err 6.900	 Test Loss 0.995
Current best accuracy (top-1 and 5 error): 26.48 6.94
* Epoch: [58/60]	 Top 1-err 26.560  Top 5-err 6.900	 Test Loss 0.995
73.44
0.7344
loss: 0.9953607692718506
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [59/60]	 Top 1-err 38.342  Top 5-err 16.056	 Train Loss 1.912
* Epoch: [59/60]	 Top 1-err 26.840  Top 5-err 6.940	 Test Loss 1.016
Current best accuracy (top-1 and 5 error): 26.48 6.94
* Epoch: [59/60]	 Top 1-err 26.840  Top 5-err 6.940	 Test Loss 1.016
73.16
0.7316
loss: 1.0155686281204224
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
Best accuracy (top-1 and 5 error): 26.48 6.94
=> loading checkpoint 'runs/cifar100_resnet_bias/model_best.pth.tar'
* Epoch: [-1/60]	 Top 1-err 26.480  Top 5-err 6.940	 Test Loss 1.000
73.52
0.7352
loss: 0.999883801651001
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
python3 cifar100_repair_bias.py --net_type resnet --dataset cifar100 --depth 50 --batch_size 256 --lr 0.1 --expname cifar100_resnet_bias --epochs 60 --beta 1.0 --cutmix_prob 1 --pretrained ./runs/cifar100_resnet_2/model_best.pth.tar --expid 0 --first 35 --second 0 --third 98 --lam 0.5
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
=> creating model 'resnet'
True
=> loading checkpoint './runs/cifar100_resnet_2/model_best.pth.tar'
=> loaded checkpoint './runs/cifar100_resnet_2/model_best.pth.tar'
the number of model parameters: 521716
* Epoch: [0/60]	 Top 1-err 57.932  Top 5-err 32.206	 Train Loss 2.806
* Epoch: [0/60]	 Top 1-err 39.970  Top 5-err 14.440	 Test Loss 1.494
* Epoch: [0/60]	 Top 1-err 39.970  Top 5-err 14.440	 Test Loss 1.494
60.03
0.6003
loss: 1.4938073635101319
(35, 0, 98) triplet: 0.375
(35, 0): 0.0
(35, 98): 0.375
* Epoch: [1/60]	 Top 1-err 57.227  Top 5-err 32.338	 Train Loss 2.621
* Epoch: [1/60]	 Top 1-err 35.990  Top 5-err 11.650	 Test Loss 1.334
* Epoch: [1/60]	 Top 1-err 35.990  Top 5-err 11.650	 Test Loss 1.334
64.01
0.6401
loss: 1.333848210144043
(35, 0, 98) triplet: 0.125
(35, 0): 0.03
(35, 98): 0.155
* Epoch: [2/60]	 Top 1-err 53.398  Top 5-err 27.871	 Train Loss 2.504
* Epoch: [2/60]	 Top 1-err 35.330  Top 5-err 11.730	 Test Loss 1.316
* Epoch: [2/60]	 Top 1-err 35.330  Top 5-err 11.730	 Test Loss 1.316
64.67
0.6467
loss: 1.3162130962371825
(35, 0, 98) triplet: 0.19
(35, 0): 0.005
(35, 98): 0.195
* Epoch: [3/60]	 Top 1-err 53.940  Top 5-err 28.672	 Train Loss 2.507
* Epoch: [3/60]	 Top 1-err 38.300  Top 5-err 13.270	 Test Loss 1.459
* Epoch: [3/60]	 Top 1-err 38.300  Top 5-err 13.270	 Test Loss 1.459
61.7
0.617
loss: 1.459044532775879
(35, 0, 98) triplet: 0.225
(35, 0): 0.02
(35, 98): 0.245
not enough sample
not enough sample
* Epoch: [4/60]	 Top 1-err 54.518  Top 5-err 28.589	 Train Loss 2.554
* Epoch: [4/60]	 Top 1-err 37.940  Top 5-err 12.860	 Test Loss 1.438
* Epoch: [4/60]	 Top 1-err 37.940  Top 5-err 12.860	 Test Loss 1.438
62.06
0.6206
loss: 1.4380963344573974
(35, 0, 98) triplet: 0.15999999999999998
(35, 0): 0.045
(35, 98): 0.205
* Epoch: [5/60]	 Top 1-err 49.715  Top 5-err 24.352	 Train Loss 2.400
* Epoch: [5/60]	 Top 1-err 38.430  Top 5-err 13.120	 Test Loss 1.435
* Epoch: [5/60]	 Top 1-err 38.430  Top 5-err 13.120	 Test Loss 1.435
61.57
0.6157
loss: 1.4348655025482178
(35, 0, 98) triplet: 0.09000000000000001
(35, 0): 0.05
(35, 98): 0.14
* Epoch: [6/60]	 Top 1-err 51.014  Top 5-err 25.695	 Train Loss 2.447
* Epoch: [6/60]	 Top 1-err 36.170  Top 5-err 11.690	 Test Loss 1.352
* Epoch: [6/60]	 Top 1-err 36.170  Top 5-err 11.690	 Test Loss 1.352
63.83
0.6383
loss: 1.352306332397461
(35, 0, 98) triplet: 0.14
(35, 0): 0.025
(35, 98): 0.165
* Epoch: [7/60]	 Top 1-err 50.210  Top 5-err 25.491	 Train Loss 2.346
* Epoch: [7/60]	 Top 1-err 35.840  Top 5-err 11.990	 Test Loss 1.348
* Epoch: [7/60]	 Top 1-err 35.840  Top 5-err 11.990	 Test Loss 1.348
64.16
0.6416
loss: 1.3480406547546386
(35, 0, 98) triplet: 0.22000000000000003
(35, 0): 0.015
(35, 98): 0.23500000000000001
* Epoch: [8/60]	 Top 1-err 51.126  Top 5-err 26.158	 Train Loss 2.370
* Epoch: [8/60]	 Top 1-err 35.000  Top 5-err 11.610	 Test Loss 1.309
* Epoch: [8/60]	 Top 1-err 35.000  Top 5-err 11.610	 Test Loss 1.309
65.0
0.65
loss: 1.3094123125076294
(35, 0, 98) triplet: 0.09
(35, 0): 0.005
(35, 98): 0.095
* Epoch: [9/60]	 Top 1-err 51.853  Top 5-err 26.356	 Train Loss 2.414
* Epoch: [9/60]	 Top 1-err 34.950  Top 5-err 11.250	 Test Loss 1.296
* Epoch: [9/60]	 Top 1-err 34.950  Top 5-err 11.250	 Test Loss 1.296
65.05
0.6505
loss: 1.295929637145996
(35, 0, 98) triplet: 0.225
(35, 0): 0.005
(35, 98): 0.23
* Epoch: [10/60]	 Top 1-err 51.772  Top 5-err 26.107	 Train Loss 2.427
* Epoch: [10/60]	 Top 1-err 38.150  Top 5-err 12.830	 Test Loss 1.413
* Epoch: [10/60]	 Top 1-err 38.150  Top 5-err 12.830	 Test Loss 1.413
61.85
0.6185
loss: 1.413094828605652
(35, 0, 98) triplet: 0.17500000000000002
(35, 0): 0.03
(35, 98): 0.20500000000000002
* Epoch: [11/60]	 Top 1-err 52.425  Top 5-err 26.453	 Train Loss 2.477
* Epoch: [11/60]	 Top 1-err 37.760  Top 5-err 12.420	 Test Loss 1.401
* Epoch: [11/60]	 Top 1-err 37.760  Top 5-err 12.420	 Test Loss 1.401
62.24
0.6224
loss: 1.40097809009552
(35, 0, 98) triplet: 0.11500000000000002
(35, 0): 0.030000000000000002
(35, 98): 0.14500000000000002
not enough sample
* Epoch: [12/60]	 Top 1-err 53.400  Top 5-err 27.686	 Train Loss 2.472
* Epoch: [12/60]	 Top 1-err 36.250  Top 5-err 11.880	 Test Loss 1.347
* Epoch: [12/60]	 Top 1-err 36.250  Top 5-err 11.880	 Test Loss 1.347
63.75
0.6375
loss: 1.3474130373001099
(35, 0, 98) triplet: 0.18000000000000002
(35, 0): 0.03
(35, 98): 0.21000000000000002
* Epoch: [13/60]	 Top 1-err 51.776  Top 5-err 25.924	 Train Loss 2.437
* Epoch: [13/60]	 Top 1-err 39.170  Top 5-err 13.270	 Test Loss 1.461
* Epoch: [13/60]	 Top 1-err 39.170  Top 5-err 13.270	 Test Loss 1.461
60.83
0.6083
loss: 1.4614797275543212
(35, 0, 98) triplet: 0.34
(35, 0): 0.0
(35, 98): 0.34
* Epoch: [14/60]	 Top 1-err 51.677  Top 5-err 25.812	 Train Loss 2.441
* Epoch: [14/60]	 Top 1-err 37.000  Top 5-err 11.820	 Test Loss 1.372
* Epoch: [14/60]	 Top 1-err 37.000  Top 5-err 11.820	 Test Loss 1.372
63.0
0.63
loss: 1.371635029220581
(35, 0, 98) triplet: 0.195
(35, 0): 0.02
(35, 98): 0.215
* Epoch: [15/60]	 Top 1-err 55.644  Top 5-err 28.920	 Train Loss 2.492
* Epoch: [15/60]	 Top 1-err 38.680  Top 5-err 13.450	 Test Loss 1.477
* Epoch: [15/60]	 Top 1-err 38.680  Top 5-err 13.450	 Test Loss 1.477
61.32
0.6132
loss: 1.4773801710128784
(35, 0, 98) triplet: 0.145
(35, 0): 0.035
(35, 98): 0.18
* Epoch: [16/60]	 Top 1-err 50.895  Top 5-err 24.369	 Train Loss 2.414
* Epoch: [16/60]	 Top 1-err 36.820  Top 5-err 11.840	 Test Loss 1.391
* Epoch: [16/60]	 Top 1-err 36.820  Top 5-err 11.840	 Test Loss 1.391
63.18
0.6318
loss: 1.3914882877349855
(35, 0, 98) triplet: 0.18999999999999997
(35, 0): 0.04
(35, 98): 0.22999999999999998
not enough sample
* Epoch: [17/60]	 Top 1-err 52.556  Top 5-err 26.658	 Train Loss 2.395
* Epoch: [17/60]	 Top 1-err 37.920  Top 5-err 13.050	 Test Loss 1.418
* Epoch: [17/60]	 Top 1-err 37.920  Top 5-err 13.050	 Test Loss 1.418
62.08
0.6208
loss: 1.417900001525879
(35, 0, 98) triplet: 0.09
(35, 0): 0.1
(35, 98): 0.19
* Epoch: [18/60]	 Top 1-err 53.138  Top 5-err 26.777	 Train Loss 2.431
* Epoch: [18/60]	 Top 1-err 37.730  Top 5-err 12.260	 Test Loss 1.424
* Epoch: [18/60]	 Top 1-err 37.730  Top 5-err 12.260	 Test Loss 1.424
62.27
0.6227
loss: 1.4243495706558227
(35, 0, 98) triplet: 0.15000000000000002
(35, 0): 0.025
(35, 98): 0.17500000000000002
* Epoch: [19/60]	 Top 1-err 51.711  Top 5-err 26.130	 Train Loss 2.368
* Epoch: [19/60]	 Top 1-err 37.480  Top 5-err 12.060	 Test Loss 1.390
* Epoch: [19/60]	 Top 1-err 37.480  Top 5-err 12.060	 Test Loss 1.390
62.52
0.6252
loss: 1.390155651664734
(35, 0, 98) triplet: 0.255
(35, 0): 0.03
(35, 98): 0.28500000000000003
* Epoch: [20/60]	 Top 1-err 48.423  Top 5-err 22.905	 Train Loss 2.301
* Epoch: [20/60]	 Top 1-err 34.540  Top 5-err 10.330	 Test Loss 1.283
* Epoch: [20/60]	 Top 1-err 34.540  Top 5-err 10.330	 Test Loss 1.283
65.46
0.6546
loss: 1.2827298629760742
(35, 0, 98) triplet: 0.13999999999999999
(35, 0): 0.01
(35, 98): 0.15
* Epoch: [21/60]	 Top 1-err 52.599  Top 5-err 26.964	 Train Loss 2.398
* Epoch: [21/60]	 Top 1-err 37.230  Top 5-err 12.910	 Test Loss 1.404
* Epoch: [21/60]	 Top 1-err 37.230  Top 5-err 12.910	 Test Loss 1.404
62.77
0.6277
loss: 1.4040043300628662
(35, 0, 98) triplet: 0.16
(35, 0): 0.005
(35, 98): 0.165
not enough sample
* Epoch: [22/60]	 Top 1-err 48.325  Top 5-err 22.082	 Train Loss 2.342
* Epoch: [22/60]	 Top 1-err 35.560  Top 5-err 10.790	 Test Loss 1.302
* Epoch: [22/60]	 Top 1-err 35.560  Top 5-err 10.790	 Test Loss 1.302
64.44
0.6444
loss: 1.3018741401672362
(35, 0, 98) triplet: 0.17
(35, 0): 0.035
(35, 98): 0.20500000000000002
* Epoch: [23/60]	 Top 1-err 50.635  Top 5-err 24.768	 Train Loss 2.350
* Epoch: [23/60]	 Top 1-err 37.010  Top 5-err 12.040	 Test Loss 1.384
* Epoch: [23/60]	 Top 1-err 37.010  Top 5-err 12.040	 Test Loss 1.384
62.99
0.6299
loss: 1.3842373916625976
(35, 0, 98) triplet: 0.24
(35, 0): 0.0
(35, 98): 0.24
* Epoch: [24/60]	 Top 1-err 51.409  Top 5-err 24.932	 Train Loss 2.383
* Epoch: [24/60]	 Top 1-err 37.880  Top 5-err 11.860	 Test Loss 1.376
* Epoch: [24/60]	 Top 1-err 37.880  Top 5-err 11.860	 Test Loss 1.376
62.12
0.6212
loss: 1.376288395690918
(35, 0, 98) triplet: 0.20500000000000002
(35, 0): 0.065
(35, 98): 0.27
* Epoch: [25/60]	 Top 1-err 50.727  Top 5-err 23.904	 Train Loss 2.392
* Epoch: [25/60]	 Top 1-err 36.480  Top 5-err 11.540	 Test Loss 1.356
* Epoch: [25/60]	 Top 1-err 36.480  Top 5-err 11.540	 Test Loss 1.356
63.52
0.6352
loss: 1.3558045278549193
(35, 0, 98) triplet: 0.145
(35, 0): 0.02
(35, 98): 0.16499999999999998
* Epoch: [26/60]	 Top 1-err 50.406  Top 5-err 23.393	 Train Loss 2.393
* Epoch: [26/60]	 Top 1-err 37.000  Top 5-err 11.470	 Test Loss 1.351
* Epoch: [26/60]	 Top 1-err 37.000  Top 5-err 11.470	 Test Loss 1.351
63.0
0.63
loss: 1.3513800117492676
(35, 0, 98) triplet: 0.0050000000000000044
(35, 0): 0.255
(35, 98): 0.26
* Epoch: [27/60]	 Top 1-err 51.096  Top 5-err 24.407	 Train Loss 2.400
* Epoch: [27/60]	 Top 1-err 35.450  Top 5-err 11.200	 Test Loss 1.317
* Epoch: [27/60]	 Top 1-err 35.450  Top 5-err 11.200	 Test Loss 1.317
64.55
0.6455
loss: 1.317435696220398
(35, 0, 98) triplet: 0.1
(35, 0): 0.08499999999999999
(35, 98): 0.185
* Epoch: [28/60]	 Top 1-err 51.874  Top 5-err 25.765	 Train Loss 2.391
* Epoch: [28/60]	 Top 1-err 34.750  Top 5-err 11.060	 Test Loss 1.305
* Epoch: [28/60]	 Top 1-err 34.750  Top 5-err 11.060	 Test Loss 1.305
65.25
0.6525
loss: 1.3054033138275147
(35, 0, 98) triplet: 0.165
(35, 0): 0.025
(35, 98): 0.19
* Epoch: [29/60]	 Top 1-err 50.623  Top 5-err 23.846	 Train Loss 2.368
* Epoch: [29/60]	 Top 1-err 37.890  Top 5-err 12.920	 Test Loss 1.424
* Epoch: [29/60]	 Top 1-err 37.890  Top 5-err 12.920	 Test Loss 1.424
62.11
0.6211
loss: 1.4235989948272705
(35, 0, 98) triplet: 0.125
(35, 0): 0.04
(35, 98): 0.165
* Epoch: [30/60]	 Top 1-err 47.841  Top 5-err 22.533	 Train Loss 2.260
* Epoch: [30/60]	 Top 1-err 29.130  Top 5-err 7.620	 Test Loss 1.085
* Epoch: [30/60]	 Top 1-err 29.130  Top 5-err 7.620	 Test Loss 1.085
70.87
0.7087
loss: 1.0850211345672607
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [31/60]	 Top 1-err 45.848  Top 5-err 21.658	 Train Loss 2.163
* Epoch: [31/60]	 Top 1-err 28.680  Top 5-err 7.660	 Test Loss 1.072
* Epoch: [31/60]	 Top 1-err 28.680  Top 5-err 7.660	 Test Loss 1.072
71.32
0.7132
loss: 1.0722920329093932
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.0
(35, 98): 0.16499999999999998
* Epoch: [32/60]	 Top 1-err 42.781  Top 5-err 18.721	 Train Loss 2.162
* Epoch: [32/60]	 Top 1-err 28.500  Top 5-err 7.630	 Test Loss 1.081
* Epoch: [32/60]	 Top 1-err 28.500  Top 5-err 7.630	 Test Loss 1.081
71.5
0.715
loss: 1.0811081993103027
(35, 0, 98) triplet: 0.175
(35, 0): 0.015
(35, 98): 0.19
not enough sample
* Epoch: [33/60]	 Top 1-err 46.617  Top 5-err 21.080	 Train Loss 2.199
* Epoch: [33/60]	 Top 1-err 28.150  Top 5-err 7.620	 Test Loss 1.071
* Epoch: [33/60]	 Top 1-err 28.150  Top 5-err 7.620	 Test Loss 1.071
71.85
0.7185
loss: 1.0706539476394654
(35, 0, 98) triplet: 0.15
(35, 0): 0.005
(35, 98): 0.155
* Epoch: [34/60]	 Top 1-err 42.578  Top 5-err 18.770	 Train Loss 2.119
* Epoch: [34/60]	 Top 1-err 28.530  Top 5-err 7.510	 Test Loss 1.079
* Epoch: [34/60]	 Top 1-err 28.530  Top 5-err 7.510	 Test Loss 1.079
71.47
0.7147
loss: 1.0788754682540893
(35, 0, 98) triplet: 0.18
(35, 0): 0.015
(35, 98): 0.195
* Epoch: [35/60]	 Top 1-err 45.963  Top 5-err 21.360	 Train Loss 2.190
* Epoch: [35/60]	 Top 1-err 28.440  Top 5-err 7.510	 Test Loss 1.063
* Epoch: [35/60]	 Top 1-err 28.440  Top 5-err 7.510	 Test Loss 1.063
71.56
0.7156
loss: 1.06255831489563
(35, 0, 98) triplet: 0.14999999999999997
(35, 0): 0.015
(35, 98): 0.16499999999999998
* Epoch: [36/60]	 Top 1-err 41.947  Top 5-err 18.143	 Train Loss 2.093
* Epoch: [36/60]	 Top 1-err 28.230  Top 5-err 7.540	 Test Loss 1.074
* Epoch: [36/60]	 Top 1-err 28.230  Top 5-err 7.540	 Test Loss 1.074
71.77
0.7177
loss: 1.0743589851379394
(35, 0, 98) triplet: 0.16
(35, 0): 0.01
(35, 98): 0.17
not enough sample
* Epoch: [37/60]	 Top 1-err 40.725  Top 5-err 17.786	 Train Loss 2.067
* Epoch: [37/60]	 Top 1-err 27.560  Top 5-err 7.360	 Test Loss 1.030
* Epoch: [37/60]	 Top 1-err 27.560  Top 5-err 7.360	 Test Loss 1.030
72.44
0.7244
loss: 1.02967006149292
(35, 0, 98) triplet: 0.11499999999999999
(35, 0): 0.005
(35, 98): 0.12
* Epoch: [38/60]	 Top 1-err 43.357  Top 5-err 19.360	 Train Loss 2.129
* Epoch: [38/60]	 Top 1-err 27.870  Top 5-err 7.510	 Test Loss 1.058
* Epoch: [38/60]	 Top 1-err 27.870  Top 5-err 7.510	 Test Loss 1.058
72.13
0.7213
loss: 1.0578234939575195
(35, 0, 98) triplet: 0.12
(35, 0): 0.0
(35, 98): 0.12
* Epoch: [39/60]	 Top 1-err 45.404  Top 5-err 20.497	 Train Loss 2.182
* Epoch: [39/60]	 Top 1-err 28.210  Top 5-err 7.290	 Test Loss 1.062
* Epoch: [39/60]	 Top 1-err 28.210  Top 5-err 7.290	 Test Loss 1.062
71.79
0.7179
loss: 1.062331139755249
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.005
(35, 98): 0.175
not enough sample
* Epoch: [40/60]	 Top 1-err 43.666  Top 5-err 19.796	 Train Loss 2.142
* Epoch: [40/60]	 Top 1-err 28.520  Top 5-err 7.410	 Test Loss 1.076
* Epoch: [40/60]	 Top 1-err 28.520  Top 5-err 7.410	 Test Loss 1.076
71.48
0.7148
loss: 1.075948285961151
(35, 0, 98) triplet: 0.15
(35, 0): 0.005
(35, 98): 0.155
* Epoch: [41/60]	 Top 1-err 43.090  Top 5-err 18.787	 Train Loss 2.120
* Epoch: [41/60]	 Top 1-err 28.030  Top 5-err 7.260	 Test Loss 1.040
* Epoch: [41/60]	 Top 1-err 28.030  Top 5-err 7.260	 Test Loss 1.040
71.97
0.7197
loss: 1.039542990875244
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.005
(35, 98): 0.16999999999999998
* Epoch: [42/60]	 Top 1-err 42.225  Top 5-err 19.222	 Train Loss 2.085
* Epoch: [42/60]	 Top 1-err 27.820  Top 5-err 7.370	 Test Loss 1.045
* Epoch: [42/60]	 Top 1-err 27.820  Top 5-err 7.370	 Test Loss 1.045
72.18
0.7218
loss: 1.045373698234558
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
* Epoch: [43/60]	 Top 1-err 42.316  Top 5-err 18.561	 Train Loss 2.085
* Epoch: [43/60]	 Top 1-err 27.950  Top 5-err 7.450	 Test Loss 1.051
* Epoch: [43/60]	 Top 1-err 27.950  Top 5-err 7.450	 Test Loss 1.051
72.05
0.7205
loss: 1.051489743423462
(35, 0, 98) triplet: 0.17
(35, 0): 0.0
(35, 98): 0.17
* Epoch: [44/60]	 Top 1-err 43.750  Top 5-err 19.951	 Train Loss 2.091
* Epoch: [44/60]	 Top 1-err 28.020  Top 5-err 7.380	 Test Loss 1.046
* Epoch: [44/60]	 Top 1-err 28.020  Top 5-err 7.380	 Test Loss 1.046
71.98
0.7198
loss: 1.0461872081756591
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
not enough sample
* Epoch: [45/60]	 Top 1-err 40.625  Top 5-err 17.809	 Train Loss 2.059
* Epoch: [45/60]	 Top 1-err 27.570  Top 5-err 7.320	 Test Loss 1.048
Current best accuracy (top-1 and 5 error): 27.57 7.32
saving best model...
* Epoch: [45/60]	 Top 1-err 27.570  Top 5-err 7.320	 Test Loss 1.048
72.43
0.7243
loss: 1.04771814994812
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [46/60]	 Top 1-err 41.904  Top 5-err 18.989	 Train Loss 2.033
* Epoch: [46/60]	 Top 1-err 27.300  Top 5-err 7.240	 Test Loss 1.037
Current best accuracy (top-1 and 5 error): 27.3 7.24
saving best model...
* Epoch: [46/60]	 Top 1-err 27.300  Top 5-err 7.240	 Test Loss 1.037
72.7
0.727
loss: 1.0373757064819336
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [47/60]	 Top 1-err 44.919  Top 5-err 20.553	 Train Loss 2.119
* Epoch: [47/60]	 Top 1-err 27.350  Top 5-err 7.250	 Test Loss 1.032
Current best accuracy (top-1 and 5 error): 27.3 7.24
* Epoch: [47/60]	 Top 1-err 27.350  Top 5-err 7.250	 Test Loss 1.032
72.65
0.7265
loss: 1.0323566881179809
(35, 0, 98) triplet: 0.15000000000000002
(35, 0): 0.0
(35, 98): 0.15000000000000002
* Epoch: [48/60]	 Top 1-err 41.029  Top 5-err 17.775	 Train Loss 2.054
* Epoch: [48/60]	 Top 1-err 27.560  Top 5-err 7.120	 Test Loss 1.037
Current best accuracy (top-1 and 5 error): 27.3 7.24
* Epoch: [48/60]	 Top 1-err 27.560  Top 5-err 7.120	 Test Loss 1.037
72.44
0.7244
loss: 1.0374709312438966
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [49/60]	 Top 1-err 42.155  Top 5-err 19.743	 Train Loss 2.012
* Epoch: [49/60]	 Top 1-err 27.330  Top 5-err 7.130	 Test Loss 1.030
Current best accuracy (top-1 and 5 error): 27.3 7.24
* Epoch: [49/60]	 Top 1-err 27.330  Top 5-err 7.130	 Test Loss 1.030
72.67
0.7267
loss: 1.0297520586967468
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [50/60]	 Top 1-err 41.409  Top 5-err 18.088	 Train Loss 2.060
* Epoch: [50/60]	 Top 1-err 27.270  Top 5-err 7.100	 Test Loss 1.034
Current best accuracy (top-1 and 5 error): 27.27 7.1
saving best model...
* Epoch: [50/60]	 Top 1-err 27.270  Top 5-err 7.100	 Test Loss 1.034
72.73
0.7273
loss: 1.0335112126350403
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [51/60]	 Top 1-err 44.889  Top 5-err 20.795	 Train Loss 2.139
* Epoch: [51/60]	 Top 1-err 27.430  Top 5-err 7.250	 Test Loss 1.039
Current best accuracy (top-1 and 5 error): 27.27 7.1
* Epoch: [51/60]	 Top 1-err 27.430  Top 5-err 7.250	 Test Loss 1.039
72.57
0.7257
loss: 1.0388438455581666
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [52/60]	 Top 1-err 41.949  Top 5-err 17.858	 Train Loss 2.084
* Epoch: [52/60]	 Top 1-err 27.380  Top 5-err 7.120	 Test Loss 1.030
Current best accuracy (top-1 and 5 error): 27.27 7.1
* Epoch: [52/60]	 Top 1-err 27.380  Top 5-err 7.120	 Test Loss 1.030
72.62
0.7262
loss: 1.0299190408706664
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [53/60]	 Top 1-err 39.040  Top 5-err 15.935	 Train Loss 2.042
* Epoch: [53/60]	 Top 1-err 27.290  Top 5-err 7.090	 Test Loss 1.026
Current best accuracy (top-1 and 5 error): 27.27 7.1
* Epoch: [53/60]	 Top 1-err 27.290  Top 5-err 7.090	 Test Loss 1.026
72.71
0.7271
loss: 1.026285129928589
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
not enough sample
* Epoch: [54/60]	 Top 1-err 41.113  Top 5-err 17.960	 Train Loss 2.085
* Epoch: [54/60]	 Top 1-err 27.440  Top 5-err 7.180	 Test Loss 1.053
Current best accuracy (top-1 and 5 error): 27.27 7.1
* Epoch: [54/60]	 Top 1-err 27.440  Top 5-err 7.180	 Test Loss 1.053
72.56
0.7256
loss: 1.0529123266220093
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
* Epoch: [55/60]	 Top 1-err 41.587  Top 5-err 18.570	 Train Loss 2.035
* Epoch: [55/60]	 Top 1-err 27.170  Top 5-err 7.050	 Test Loss 1.016
Current best accuracy (top-1 and 5 error): 27.17 7.05
saving best model...
* Epoch: [55/60]	 Top 1-err 27.170  Top 5-err 7.050	 Test Loss 1.016
72.83
0.7283
loss: 1.0161723756790162
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
not enough sample
* Epoch: [56/60]	 Top 1-err 40.863  Top 5-err 16.961	 Train Loss 2.072
* Epoch: [56/60]	 Top 1-err 27.270  Top 5-err 7.240	 Test Loss 1.038
Current best accuracy (top-1 and 5 error): 27.17 7.05
* Epoch: [56/60]	 Top 1-err 27.270  Top 5-err 7.240	 Test Loss 1.038
72.73
0.7273
loss: 1.0376708503723144
(35, 0, 98) triplet: 0.155
(35, 0): 0.005
(35, 98): 0.16
* Epoch: [57/60]	 Top 1-err 42.057  Top 5-err 18.483	 Train Loss 2.093
* Epoch: [57/60]	 Top 1-err 27.100  Top 5-err 7.020	 Test Loss 1.017
Current best accuracy (top-1 and 5 error): 27.1 7.02
saving best model...
* Epoch: [57/60]	 Top 1-err 27.100  Top 5-err 7.020	 Test Loss 1.017
72.9
0.729
loss: 1.0166690918922425
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [58/60]	 Top 1-err 41.373  Top 5-err 18.228	 Train Loss 2.017
* Epoch: [58/60]	 Top 1-err 27.250  Top 5-err 7.110	 Test Loss 1.013
Current best accuracy (top-1 and 5 error): 27.1 7.02
* Epoch: [58/60]	 Top 1-err 27.250  Top 5-err 7.110	 Test Loss 1.013
72.75
0.7275
loss: 1.013119006538391
(35, 0, 98) triplet: 0.15
(35, 0): 0.005
(35, 98): 0.155
* Epoch: [59/60]	 Top 1-err 39.985  Top 5-err 17.127	 Train Loss 2.011
* Epoch: [59/60]	 Top 1-err 27.360  Top 5-err 7.060	 Test Loss 1.030
Current best accuracy (top-1 and 5 error): 27.1 7.02
* Epoch: [59/60]	 Top 1-err 27.360  Top 5-err 7.060	 Test Loss 1.030
72.64
0.7264
loss: 1.029672819519043
(35, 0, 98) triplet: 0.16
(35, 0): 0.005
(35, 98): 0.165
Best accuracy (top-1 and 5 error): 27.1 7.02
=> loading checkpoint 'runs/cifar100_resnet_bias/model_best.pth.tar'
* Epoch: [-1/60]	 Top 1-err 27.100  Top 5-err 7.020	 Test Loss 1.017
72.9
0.729
loss: 1.0166691036224365
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
python3 cifar100_repair_bias.py --net_type resnet --dataset cifar100 --depth 50 --batch_size 256 --lr 0.1 --expname cifar100_resnet_bias --epochs 60 --beta 1.0 --cutmix_prob 1 --pretrained ./runs/cifar100_resnet_2/model_best.pth.tar --expid 0 --first 35 --second 0 --third 98 --lam 1
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
=> creating model 'resnet'
True
=> loading checkpoint './runs/cifar100_resnet_2/model_best.pth.tar'
=> loaded checkpoint './runs/cifar100_resnet_2/model_best.pth.tar'
the number of model parameters: 521716
* Epoch: [0/60]	 Top 1-err 63.177  Top 5-err 36.861	 Train Loss 3.032
* Epoch: [0/60]	 Top 1-err 42.500  Top 5-err 15.510	 Test Loss 1.575
* Epoch: [0/60]	 Top 1-err 42.500  Top 5-err 15.510	 Test Loss 1.575
57.5
0.575
loss: 1.5752682111740113
(35, 0, 98) triplet: 0.15500000000000003
(35, 0): 0.05
(35, 98): 0.20500000000000002
* Epoch: [1/60]	 Top 1-err 60.990  Top 5-err 35.096	 Train Loss 2.767
* Epoch: [1/60]	 Top 1-err 38.460  Top 5-err 13.770	 Test Loss 1.439
* Epoch: [1/60]	 Top 1-err 38.460  Top 5-err 13.770	 Test Loss 1.439
61.54
0.6154
loss: 1.4394095270156861
(35, 0, 98) triplet: 0.175
(35, 0): 0.04
(35, 98): 0.215
* Epoch: [2/60]	 Top 1-err 56.861  Top 5-err 30.353	 Train Loss 2.619
* Epoch: [2/60]	 Top 1-err 39.810  Top 5-err 13.490	 Test Loss 1.465
* Epoch: [2/60]	 Top 1-err 39.810  Top 5-err 13.490	 Test Loss 1.465
60.19
0.6019
loss: 1.4650429222106933
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.015
(35, 98): 0.16
* Epoch: [3/60]	 Top 1-err 57.393  Top 5-err 31.315	 Train Loss 2.649
* Epoch: [3/60]	 Top 1-err 49.160  Top 5-err 21.770	 Test Loss 1.985
* Epoch: [3/60]	 Top 1-err 49.160  Top 5-err 21.770	 Test Loss 1.985
50.84
0.5084
loss: 1.9851135318756103
(35, 0, 98) triplet: 0.015
(35, 0): 0.065
(35, 98): 0.08
not enough sample
not enough sample
* Epoch: [4/60]	 Top 1-err 58.521  Top 5-err 31.462	 Train Loss 2.705
* Epoch: [4/60]	 Top 1-err 44.680  Top 5-err 16.280	 Test Loss 1.672
* Epoch: [4/60]	 Top 1-err 44.680  Top 5-err 16.280	 Test Loss 1.672
55.32
0.5532
loss: 1.6722186054229737
(35, 0, 98) triplet: 0.15499999999999997
(35, 0): 0.435
(35, 98): 0.28
* Epoch: [5/60]	 Top 1-err 53.211  Top 5-err 26.932	 Train Loss 2.538
* Epoch: [5/60]	 Top 1-err 38.760  Top 5-err 13.300	 Test Loss 1.464
* Epoch: [5/60]	 Top 1-err 38.760  Top 5-err 13.300	 Test Loss 1.464
61.24
0.6124
loss: 1.4638564109802246
(35, 0, 98) triplet: 0.18
(35, 0): 0.030000000000000002
(35, 98): 0.21
* Epoch: [6/60]	 Top 1-err 53.999  Top 5-err 27.716	 Train Loss 2.553
* Epoch: [6/60]	 Top 1-err 41.030  Top 5-err 15.090	 Test Loss 1.554
* Epoch: [6/60]	 Top 1-err 41.030  Top 5-err 15.090	 Test Loss 1.554
58.97
0.5897
loss: 1.5542853656768798
(35, 0, 98) triplet: 0.045
(35, 0): 0.09000000000000001
(35, 98): 0.135
* Epoch: [7/60]	 Top 1-err 52.977  Top 5-err 27.291	 Train Loss 2.457
* Epoch: [7/60]	 Top 1-err 37.790  Top 5-err 13.130	 Test Loss 1.431
* Epoch: [7/60]	 Top 1-err 37.790  Top 5-err 13.130	 Test Loss 1.431
62.21
0.6221
loss: 1.4307503063201905
(35, 0, 98) triplet: 0.024999999999999994
(35, 0): 0.11
(35, 98): 0.085
* Epoch: [8/60]	 Top 1-err 54.309  Top 5-err 28.258	 Train Loss 2.481
* Epoch: [8/60]	 Top 1-err 39.070  Top 5-err 12.690	 Test Loss 1.427
* Epoch: [8/60]	 Top 1-err 39.070  Top 5-err 12.690	 Test Loss 1.427
60.93
0.6093
loss: 1.4271931224822998
(35, 0, 98) triplet: 0.15000000000000002
(35, 0): 0.015
(35, 98): 0.165
* Epoch: [9/60]	 Top 1-err 54.711  Top 5-err 28.438	 Train Loss 2.527
* Epoch: [9/60]	 Top 1-err 41.290  Top 5-err 14.350	 Test Loss 1.534
* Epoch: [9/60]	 Top 1-err 41.290  Top 5-err 14.350	 Test Loss 1.534
58.71
0.5871
loss: 1.5339321636199952
(35, 0, 98) triplet: 0.075
(35, 0): 0.105
(35, 98): 0.18
* Epoch: [10/60]	 Top 1-err 54.751  Top 5-err 28.277	 Train Loss 2.549
* Epoch: [10/60]	 Top 1-err 39.550  Top 5-err 13.740	 Test Loss 1.498
* Epoch: [10/60]	 Top 1-err 39.550  Top 5-err 13.740	 Test Loss 1.498
60.45
0.6045
loss: 1.4984038974761962
(35, 0, 98) triplet: 0.2
(35, 0): 0.015
(35, 98): 0.215
* Epoch: [11/60]	 Top 1-err 55.559  Top 5-err 28.466	 Train Loss 2.592
* Epoch: [11/60]	 Top 1-err 43.420  Top 5-err 15.630	 Test Loss 1.611
* Epoch: [11/60]	 Top 1-err 43.420  Top 5-err 15.630	 Test Loss 1.611
56.58
0.5658
loss: 1.6114379348754884
(35, 0, 98) triplet: 0.06
(35, 0): 0.015
(35, 98): 0.075
not enough sample
* Epoch: [12/60]	 Top 1-err 56.679  Top 5-err 29.751	 Train Loss 2.590
* Epoch: [12/60]	 Top 1-err 36.770  Top 5-err 12.400	 Test Loss 1.353
* Epoch: [12/60]	 Top 1-err 36.770  Top 5-err 12.400	 Test Loss 1.353
63.23
0.6323
loss: 1.353342310142517
(35, 0, 98) triplet: 0.125
(35, 0): 0.035
(35, 98): 0.16
* Epoch: [13/60]	 Top 1-err 54.481  Top 5-err 27.741	 Train Loss 2.534
* Epoch: [13/60]	 Top 1-err 36.350  Top 5-err 12.170	 Test Loss 1.373
* Epoch: [13/60]	 Top 1-err 36.350  Top 5-err 12.170	 Test Loss 1.373
63.65
0.6365
loss: 1.3729539180755614
(35, 0, 98) triplet: 0.21
(35, 0): 0.034999999999999996
(35, 98): 0.245
* Epoch: [14/60]	 Top 1-err 54.148  Top 5-err 27.543	 Train Loss 2.534
* Epoch: [14/60]	 Top 1-err 35.610  Top 5-err 11.010	 Test Loss 1.337
* Epoch: [14/60]	 Top 1-err 35.610  Top 5-err 11.010	 Test Loss 1.337
64.39
0.6439
loss: 1.3368209524154664
(35, 0, 98) triplet: 0.19
(35, 0): 0.0
(35, 98): 0.19
* Epoch: [15/60]	 Top 1-err 57.888  Top 5-err 30.882	 Train Loss 2.591
* Epoch: [15/60]	 Top 1-err 39.920  Top 5-err 13.260	 Test Loss 1.480
* Epoch: [15/60]	 Top 1-err 39.920  Top 5-err 13.260	 Test Loss 1.480
60.08
0.6008
loss: 1.48021552734375
(35, 0, 98) triplet: 0.08499999999999999
(35, 0): 0.165
(35, 98): 0.25
* Epoch: [16/60]	 Top 1-err 53.268  Top 5-err 26.182	 Train Loss 2.506
* Epoch: [16/60]	 Top 1-err 38.660  Top 5-err 13.320	 Test Loss 1.439
* Epoch: [16/60]	 Top 1-err 38.660  Top 5-err 13.320	 Test Loss 1.439
61.34
0.6134
loss: 1.4387498695373535
(35, 0, 98) triplet: 0.175
(35, 0): 0.215
(35, 98): 0.04
not enough sample
* Epoch: [17/60]	 Top 1-err 55.077  Top 5-err 28.581	 Train Loss 2.504
* Epoch: [17/60]	 Top 1-err 37.840  Top 5-err 12.650	 Test Loss 1.407
* Epoch: [17/60]	 Top 1-err 37.840  Top 5-err 12.650	 Test Loss 1.407
62.16
0.6216
loss: 1.4069611757278442
(35, 0, 98) triplet: 0.14
(35, 0): 0.065
(35, 98): 0.20500000000000002
* Epoch: [18/60]	 Top 1-err 55.759  Top 5-err 28.699	 Train Loss 2.539
* Epoch: [18/60]	 Top 1-err 37.230  Top 5-err 12.500	 Test Loss 1.399
* Epoch: [18/60]	 Top 1-err 37.230  Top 5-err 12.500	 Test Loss 1.399
62.77
0.6277
loss: 1.3994213115692138
(35, 0, 98) triplet: 0.06999999999999998
(35, 0): 0.07
(35, 98): 0.13999999999999999
* Epoch: [19/60]	 Top 1-err 54.252  Top 5-err 28.030	 Train Loss 2.467
* Epoch: [19/60]	 Top 1-err 37.890  Top 5-err 11.990	 Test Loss 1.430
* Epoch: [19/60]	 Top 1-err 37.890  Top 5-err 11.990	 Test Loss 1.430
62.11
0.6211
loss: 1.4304771354675292
(35, 0, 98) triplet: 0.015
(35, 0): 0.125
(35, 98): 0.11
* Epoch: [20/60]	 Top 1-err 51.643  Top 5-err 24.788	 Train Loss 2.415
* Epoch: [20/60]	 Top 1-err 36.080  Top 5-err 11.230	 Test Loss 1.330
* Epoch: [20/60]	 Top 1-err 36.080  Top 5-err 11.230	 Test Loss 1.330
63.92
0.6392
loss: 1.3303576457977295
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.060000000000000005
(35, 98): 0.20500000000000002
* Epoch: [21/60]	 Top 1-err 55.342  Top 5-err 28.965	 Train Loss 2.504
* Epoch: [21/60]	 Top 1-err 38.230  Top 5-err 13.070	 Test Loss 1.434
* Epoch: [21/60]	 Top 1-err 38.230  Top 5-err 13.070	 Test Loss 1.434
61.77
0.6177
loss: 1.433838026046753
(35, 0, 98) triplet: 0.15
(35, 0): 0.025
(35, 98): 0.175
not enough sample
* Epoch: [22/60]	 Top 1-err 51.031  Top 5-err 23.869	 Train Loss 2.444
* Epoch: [22/60]	 Top 1-err 38.290  Top 5-err 12.680	 Test Loss 1.407
* Epoch: [22/60]	 Top 1-err 38.290  Top 5-err 12.680	 Test Loss 1.407
61.71
0.6171
loss: 1.4073601665496827
(35, 0, 98) triplet: 0.285
(35, 0): 0.015
(35, 98): 0.3
* Epoch: [23/60]	 Top 1-err 53.784  Top 5-err 26.762	 Train Loss 2.465
* Epoch: [23/60]	 Top 1-err 39.710  Top 5-err 13.680	 Test Loss 1.491
* Epoch: [23/60]	 Top 1-err 39.710  Top 5-err 13.680	 Test Loss 1.491
60.29
0.6029
loss: 1.4914490005493164
(35, 0, 98) triplet: 0.30000000000000004
(35, 0): 0.045
(35, 98): 0.34500000000000003
* Epoch: [24/60]	 Top 1-err 53.547  Top 5-err 26.628	 Train Loss 2.481
* Epoch: [24/60]	 Top 1-err 40.350  Top 5-err 13.270	 Test Loss 1.484
* Epoch: [24/60]	 Top 1-err 40.350  Top 5-err 13.270	 Test Loss 1.484
59.65
0.5965
loss: 1.4839406257629395
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.049999999999999996
(35, 98): 0.195
* Epoch: [25/60]	 Top 1-err 53.126  Top 5-err 25.952	 Train Loss 2.496
* Epoch: [25/60]	 Top 1-err 44.360  Top 5-err 17.390	 Test Loss 1.727
* Epoch: [25/60]	 Top 1-err 44.360  Top 5-err 17.390	 Test Loss 1.727
55.64
0.5564
loss: 1.727166293334961
(35, 0, 98) triplet: 0.21000000000000002
(35, 0): 0.03
(35, 98): 0.24000000000000002
* Epoch: [26/60]	 Top 1-err 52.945  Top 5-err 25.317	 Train Loss 2.491
* Epoch: [26/60]	 Top 1-err 37.690  Top 5-err 11.680	 Test Loss 1.392
* Epoch: [26/60]	 Top 1-err 37.690  Top 5-err 11.680	 Test Loss 1.392
62.31
0.6231
loss: 1.3920087966918946
(35, 0, 98) triplet: 0.205
(35, 0): 0.02
(35, 98): 0.22499999999999998
* Epoch: [27/60]	 Top 1-err 52.969  Top 5-err 26.109	 Train Loss 2.486
* Epoch: [27/60]	 Top 1-err 38.050  Top 5-err 12.400	 Test Loss 1.423
* Epoch: [27/60]	 Top 1-err 38.050  Top 5-err 12.400	 Test Loss 1.423
61.95
0.6195
loss: 1.4228501646041871
(35, 0, 98) triplet: 0.14
(35, 0): 0.025
(35, 98): 0.165
* Epoch: [28/60]	 Top 1-err 54.016  Top 5-err 27.335	 Train Loss 2.483
* Epoch: [28/60]	 Top 1-err 38.890  Top 5-err 13.460	 Test Loss 1.462
* Epoch: [28/60]	 Top 1-err 38.890  Top 5-err 13.460	 Test Loss 1.462
61.11
0.6111
loss: 1.4620342247009277
(35, 0, 98) triplet: 0.065
(35, 0): 0.04
(35, 98): 0.105
* Epoch: [29/60]	 Top 1-err 53.292  Top 5-err 25.977	 Train Loss 2.471
* Epoch: [29/60]	 Top 1-err 38.550  Top 5-err 12.360	 Test Loss 1.427
* Epoch: [29/60]	 Top 1-err 38.550  Top 5-err 12.360	 Test Loss 1.427
61.45
0.6145
loss: 1.4267456001281738
(35, 0, 98) triplet: 0.165
(35, 0): 0.035
(35, 98): 0.2
* Epoch: [30/60]	 Top 1-err 49.924  Top 5-err 23.719	 Train Loss 2.337
* Epoch: [30/60]	 Top 1-err 30.640  Top 5-err 8.350	 Test Loss 1.130
* Epoch: [30/60]	 Top 1-err 30.640  Top 5-err 8.350	 Test Loss 1.130
69.36
0.6936
loss: 1.1295956607818602
(35, 0, 98) triplet: 0.15000000000000002
(35, 0): 0.015
(35, 98): 0.165
* Epoch: [31/60]	 Top 1-err 48.141  Top 5-err 23.117	 Train Loss 2.249
* Epoch: [31/60]	 Top 1-err 29.880  Top 5-err 8.170	 Test Loss 1.113
* Epoch: [31/60]	 Top 1-err 29.880  Top 5-err 8.170	 Test Loss 1.113
70.12
0.7012
loss: 1.1132154052734375
(35, 0, 98) triplet: 0.16
(35, 0): 0.02
(35, 98): 0.18
* Epoch: [32/60]	 Top 1-err 44.855  Top 5-err 20.151	 Train Loss 2.241
* Epoch: [32/60]	 Top 1-err 29.560  Top 5-err 7.850	 Test Loss 1.107
* Epoch: [32/60]	 Top 1-err 29.560  Top 5-err 7.850	 Test Loss 1.107
70.44
0.7044
loss: 1.1067867305755614
(35, 0, 98) triplet: 0.13
(35, 0): 0.02
(35, 98): 0.15
not enough sample
* Epoch: [33/60]	 Top 1-err 48.476  Top 5-err 22.603	 Train Loss 2.284
* Epoch: [33/60]	 Top 1-err 29.890  Top 5-err 8.090	 Test Loss 1.105
* Epoch: [33/60]	 Top 1-err 29.890  Top 5-err 8.090	 Test Loss 1.105
70.11
0.7011
loss: 1.1051469018936158
(35, 0, 98) triplet: 0.095
(35, 0): 0.04
(35, 98): 0.135
* Epoch: [34/60]	 Top 1-err 44.698  Top 5-err 20.004	 Train Loss 2.204
* Epoch: [34/60]	 Top 1-err 29.270  Top 5-err 8.080	 Test Loss 1.107
* Epoch: [34/60]	 Top 1-err 29.270  Top 5-err 8.080	 Test Loss 1.107
70.73
0.7073
loss: 1.1068961252212524
(35, 0, 98) triplet: 0.15
(35, 0): 0.01
(35, 98): 0.16
* Epoch: [35/60]	 Top 1-err 47.605  Top 5-err 22.533	 Train Loss 2.273
* Epoch: [35/60]	 Top 1-err 29.420  Top 5-err 7.910	 Test Loss 1.088
* Epoch: [35/60]	 Top 1-err 29.420  Top 5-err 7.910	 Test Loss 1.088
70.58
0.7058
loss: 1.0876472837448121
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.01
(35, 98): 0.175
* Epoch: [36/60]	 Top 1-err 43.665  Top 5-err 19.384	 Train Loss 2.168
* Epoch: [36/60]	 Top 1-err 29.080  Top 5-err 7.870	 Test Loss 1.098
* Epoch: [36/60]	 Top 1-err 29.080  Top 5-err 7.870	 Test Loss 1.098
70.92
0.7092
loss: 1.0980720314979553
(35, 0, 98) triplet: 0.15499999999999997
(35, 0): 0.01
(35, 98): 0.16499999999999998
not enough sample
* Epoch: [37/60]	 Top 1-err 42.631  Top 5-err 19.006	 Train Loss 2.150
* Epoch: [37/60]	 Top 1-err 28.670  Top 5-err 7.480	 Test Loss 1.061
* Epoch: [37/60]	 Top 1-err 28.670  Top 5-err 7.480	 Test Loss 1.061
71.33
0.7133
loss: 1.0613302589416505
(35, 0, 98) triplet: 0.155
(35, 0): 0.005
(35, 98): 0.16
* Epoch: [38/60]	 Top 1-err 45.151  Top 5-err 20.470	 Train Loss 2.206
* Epoch: [38/60]	 Top 1-err 29.100  Top 5-err 8.100	 Test Loss 1.098
* Epoch: [38/60]	 Top 1-err 29.100  Top 5-err 8.100	 Test Loss 1.098
70.9
0.709
loss: 1.0977406429290772
(35, 0, 98) triplet: 0.125
(35, 0): 0.03
(35, 98): 0.155
* Epoch: [39/60]	 Top 1-err 47.176  Top 5-err 21.866	 Train Loss 2.263
* Epoch: [39/60]	 Top 1-err 28.910  Top 5-err 7.830	 Test Loss 1.081
* Epoch: [39/60]	 Top 1-err 28.910  Top 5-err 7.830	 Test Loss 1.081
71.09
0.7109
loss: 1.0807361972808838
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.01
(35, 98): 0.18
not enough sample
* Epoch: [40/60]	 Top 1-err 45.440  Top 5-err 21.009	 Train Loss 2.223
* Epoch: [40/60]	 Top 1-err 29.040  Top 5-err 7.970	 Test Loss 1.104
* Epoch: [40/60]	 Top 1-err 29.040  Top 5-err 7.970	 Test Loss 1.104
70.96
0.7096
loss: 1.103549301624298
(35, 0, 98) triplet: 0.15500000000000003
(35, 0): 0.015
(35, 98): 0.17
* Epoch: [41/60]	 Top 1-err 44.628  Top 5-err 19.975	 Train Loss 2.199
* Epoch: [41/60]	 Top 1-err 28.640  Top 5-err 7.720	 Test Loss 1.070
* Epoch: [41/60]	 Top 1-err 28.640  Top 5-err 7.720	 Test Loss 1.070
71.36
0.7136
loss: 1.070151816558838
(35, 0, 98) triplet: 0.11500000000000002
(35, 0): 0.025
(35, 98): 0.14
* Epoch: [42/60]	 Top 1-err 44.065  Top 5-err 20.538	 Train Loss 2.164
* Epoch: [42/60]	 Top 1-err 28.930  Top 5-err 7.880	 Test Loss 1.074
* Epoch: [42/60]	 Top 1-err 28.930  Top 5-err 7.880	 Test Loss 1.074
71.07
0.7107
loss: 1.0738191986083985
(35, 0, 98) triplet: 0.18
(35, 0): 0.005
(35, 98): 0.185
* Epoch: [43/60]	 Top 1-err 44.082  Top 5-err 19.879	 Train Loss 2.163
* Epoch: [43/60]	 Top 1-err 28.690  Top 5-err 8.090	 Test Loss 1.079
* Epoch: [43/60]	 Top 1-err 28.690  Top 5-err 8.090	 Test Loss 1.079
71.31
0.7131
loss: 1.0790973096847534
(35, 0, 98) triplet: 0.135
(35, 0): 0.01
(35, 98): 0.14500000000000002
* Epoch: [44/60]	 Top 1-err 45.523  Top 5-err 21.364	 Train Loss 2.171
* Epoch: [44/60]	 Top 1-err 28.850  Top 5-err 7.940	 Test Loss 1.077
* Epoch: [44/60]	 Top 1-err 28.850  Top 5-err 7.940	 Test Loss 1.077
71.15
0.7115
loss: 1.0771679389953612
(35, 0, 98) triplet: 0.18
(35, 0): 0.005
(35, 98): 0.185
not enough sample
* Epoch: [45/60]	 Top 1-err 42.505  Top 5-err 18.742	 Train Loss 2.134
* Epoch: [45/60]	 Top 1-err 28.580  Top 5-err 7.890	 Test Loss 1.082
Current best accuracy (top-1 and 5 error): 28.58 7.89
saving best model...
* Epoch: [45/60]	 Top 1-err 28.580  Top 5-err 7.890	 Test Loss 1.082
71.42
0.7142
loss: 1.0816644081115723
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.01
(35, 98): 0.18
* Epoch: [46/60]	 Top 1-err 43.638  Top 5-err 20.263	 Train Loss 2.111
* Epoch: [46/60]	 Top 1-err 28.510  Top 5-err 7.720	 Test Loss 1.070
Current best accuracy (top-1 and 5 error): 28.51 7.72
saving best model...
* Epoch: [46/60]	 Top 1-err 28.510  Top 5-err 7.720	 Test Loss 1.070
71.49
0.7149
loss: 1.0695632659912109
(35, 0, 98) triplet: 0.18
(35, 0): 0.005
(35, 98): 0.185
* Epoch: [47/60]	 Top 1-err 46.437  Top 5-err 21.942	 Train Loss 2.197
* Epoch: [47/60]	 Top 1-err 28.550  Top 5-err 7.740	 Test Loss 1.065
Current best accuracy (top-1 and 5 error): 28.51 7.72
* Epoch: [47/60]	 Top 1-err 28.550  Top 5-err 7.740	 Test Loss 1.065
71.45
0.7145
loss: 1.0648124450683594
(35, 0, 98) triplet: 0.175
(35, 0): 0.005
(35, 98): 0.18
* Epoch: [48/60]	 Top 1-err 42.745  Top 5-err 18.906	 Train Loss 2.133
* Epoch: [48/60]	 Top 1-err 28.550  Top 5-err 7.820	 Test Loss 1.070
Current best accuracy (top-1 and 5 error): 28.51 7.72
* Epoch: [48/60]	 Top 1-err 28.550  Top 5-err 7.820	 Test Loss 1.070
71.45
0.7145
loss: 1.0703189706802367
(35, 0, 98) triplet: 0.175
(35, 0): 0.005
(35, 98): 0.18
* Epoch: [49/60]	 Top 1-err 43.831  Top 5-err 20.829	 Train Loss 2.089
* Epoch: [49/60]	 Top 1-err 28.400  Top 5-err 7.760	 Test Loss 1.061
Current best accuracy (top-1 and 5 error): 28.4 7.76
saving best model...
* Epoch: [49/60]	 Top 1-err 28.400  Top 5-err 7.760	 Test Loss 1.061
71.6
0.716
loss: 1.0606603142738342
(35, 0, 98) triplet: 0.16
(35, 0): 0.005
(35, 98): 0.165
* Epoch: [50/60]	 Top 1-err 43.181  Top 5-err 19.286	 Train Loss 2.136
* Epoch: [50/60]	 Top 1-err 28.350  Top 5-err 7.720	 Test Loss 1.063
Current best accuracy (top-1 and 5 error): 28.35 7.72
saving best model...
* Epoch: [50/60]	 Top 1-err 28.350  Top 5-err 7.720	 Test Loss 1.063
71.65
0.7165
loss: 1.0634477668762208
(35, 0, 98) triplet: 0.15
(35, 0): 0.01
(35, 98): 0.16
* Epoch: [51/60]	 Top 1-err 46.738  Top 5-err 22.125	 Train Loss 2.219
* Epoch: [51/60]	 Top 1-err 28.450  Top 5-err 7.760	 Test Loss 1.068
Current best accuracy (top-1 and 5 error): 28.35 7.72
* Epoch: [51/60]	 Top 1-err 28.450  Top 5-err 7.760	 Test Loss 1.068
71.55
0.7155
loss: 1.0677027031898498
(35, 0, 98) triplet: 0.145
(35, 0): 0.01
(35, 98): 0.155
* Epoch: [52/60]	 Top 1-err 43.629  Top 5-err 19.048	 Train Loss 2.162
* Epoch: [52/60]	 Top 1-err 28.400  Top 5-err 7.690	 Test Loss 1.059
Current best accuracy (top-1 and 5 error): 28.35 7.72
* Epoch: [52/60]	 Top 1-err 28.400  Top 5-err 7.690	 Test Loss 1.059
71.6
0.716
loss: 1.0594866870880126
(35, 0, 98) triplet: 0.155
(35, 0): 0.005
(35, 98): 0.16
* Epoch: [53/60]	 Top 1-err 40.797  Top 5-err 16.947	 Train Loss 2.119
* Epoch: [53/60]	 Top 1-err 28.410  Top 5-err 7.660	 Test Loss 1.058
Current best accuracy (top-1 and 5 error): 28.35 7.72
* Epoch: [53/60]	 Top 1-err 28.410  Top 5-err 7.660	 Test Loss 1.058
71.59
0.7159
loss: 1.0578479234695435
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.005
(35, 98): 0.16999999999999998
not enough sample
* Epoch: [54/60]	 Top 1-err 42.913  Top 5-err 19.061	 Train Loss 2.159
* Epoch: [54/60]	 Top 1-err 28.470  Top 5-err 7.760	 Test Loss 1.083
Current best accuracy (top-1 and 5 error): 28.35 7.72
* Epoch: [54/60]	 Top 1-err 28.470  Top 5-err 7.760	 Test Loss 1.083
71.53
0.7153
loss: 1.0826625137329102
(35, 0, 98) triplet: 0.175
(35, 0): 0.005
(35, 98): 0.18
* Epoch: [55/60]	 Top 1-err 43.156  Top 5-err 19.656	 Train Loss 2.112
* Epoch: [55/60]	 Top 1-err 28.090  Top 5-err 7.530	 Test Loss 1.047
Current best accuracy (top-1 and 5 error): 28.09 7.53
saving best model...
* Epoch: [55/60]	 Top 1-err 28.090  Top 5-err 7.530	 Test Loss 1.047
71.91
0.7191
loss: 1.0469134523391723
(35, 0, 98) triplet: 0.16
(35, 0): 0.005
(35, 98): 0.165
not enough sample
* Epoch: [56/60]	 Top 1-err 42.550  Top 5-err 18.162	 Train Loss 2.148
* Epoch: [56/60]	 Top 1-err 28.420  Top 5-err 7.700	 Test Loss 1.068
Current best accuracy (top-1 and 5 error): 28.09 7.53
* Epoch: [56/60]	 Top 1-err 28.420  Top 5-err 7.700	 Test Loss 1.068
71.58
0.7158
loss: 1.0680721939086915
(35, 0, 98) triplet: 0.15
(35, 0): 0.01
(35, 98): 0.16
* Epoch: [57/60]	 Top 1-err 43.912  Top 5-err 19.717	 Train Loss 2.169
* Epoch: [57/60]	 Top 1-err 28.100  Top 5-err 7.460	 Test Loss 1.048
Current best accuracy (top-1 and 5 error): 28.09 7.53
* Epoch: [57/60]	 Top 1-err 28.100  Top 5-err 7.460	 Test Loss 1.048
71.9
0.719
loss: 1.0482325353622437
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [58/60]	 Top 1-err 43.000  Top 5-err 19.469	 Train Loss 2.095
* Epoch: [58/60]	 Top 1-err 28.080  Top 5-err 7.520	 Test Loss 1.044
Current best accuracy (top-1 and 5 error): 28.08 7.52
saving best model...
* Epoch: [58/60]	 Top 1-err 28.080  Top 5-err 7.520	 Test Loss 1.044
71.92
0.7192
loss: 1.043502332687378
(35, 0, 98) triplet: 0.15
(35, 0): 0.005
(35, 98): 0.155
* Epoch: [59/60]	 Top 1-err 41.776  Top 5-err 18.308	 Train Loss 2.091
* Epoch: [59/60]	 Top 1-err 28.110  Top 5-err 7.610	 Test Loss 1.058
Current best accuracy (top-1 and 5 error): 28.08 7.52
* Epoch: [59/60]	 Top 1-err 28.110  Top 5-err 7.610	 Test Loss 1.058
71.89
0.7189
loss: 1.0581940906524658
(35, 0, 98) triplet: 0.155
(35, 0): 0.005
(35, 98): 0.16
Best accuracy (top-1 and 5 error): 28.08 7.52
=> loading checkpoint 'runs/cifar100_resnet_bias/model_best.pth.tar'
* Epoch: [-1/60]	 Top 1-err 28.080  Top 5-err 7.520	 Test Loss 1.044
71.92
0.7192
loss: 1.0435023044586182
(35, 0, 98) triplet: 0.15
(35, 0): 0.005
(35, 98): 0.155
python3 cifar100_repair_bias.py --net_type resnet --dataset cifar100 --depth 50 --batch_size 256 --lr 0.1 --expname cifar100_resnet_bias --epochs 60 --beta 1.0 --cutmix_prob 1 --pretrained ./runs/cifar100_resnet_2/model_best.pth.tar --expid 0 --first 35 --second 0 --third 98 --lam 2
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
=> creating model 'resnet'
True
=> loading checkpoint './runs/cifar100_resnet_2/model_best.pth.tar'
=> loaded checkpoint './runs/cifar100_resnet_2/model_best.pth.tar'
the number of model parameters: 521716
* Epoch: [0/60]	 Top 1-err 74.356  Top 5-err 48.649	 Train Loss 3.547
* Epoch: [0/60]	 Top 1-err 55.440  Top 5-err 24.820	 Test Loss 2.101
* Epoch: [0/60]	 Top 1-err 55.440  Top 5-err 24.820	 Test Loss 2.101
44.56
0.4456
loss: 2.101412269592285
(35, 0, 98) triplet: 0.4
(35, 0): 0.0
(35, 98): 0.4
* Epoch: [1/60]	 Top 1-err 68.079  Top 5-err 41.592	 Train Loss 3.052
* Epoch: [1/60]	 Top 1-err 53.530  Top 5-err 22.960	 Test Loss 2.070
* Epoch: [1/60]	 Top 1-err 53.530  Top 5-err 22.960	 Test Loss 2.070
46.47
0.4647
loss: 2.0700353746414186
(35, 0, 98) triplet: 0.045
(35, 0): 0.115
(35, 98): 0.16
* Epoch: [2/60]	 Top 1-err 63.162  Top 5-err 35.655	 Train Loss 2.877
* Epoch: [2/60]	 Top 1-err 43.860  Top 5-err 16.370	 Test Loss 1.643
* Epoch: [2/60]	 Top 1-err 43.860  Top 5-err 16.370	 Test Loss 1.643
56.14
0.5614
loss: 1.64295773563385
(35, 0, 98) triplet: 0.004999999999999977
(35, 0): 0.23500000000000001
(35, 98): 0.24
* Epoch: [3/60]	 Top 1-err 63.270  Top 5-err 36.005	 Train Loss 2.886
* Epoch: [3/60]	 Top 1-err 57.920  Top 5-err 28.660	 Test Loss 2.373
* Epoch: [3/60]	 Top 1-err 57.920  Top 5-err 28.660	 Test Loss 2.373
42.08
0.4208
loss: 2.3726443405151367
(35, 0, 98) triplet: 0.04000000000000001
(35, 0): 0.09
(35, 98): 0.13
not enough sample
not enough sample
* Epoch: [4/60]	 Top 1-err 64.552  Top 5-err 36.339	 Train Loss 2.929
* Epoch: [4/60]	 Top 1-err 49.750  Top 5-err 20.440	 Test Loss 1.887
* Epoch: [4/60]	 Top 1-err 49.750  Top 5-err 20.440	 Test Loss 1.887
50.25
0.5025
loss: 1.886617124938965
(35, 0, 98) triplet: 0.16
(35, 0): 0.39
(35, 98): 0.23
* Epoch: [5/60]	 Top 1-err 59.326  Top 5-err 31.084	 Train Loss 2.755
* Epoch: [5/60]	 Top 1-err 43.100  Top 5-err 16.220	 Test Loss 1.662
* Epoch: [5/60]	 Top 1-err 43.100  Top 5-err 16.220	 Test Loss 1.662
56.9
0.569
loss: 1.6618114379882813
(35, 0, 98) triplet: 0.04000000000000001
(35, 0): 0.08499999999999999
(35, 98): 0.125
* Epoch: [6/60]	 Top 1-err 59.843  Top 5-err 32.289	 Train Loss 2.785
* Epoch: [6/60]	 Top 1-err 50.270  Top 5-err 21.080	 Test Loss 1.951
* Epoch: [6/60]	 Top 1-err 50.270  Top 5-err 21.080	 Test Loss 1.951
49.73
0.4973
loss: 1.9508938428878784
(35, 0, 98) triplet: 0.07
(35, 0): 0.125
(35, 98): 0.055
* Epoch: [7/60]	 Top 1-err 58.725  Top 5-err 31.883	 Train Loss 2.692
* Epoch: [7/60]	 Top 1-err 44.290  Top 5-err 17.300	 Test Loss 1.692
* Epoch: [7/60]	 Top 1-err 44.290  Top 5-err 17.300	 Test Loss 1.692
55.71
0.5571
loss: 1.6921289228439331
(35, 0, 98) triplet: 0.24
(35, 0): 0.005
(35, 98): 0.245
* Epoch: [8/60]	 Top 1-err 60.476  Top 5-err 32.983	 Train Loss 2.730
* Epoch: [8/60]	 Top 1-err 42.350  Top 5-err 14.990	 Test Loss 1.579
* Epoch: [8/60]	 Top 1-err 42.350  Top 5-err 14.990	 Test Loss 1.579
57.65
0.5765
loss: 1.5792302291870117
(35, 0, 98) triplet: 0.095
(35, 0): 0.065
(35, 98): 0.16
* Epoch: [9/60]	 Top 1-err 60.861  Top 5-err 33.279	 Train Loss 2.781
* Epoch: [9/60]	 Top 1-err 44.760  Top 5-err 16.350	 Test Loss 1.666
* Epoch: [9/60]	 Top 1-err 44.760  Top 5-err 16.350	 Test Loss 1.666
55.24
0.5524
loss: 1.6657716709136963
(35, 0, 98) triplet: 0.045000000000000026
(35, 0): 0.15500000000000003
(35, 98): 0.11
* Epoch: [10/60]	 Top 1-err 60.382  Top 5-err 32.533	 Train Loss 2.757
* Epoch: [10/60]	 Top 1-err 45.030  Top 5-err 17.380	 Test Loss 1.722
* Epoch: [10/60]	 Top 1-err 45.030  Top 5-err 17.380	 Test Loss 1.722
54.97
0.5497
loss: 1.7222270992279052
(35, 0, 98) triplet: 0.11500000000000002
(35, 0): 0.1
(35, 98): 0.21500000000000002
* Epoch: [11/60]	 Top 1-err 61.154  Top 5-err 32.839	 Train Loss 2.813
* Epoch: [11/60]	 Top 1-err 44.880  Top 5-err 16.150	 Test Loss 1.666
* Epoch: [11/60]	 Top 1-err 44.880  Top 5-err 16.150	 Test Loss 1.666
55.12
0.5512
loss: 1.665805115699768
(35, 0, 98) triplet: 0.030000000000000027
(35, 0): 0.27
(35, 98): 0.24
not enough sample
* Epoch: [12/60]	 Top 1-err 61.477  Top 5-err 34.159	 Train Loss 2.819
* Epoch: [12/60]	 Top 1-err 40.970  Top 5-err 14.350	 Test Loss 1.516
* Epoch: [12/60]	 Top 1-err 40.970  Top 5-err 14.350	 Test Loss 1.516
59.03
0.5903
loss: 1.5157537830352783
(35, 0, 98) triplet: 0.075
(35, 0): 0.08
(35, 98): 0.155
* Epoch: [13/60]	 Top 1-err 59.233  Top 5-err 31.649	 Train Loss 2.735
* Epoch: [13/60]	 Top 1-err 45.470  Top 5-err 16.870	 Test Loss 1.689
* Epoch: [13/60]	 Top 1-err 45.470  Top 5-err 16.870	 Test Loss 1.689
54.53
0.5453
loss: 1.6890742893218995
(35, 0, 98) triplet: 0.32
(35, 0): 0.055
(35, 98): 0.375
* Epoch: [14/60]	 Top 1-err 59.388  Top 5-err 31.366	 Train Loss 2.743
* Epoch: [14/60]	 Top 1-err 38.900  Top 5-err 12.770	 Test Loss 1.450
* Epoch: [14/60]	 Top 1-err 38.900  Top 5-err 12.770	 Test Loss 1.450
61.1
0.611
loss: 1.449599422454834
(35, 0, 98) triplet: 0.060000000000000005
(35, 0): 0.045000000000000005
(35, 98): 0.10500000000000001
* Epoch: [15/60]	 Top 1-err 62.641  Top 5-err 34.866	 Train Loss 2.796
* Epoch: [15/60]	 Top 1-err 45.560  Top 5-err 17.450	 Test Loss 1.735
* Epoch: [15/60]	 Top 1-err 45.560  Top 5-err 17.450	 Test Loss 1.735
54.44
0.5444
loss: 1.7353552585601806
(35, 0, 98) triplet: 0.11
(35, 0): 0.08
(35, 98): 0.19
* Epoch: [16/60]	 Top 1-err 59.210  Top 5-err 31.288	 Train Loss 2.766
* Epoch: [16/60]	 Top 1-err 44.020  Top 5-err 16.540	 Test Loss 1.679
* Epoch: [16/60]	 Top 1-err 44.020  Top 5-err 16.540	 Test Loss 1.679
55.98
0.5598
loss: 1.678721831703186
(35, 0, 98) triplet: 0.11
(35, 0): 0.08
(35, 98): 0.19
not enough sample
* Epoch: [17/60]	 Top 1-err 60.355  Top 5-err 32.546	 Train Loss 2.714
* Epoch: [17/60]	 Top 1-err 41.380  Top 5-err 14.870	 Test Loss 1.546
* Epoch: [17/60]	 Top 1-err 41.380  Top 5-err 14.870	 Test Loss 1.546
58.62
0.5862
loss: 1.546051050376892
(35, 0, 98) triplet: 0.18
(35, 0): 0.065
(35, 98): 0.245
* Epoch: [18/60]	 Top 1-err 60.697  Top 5-err 32.847	 Train Loss 2.753
* Epoch: [18/60]	 Top 1-err 44.780  Top 5-err 16.310	 Test Loss 1.684
* Epoch: [18/60]	 Top 1-err 44.780  Top 5-err 16.310	 Test Loss 1.684
55.22
0.5522
loss: 1.6840819654464723
(35, 0, 98) triplet: 0.175
(35, 0): 0.035
(35, 98): 0.21
* Epoch: [19/60]	 Top 1-err 59.284  Top 5-err 32.235	 Train Loss 2.683
* Epoch: [19/60]	 Top 1-err 43.190  Top 5-err 15.890	 Test Loss 1.635
* Epoch: [19/60]	 Top 1-err 43.190  Top 5-err 15.890	 Test Loss 1.635
56.81
0.5681
loss: 1.6353716312408448
(35, 0, 98) triplet: 0.115
(35, 0): 0.08
(35, 98): 0.195
* Epoch: [20/60]	 Top 1-err 56.883  Top 5-err 28.852	 Train Loss 2.627
* Epoch: [20/60]	 Top 1-err 44.910  Top 5-err 15.760	 Test Loss 1.692
* Epoch: [20/60]	 Top 1-err 44.910  Top 5-err 15.760	 Test Loss 1.692
55.09
0.5509
loss: 1.6915330425262451
(35, 0, 98) triplet: 0.06500000000000002
(35, 0): 0.08
(35, 98): 0.14500000000000002
* Epoch: [21/60]	 Top 1-err 60.784  Top 5-err 33.262	 Train Loss 2.727
* Epoch: [21/60]	 Top 1-err 41.730  Top 5-err 15.220	 Test Loss 1.578
* Epoch: [21/60]	 Top 1-err 41.730  Top 5-err 15.220	 Test Loss 1.578
58.27
0.5827
loss: 1.5777226722717286
(35, 0, 98) triplet: 0.17999999999999997
(35, 0): 0.345
(35, 98): 0.165
not enough sample
* Epoch: [22/60]	 Top 1-err 56.169  Top 5-err 27.935	 Train Loss 2.642
* Epoch: [22/60]	 Top 1-err 41.910  Top 5-err 15.800	 Test Loss 1.605
* Epoch: [22/60]	 Top 1-err 41.910  Top 5-err 15.800	 Test Loss 1.605
58.09
0.5809
loss: 1.6052208599090576
(35, 0, 98) triplet: 0.11499999999999999
(35, 0): 0.03
(35, 98): 0.145
* Epoch: [23/60]	 Top 1-err 59.008  Top 5-err 30.920	 Train Loss 2.687
* Epoch: [23/60]	 Top 1-err 42.290  Top 5-err 15.840	 Test Loss 1.595
* Epoch: [23/60]	 Top 1-err 42.290  Top 5-err 15.840	 Test Loss 1.595
57.71
0.5771
loss: 1.5953687896728517
(35, 0, 98) triplet: 0.15000000000000002
(35, 0): 0.105
(35, 98): 0.255
* Epoch: [24/60]	 Top 1-err 58.461  Top 5-err 30.699	 Train Loss 2.689
* Epoch: [24/60]	 Top 1-err 41.660  Top 5-err 14.930	 Test Loss 1.566
* Epoch: [24/60]	 Top 1-err 41.660  Top 5-err 14.930	 Test Loss 1.566
58.34
0.5834
loss: 1.5658740242004394
(35, 0, 98) triplet: 0.010000000000000002
(35, 0): 0.025
(35, 98): 0.035
* Epoch: [25/60]	 Top 1-err 58.138  Top 5-err 29.919	 Train Loss 2.694
* Epoch: [25/60]	 Top 1-err 43.670  Top 5-err 15.850	 Test Loss 1.634
* Epoch: [25/60]	 Top 1-err 43.670  Top 5-err 15.850	 Test Loss 1.634
56.33
0.5633
loss: 1.6344268962860107
(35, 0, 98) triplet: 0.08500000000000002
(35, 0): 0.195
(35, 98): 0.28
* Epoch: [26/60]	 Top 1-err 57.484  Top 5-err 28.984	 Train Loss 2.672
* Epoch: [26/60]	 Top 1-err 40.040  Top 5-err 13.010	 Test Loss 1.476
* Epoch: [26/60]	 Top 1-err 40.040  Top 5-err 13.010	 Test Loss 1.476
59.96
0.5996
loss: 1.476477437210083
(35, 0, 98) triplet: 0.05
(35, 0): 0.06
(35, 98): 0.11
* Epoch: [27/60]	 Top 1-err 57.743  Top 5-err 29.957	 Train Loss 2.683
* Epoch: [27/60]	 Top 1-err 44.410  Top 5-err 17.750	 Test Loss 1.750
* Epoch: [27/60]	 Top 1-err 44.410  Top 5-err 17.750	 Test Loss 1.750
55.59
0.5559
loss: 1.7498738983154296
(35, 0, 98) triplet: 0.17
(35, 0): 0.060000000000000005
(35, 98): 0.23
* Epoch: [28/60]	 Top 1-err 60.268  Top 5-err 32.191	 Train Loss 2.740
* Epoch: [28/60]	 Top 1-err 40.640  Top 5-err 13.930	 Test Loss 1.527
* Epoch: [28/60]	 Top 1-err 40.640  Top 5-err 13.930	 Test Loss 1.527
59.36
0.5936
loss: 1.527476840209961
(35, 0, 98) triplet: 0.035
(35, 0): 0.065
(35, 98): 0.1
* Epoch: [29/60]	 Top 1-err 58.493  Top 5-err 29.838	 Train Loss 2.680
* Epoch: [29/60]	 Top 1-err 43.540  Top 5-err 15.690	 Test Loss 1.621
* Epoch: [29/60]	 Top 1-err 43.540  Top 5-err 15.690	 Test Loss 1.621
56.46
0.5646
loss: 1.620660466003418
(35, 0, 98) triplet: 0.095
(35, 0): 0.04
(35, 98): 0.135
* Epoch: [30/60]	 Top 1-err 54.562  Top 5-err 27.603	 Train Loss 2.542
* Epoch: [30/60]	 Top 1-err 33.440  Top 5-err 9.850	 Test Loss 1.233
* Epoch: [30/60]	 Top 1-err 33.440  Top 5-err 9.850	 Test Loss 1.233
66.56
0.6656
loss: 1.232903656387329
(35, 0, 98) triplet: 0.075
(35, 0): 0.105
(35, 98): 0.18
* Epoch: [31/60]	 Top 1-err 52.629  Top 5-err 26.369	 Train Loss 2.445
* Epoch: [31/60]	 Top 1-err 32.600  Top 5-err 9.490	 Test Loss 1.206
* Epoch: [31/60]	 Top 1-err 32.600  Top 5-err 9.490	 Test Loss 1.206
67.4
0.674
loss: 1.2056343723297118
(35, 0, 98) triplet: 0.08499999999999999
(35, 0): 0.07500000000000001
(35, 98): 0.16
* Epoch: [32/60]	 Top 1-err 49.592  Top 5-err 23.294	 Train Loss 2.423
* Epoch: [32/60]	 Top 1-err 31.580  Top 5-err 8.830	 Test Loss 1.180
* Epoch: [32/60]	 Top 1-err 31.580  Top 5-err 8.830	 Test Loss 1.180
68.42
0.6842
loss: 1.1801028896331787
(35, 0, 98) triplet: 0.09000000000000001
(35, 0): 0.045
(35, 98): 0.135
not enough sample
* Epoch: [33/60]	 Top 1-err 52.883  Top 5-err 25.765	 Train Loss 2.462
* Epoch: [33/60]	 Top 1-err 32.320  Top 5-err 9.300	 Test Loss 1.191
* Epoch: [33/60]	 Top 1-err 32.320  Top 5-err 9.300	 Test Loss 1.191
67.68
0.6768
loss: 1.191186397075653
(35, 0, 98) triplet: 0.044999999999999984
(35, 0): 0.125
(35, 98): 0.16999999999999998
* Epoch: [34/60]	 Top 1-err 49.156  Top 5-err 23.204	 Train Loss 2.382
* Epoch: [34/60]	 Top 1-err 31.690  Top 5-err 8.980	 Test Loss 1.179
* Epoch: [34/60]	 Top 1-err 31.690  Top 5-err 8.980	 Test Loss 1.179
68.31
0.6831
loss: 1.179443949508667
(35, 0, 98) triplet: 0.07000000000000002
(35, 0): 0.08
(35, 98): 0.15000000000000002
* Epoch: [35/60]	 Top 1-err 51.938  Top 5-err 25.576	 Train Loss 2.447
* Epoch: [35/60]	 Top 1-err 31.420  Top 5-err 8.840	 Test Loss 1.174
* Epoch: [35/60]	 Top 1-err 31.420  Top 5-err 8.840	 Test Loss 1.174
68.58
0.6858
loss: 1.1738563323020934
(35, 0, 98) triplet: 0.08500000000000002
(35, 0): 0.06
(35, 98): 0.14500000000000002
* Epoch: [36/60]	 Top 1-err 48.107  Top 5-err 22.444	 Train Loss 2.347
* Epoch: [36/60]	 Top 1-err 31.580  Top 5-err 8.740	 Test Loss 1.179
* Epoch: [36/60]	 Top 1-err 31.580  Top 5-err 8.740	 Test Loss 1.179
68.42
0.6842
loss: 1.1793565841674805
(35, 0, 98) triplet: 0.11
(35, 0): 0.05
(35, 98): 0.16
not enough sample
* Epoch: [37/60]	 Top 1-err 47.201  Top 5-err 21.679	 Train Loss 2.319
* Epoch: [37/60]	 Top 1-err 30.800  Top 5-err 8.420	 Test Loss 1.135
* Epoch: [37/60]	 Top 1-err 30.800  Top 5-err 8.420	 Test Loss 1.135
69.2
0.692
loss: 1.1346287578582763
(35, 0, 98) triplet: 0.04999999999999999
(35, 0): 0.07
(35, 98): 0.12
* Epoch: [38/60]	 Top 1-err 49.598  Top 5-err 23.400	 Train Loss 2.374
* Epoch: [38/60]	 Top 1-err 31.290  Top 5-err 8.860	 Test Loss 1.170
* Epoch: [38/60]	 Top 1-err 31.290  Top 5-err 8.860	 Test Loss 1.170
68.71
0.6871
loss: 1.169733660697937
(35, 0, 98) triplet: 0.185
(35, 0): 0.030000000000000002
(35, 98): 0.215
* Epoch: [39/60]	 Top 1-err 51.237  Top 5-err 24.991	 Train Loss 2.428
* Epoch: [39/60]	 Top 1-err 30.710  Top 5-err 8.550	 Test Loss 1.150
* Epoch: [39/60]	 Top 1-err 30.710  Top 5-err 8.550	 Test Loss 1.150
69.29
0.6929
loss: 1.1504420700073241
(35, 0, 98) triplet: 0.07500000000000001
(35, 0): 0.06
(35, 98): 0.135
not enough sample
* Epoch: [40/60]	 Top 1-err 49.671  Top 5-err 23.683	 Train Loss 2.395
* Epoch: [40/60]	 Top 1-err 30.780  Top 5-err 8.740	 Test Loss 1.168
* Epoch: [40/60]	 Top 1-err 30.780  Top 5-err 8.740	 Test Loss 1.168
69.22
0.6922
loss: 1.1678634452819825
(35, 0, 98) triplet: 0.13999999999999999
(35, 0): 0.035
(35, 98): 0.175
* Epoch: [41/60]	 Top 1-err 48.785  Top 5-err 22.945	 Train Loss 2.376
* Epoch: [41/60]	 Top 1-err 30.400  Top 5-err 8.520	 Test Loss 1.139
* Epoch: [41/60]	 Top 1-err 30.400  Top 5-err 8.520	 Test Loss 1.139
69.6
0.696
loss: 1.139403973007202
(35, 0, 98) triplet: 0.11000000000000001
(35, 0): 0.03
(35, 98): 0.14
* Epoch: [42/60]	 Top 1-err 48.173  Top 5-err 22.956	 Train Loss 2.331
* Epoch: [42/60]	 Top 1-err 30.590  Top 5-err 8.540	 Test Loss 1.141
* Epoch: [42/60]	 Top 1-err 30.590  Top 5-err 8.540	 Test Loss 1.141
69.41
0.6941
loss: 1.1411370434761048
(35, 0, 98) triplet: 0.17
(35, 0): 0.025
(35, 98): 0.195
* Epoch: [43/60]	 Top 1-err 48.196  Top 5-err 22.578	 Train Loss 2.332
* Epoch: [43/60]	 Top 1-err 30.680  Top 5-err 8.470	 Test Loss 1.141
* Epoch: [43/60]	 Top 1-err 30.680  Top 5-err 8.470	 Test Loss 1.141
69.32
0.6932
loss: 1.1406268828392028
(35, 0, 98) triplet: 0.17
(35, 0): 0.02
(35, 98): 0.19
* Epoch: [44/60]	 Top 1-err 49.573  Top 5-err 24.074	 Train Loss 2.334
* Epoch: [44/60]	 Top 1-err 30.150  Top 5-err 8.450	 Test Loss 1.141
* Epoch: [44/60]	 Top 1-err 30.150  Top 5-err 8.450	 Test Loss 1.141
69.85
0.6985
loss: 1.1406966152191163
(35, 0, 98) triplet: 0.10500000000000001
(35, 0): 0.025
(35, 98): 0.13
not enough sample
* Epoch: [45/60]	 Top 1-err 46.292  Top 5-err 21.247	 Train Loss 2.301
* Epoch: [45/60]	 Top 1-err 30.200  Top 5-err 8.200	 Test Loss 1.135
Current best accuracy (top-1 and 5 error): 30.2 8.2
saving best model...
* Epoch: [45/60]	 Top 1-err 30.200  Top 5-err 8.200	 Test Loss 1.135
69.8
0.698
loss: 1.1354062057495118
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.01
(35, 98): 0.18
* Epoch: [46/60]	 Top 1-err 47.416  Top 5-err 22.696	 Train Loss 2.276
* Epoch: [46/60]	 Top 1-err 29.840  Top 5-err 8.110	 Test Loss 1.124
Current best accuracy (top-1 and 5 error): 29.84 8.11
saving best model...
* Epoch: [46/60]	 Top 1-err 29.840  Top 5-err 8.110	 Test Loss 1.124
70.16
0.7016
loss: 1.12375625
(35, 0, 98) triplet: 0.14
(35, 0): 0.02
(35, 98): 0.16
* Epoch: [47/60]	 Top 1-err 50.161  Top 5-err 24.683	 Train Loss 2.358
* Epoch: [47/60]	 Top 1-err 29.830  Top 5-err 7.980	 Test Loss 1.116
Current best accuracy (top-1 and 5 error): 29.83 7.98
saving best model...
* Epoch: [47/60]	 Top 1-err 29.830  Top 5-err 7.980	 Test Loss 1.116
70.17
0.7017
loss: 1.116120651626587
(35, 0, 98) triplet: 0.175
(35, 0): 0.005
(35, 98): 0.18
* Epoch: [48/60]	 Top 1-err 46.560  Top 5-err 21.568	 Train Loss 2.300
* Epoch: [48/60]	 Top 1-err 30.050  Top 5-err 8.030	 Test Loss 1.123
Current best accuracy (top-1 and 5 error): 29.83 7.98
* Epoch: [48/60]	 Top 1-err 30.050  Top 5-err 8.030	 Test Loss 1.123
69.95
0.6995
loss: 1.1232435573577881
(35, 0, 98) triplet: 0.185
(35, 0): 0.005
(35, 98): 0.19
* Epoch: [49/60]	 Top 1-err 47.554  Top 5-err 23.343	 Train Loss 2.256
* Epoch: [49/60]	 Top 1-err 29.790  Top 5-err 8.030	 Test Loss 1.117
Current best accuracy (top-1 and 5 error): 29.79 8.03
saving best model...
* Epoch: [49/60]	 Top 1-err 29.790  Top 5-err 8.030	 Test Loss 1.117
70.21
0.7021
loss: 1.1174913164138793
(35, 0, 98) triplet: 0.175
(35, 0): 0.005
(35, 98): 0.18
* Epoch: [50/60]	 Top 1-err 46.961  Top 5-err 22.072	 Train Loss 2.298
* Epoch: [50/60]	 Top 1-err 29.840  Top 5-err 8.000	 Test Loss 1.119
Current best accuracy (top-1 and 5 error): 29.79 8.03
* Epoch: [50/60]	 Top 1-err 29.840  Top 5-err 8.000	 Test Loss 1.119
70.16
0.7016
loss: 1.1190359958648681
(35, 0, 98) triplet: 0.175
(35, 0): 0.005
(35, 98): 0.18
* Epoch: [51/60]	 Top 1-err 50.421  Top 5-err 24.819	 Train Loss 2.381
* Epoch: [51/60]	 Top 1-err 29.840  Top 5-err 8.010	 Test Loss 1.118
Current best accuracy (top-1 and 5 error): 29.79 8.03
* Epoch: [51/60]	 Top 1-err 29.840  Top 5-err 8.010	 Test Loss 1.118
70.16
0.7016
loss: 1.1182116749763489
(35, 0, 98) triplet: 0.165
(35, 0): 0.005
(35, 98): 0.17
* Epoch: [52/60]	 Top 1-err 47.344  Top 5-err 21.691	 Train Loss 2.324
* Epoch: [52/60]	 Top 1-err 29.700  Top 5-err 8.050	 Test Loss 1.114
Current best accuracy (top-1 and 5 error): 29.7 8.05
saving best model...
* Epoch: [52/60]	 Top 1-err 29.700  Top 5-err 8.050	 Test Loss 1.114
70.3
0.703
loss: 1.1141842048645019
(35, 0, 98) triplet: 0.155
(35, 0): 0.01
(35, 98): 0.165
* Epoch: [53/60]	 Top 1-err 45.030  Top 5-err 19.581	 Train Loss 2.284
* Epoch: [53/60]	 Top 1-err 29.440  Top 5-err 7.930	 Test Loss 1.110
Current best accuracy (top-1 and 5 error): 29.44 7.93
saving best model...
* Epoch: [53/60]	 Top 1-err 29.440  Top 5-err 7.930	 Test Loss 1.110
70.56
0.7056
loss: 1.1100280685424804
(35, 0, 98) triplet: 0.15
(35, 0): 0.01
(35, 98): 0.16
not enough sample
* Epoch: [54/60]	 Top 1-err 46.785  Top 5-err 21.556	 Train Loss 2.324
* Epoch: [54/60]	 Top 1-err 29.970  Top 5-err 8.180	 Test Loss 1.134
Current best accuracy (top-1 and 5 error): 29.44 7.93
* Epoch: [54/60]	 Top 1-err 29.970  Top 5-err 8.180	 Test Loss 1.134
70.03
0.7003
loss: 1.1339390333175658
(35, 0, 98) triplet: 0.15000000000000002
(35, 0): 0.015
(35, 98): 0.165
* Epoch: [55/60]	 Top 1-err 47.144  Top 5-err 22.286	 Train Loss 2.278
* Epoch: [55/60]	 Top 1-err 29.770  Top 5-err 8.030	 Test Loss 1.103
Current best accuracy (top-1 and 5 error): 29.44 7.93
* Epoch: [55/60]	 Top 1-err 29.770  Top 5-err 8.030	 Test Loss 1.103
70.23
0.7023
loss: 1.1026427582740783
(35, 0, 98) triplet: 0.185
(35, 0): 0.005
(35, 98): 0.19
not enough sample
* Epoch: [56/60]	 Top 1-err 46.158  Top 5-err 20.916	 Train Loss 2.313
* Epoch: [56/60]	 Top 1-err 29.770  Top 5-err 8.100	 Test Loss 1.117
Current best accuracy (top-1 and 5 error): 29.44 7.93
* Epoch: [56/60]	 Top 1-err 29.770  Top 5-err 8.100	 Test Loss 1.117
70.23
0.7023
loss: 1.1172152669906616
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.01
(35, 98): 0.18
* Epoch: [57/60]	 Top 1-err 47.629  Top 5-err 22.229	 Train Loss 2.329
* Epoch: [57/60]	 Top 1-err 29.730  Top 5-err 8.100	 Test Loss 1.104
Current best accuracy (top-1 and 5 error): 29.44 7.93
* Epoch: [57/60]	 Top 1-err 29.730  Top 5-err 8.100	 Test Loss 1.104
70.27
0.7027
loss: 1.1042092877388001
(35, 0, 98) triplet: 0.19
(35, 0): 0.005
(35, 98): 0.195
* Epoch: [58/60]	 Top 1-err 46.974  Top 5-err 22.068	 Train Loss 2.263
* Epoch: [58/60]	 Top 1-err 29.300  Top 5-err 7.800	 Test Loss 1.096
Current best accuracy (top-1 and 5 error): 29.3 7.8
saving best model...
* Epoch: [58/60]	 Top 1-err 29.300  Top 5-err 7.800	 Test Loss 1.096
70.7
0.707
loss: 1.0957347940444946
(35, 0, 98) triplet: 0.13
(35, 0): 0.015
(35, 98): 0.14500000000000002
* Epoch: [59/60]	 Top 1-err 45.654  Top 5-err 20.816	 Train Loss 2.254
* Epoch: [59/60]	 Top 1-err 29.770  Top 5-err 8.090	 Test Loss 1.110
Current best accuracy (top-1 and 5 error): 29.3 7.8
* Epoch: [59/60]	 Top 1-err 29.770  Top 5-err 8.090	 Test Loss 1.110
70.23
0.7023
loss: 1.109834190940857
(35, 0, 98) triplet: 0.17
(35, 0): 0.005
(35, 98): 0.17500000000000002
Best accuracy (top-1 and 5 error): 29.3 7.8
=> loading checkpoint 'runs/cifar100_resnet_bias/model_best.pth.tar'
* Epoch: [-1/60]	 Top 1-err 29.300  Top 5-err 7.800	 Test Loss 1.096
70.7
0.707
loss: 1.0957347972869873
(35, 0, 98) triplet: 0.13
(35, 0): 0.015
(35, 98): 0.14500000000000002

python3 cifar100_repair_bias.py --net_type resnet --dataset cifar100 --depth 50 --batch_size 256 --lr 0.1 --expname cifar100_resnet_1 --epochs 60 --beta 1.0 --cutmix_prob 0 --pretrained ./runs/cifar100_resnet_1/model_best.pth.tar --expid 0 --first 35 --second 0 --third 98 --lam 0
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
=> creating model 'resnet'
True
=> loading checkpoint './runs/cifar100_resnet_1/model_best.pth.tar'
=> loaded checkpoint './runs/cifar100_resnet_1/model_best.pth.tar'
the number of model parameters: 521716
* Epoch: [0/60]	 Top 1-err 94.545  Top 5-err 80.944	 Train Loss 4.269
* Epoch: [0/60]	 Top 1-err 96.450  Top 5-err 82.500	 Test Loss 4.273
* Epoch: [0/60]	 Top 1-err 96.450  Top 5-err 82.500	 Test Loss 4.273
3.55
0.0355
loss: 4.272500730133056
(35, 0, 98) triplet: 0.32
(35, 0): 0.07
(35, 98): 0.39
* Epoch: [1/60]	 Top 1-err 93.570  Top 5-err 78.045	 Train Loss 4.162
* Epoch: [1/60]	 Top 1-err 95.520  Top 5-err 82.410	 Test Loss 4.272
* Epoch: [1/60]	 Top 1-err 95.520  Top 5-err 82.410	 Test Loss 4.272
4.48
0.0448
loss: 4.2721479187011715
(35, 0, 98) triplet: 0.27499999999999997
(35, 0): 0.025
(35, 98): 0.3
* Epoch: [2/60]	 Top 1-err 92.023  Top 5-err 74.520	 Train Loss 4.058
* Epoch: [2/60]	 Top 1-err 93.710  Top 5-err 77.060	 Test Loss 4.108
* Epoch: [2/60]	 Top 1-err 93.710  Top 5-err 77.060	 Test Loss 4.108
6.29
0.0629
loss: 4.107787059020996
(35, 0, 98) triplet: 0.28500000000000003
(35, 0): 0.045
(35, 98): 0.33
* Epoch: [3/60]	 Top 1-err 90.895  Top 5-err 72.119	 Train Loss 3.981
* Epoch: [3/60]	 Top 1-err 92.880  Top 5-err 74.930	 Test Loss 4.035
* Epoch: [3/60]	 Top 1-err 92.880  Top 5-err 74.930	 Test Loss 4.035
7.12
0.0712
loss: 4.034927436828613
(35, 0, 98) triplet: 0.33
(35, 0): 0.04
(35, 98): 0.37
* Epoch: [4/60]	 Top 1-err 90.085  Top 5-err 70.391	 Train Loss 3.921
* Epoch: [4/60]	 Top 1-err 91.580  Top 5-err 72.520	 Test Loss 3.978
* Epoch: [4/60]	 Top 1-err 91.580  Top 5-err 72.520	 Test Loss 3.978
8.42
0.0842
loss: 3.9775399723052978
(35, 0, 98) triplet: 0.215
(35, 0): 0.030000000000000002
(35, 98): 0.245
* Epoch: [5/60]	 Top 1-err 89.471  Top 5-err 68.542	 Train Loss 3.855
* Epoch: [5/60]	 Top 1-err 90.910  Top 5-err 70.420	 Test Loss 3.940
* Epoch: [5/60]	 Top 1-err 90.910  Top 5-err 70.420	 Test Loss 3.940
9.09
0.0909
loss: 3.9403729846954345
(35, 0, 98) triplet: 0.17999999999999997
(35, 0): 0.04
(35, 98): 0.21999999999999997
* Epoch: [6/60]	 Top 1-err 88.506  Top 5-err 66.298	 Train Loss 3.779
* Epoch: [6/60]	 Top 1-err 89.460  Top 5-err 67.560	 Test Loss 3.806
* Epoch: [6/60]	 Top 1-err 89.460  Top 5-err 67.560	 Test Loss 3.806
10.54
0.1054
loss: 3.8058236236572265
(35, 0, 98) triplet: 0.22999999999999998
(35, 0): 0.015
(35, 98): 0.245
* Epoch: [7/60]	 Top 1-err 87.459  Top 5-err 64.698	 Train Loss 3.720
* Epoch: [7/60]	 Top 1-err 89.730  Top 5-err 67.020	 Test Loss 3.793
* Epoch: [7/60]	 Top 1-err 89.730  Top 5-err 67.020	 Test Loss 3.793
10.27
0.1027
loss: 3.793252251434326
(35, 0, 98) triplet: 0.205
(35, 0): 0.02
(35, 98): 0.22499999999999998
* Epoch: [8/60]	 Top 1-err 86.534  Top 5-err 63.096	 Train Loss 3.666
* Epoch: [8/60]	 Top 1-err 88.550  Top 5-err 64.830	 Test Loss 3.743
* Epoch: [8/60]	 Top 1-err 88.550  Top 5-err 64.830	 Test Loss 3.743
11.45
0.1145
loss: 3.7427375076293945
(35, 0, 98) triplet: 0.19
(35, 0): 0.015
(35, 98): 0.20500000000000002
* Epoch: [9/60]	 Top 1-err 86.060  Top 5-err 61.963	 Train Loss 3.619
* Epoch: [9/60]	 Top 1-err 89.330  Top 5-err 67.190	 Test Loss 3.856
* Epoch: [9/60]	 Top 1-err 89.330  Top 5-err 67.190	 Test Loss 3.856
10.67
0.1067
loss: 3.8563650691986084
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.04
(35, 98): 0.205
* Epoch: [10/60]	 Top 1-err 85.525  Top 5-err 60.448	 Train Loss 3.572
* Epoch: [10/60]	 Top 1-err 86.910  Top 5-err 61.890	 Test Loss 3.627
* Epoch: [10/60]	 Top 1-err 86.910  Top 5-err 61.890	 Test Loss 3.627
13.09
0.1309
loss: 3.626979913330078
(35, 0, 98) triplet: 0.27
(35, 0): 0.01
(35, 98): 0.28
* Epoch: [11/60]	 Top 1-err 84.243  Top 5-err 58.746	 Train Loss 3.510
* Epoch: [11/60]	 Top 1-err 87.010  Top 5-err 62.290	 Test Loss 3.648
* Epoch: [11/60]	 Top 1-err 87.010  Top 5-err 62.290	 Test Loss 3.648
12.99
0.1299
loss: 3.6484337814331056
(35, 0, 98) triplet: 0.23500000000000001
(35, 0): 0.025
(35, 98): 0.26
* Epoch: [12/60]	 Top 1-err 83.136  Top 5-err 56.691	 Train Loss 3.454
* Epoch: [12/60]	 Top 1-err 86.020  Top 5-err 60.890	 Test Loss 3.594
* Epoch: [12/60]	 Top 1-err 86.020  Top 5-err 60.890	 Test Loss 3.594
13.98
0.1398
loss: 3.594379565811157
(35, 0, 98) triplet: 0.245
(35, 0): 0.01
(35, 98): 0.255
* Epoch: [13/60]	 Top 1-err 82.320  Top 5-err 55.463	 Train Loss 3.401
* Epoch: [13/60]	 Top 1-err 83.750  Top 5-err 57.420	 Test Loss 3.508
* Epoch: [13/60]	 Top 1-err 83.750  Top 5-err 57.420	 Test Loss 3.508
16.25
0.1625
loss: 3.5076867294311525
(35, 0, 98) triplet: 0.19
(35, 0): 0.035
(35, 98): 0.225
not enough sample
* Epoch: [14/60]	 Top 1-err 81.026  Top 5-err 53.674	 Train Loss 3.338
* Epoch: [14/60]	 Top 1-err 82.410  Top 5-err 55.440	 Test Loss 3.405
* Epoch: [14/60]	 Top 1-err 82.410  Top 5-err 55.440	 Test Loss 3.405
17.59
0.1759
loss: 3.404932028579712
(35, 0, 98) triplet: 0.29
(35, 0): 0.005
(35, 98): 0.295
* Epoch: [15/60]	 Top 1-err 80.111  Top 5-err 51.827	 Train Loss 3.269
* Epoch: [15/60]	 Top 1-err 82.250  Top 5-err 54.540	 Test Loss 3.387
* Epoch: [15/60]	 Top 1-err 82.250  Top 5-err 54.540	 Test Loss 3.387
17.75
0.1775
loss: 3.3869230949401854
(35, 0, 98) triplet: 0.26999999999999996
(35, 0): 0.02
(35, 98): 0.29
* Epoch: [16/60]	 Top 1-err 78.255  Top 5-err 49.520	 Train Loss 3.184
* Epoch: [16/60]	 Top 1-err 80.670  Top 5-err 51.540	 Test Loss 3.286
* Epoch: [16/60]	 Top 1-err 80.670  Top 5-err 51.540	 Test Loss 3.286
19.33
0.1933
loss: 3.2857590591430665
(35, 0, 98) triplet: 0.285
(35, 0): 0.01
(35, 98): 0.295
* Epoch: [17/60]	 Top 1-err 76.468  Top 5-err 46.726	 Train Loss 3.090
* Epoch: [17/60]	 Top 1-err 78.730  Top 5-err 50.480	 Test Loss 3.282
* Epoch: [17/60]	 Top 1-err 78.730  Top 5-err 50.480	 Test Loss 3.282
21.27
0.2127
loss: 3.2817434017181397
(35, 0, 98) triplet: 0.23
(35, 0): 0.0
(35, 98): 0.23
* Epoch: [18/60]	 Top 1-err 75.093  Top 5-err 44.273	 Train Loss 2.988
* Epoch: [18/60]	 Top 1-err 79.280  Top 5-err 48.470	 Test Loss 3.284
* Epoch: [18/60]	 Top 1-err 79.280  Top 5-err 48.470	 Test Loss 3.284
20.72
0.2072
loss: 3.283545207977295
(35, 0, 98) triplet: 0.305
(35, 0): 0.045
(35, 98): 0.35
* Epoch: [19/60]	 Top 1-err 72.603  Top 5-err 41.759	 Train Loss 2.872
* Epoch: [19/60]	 Top 1-err 78.430  Top 5-err 49.310	 Test Loss 3.268
* Epoch: [19/60]	 Top 1-err 78.430  Top 5-err 49.310	 Test Loss 3.268
21.57
0.2157
loss: 3.268408973312378
(35, 0, 98) triplet: 0.24
(35, 0): 0.0
(35, 98): 0.24
* Epoch: [20/60]	 Top 1-err 70.876  Top 5-err 39.150	 Train Loss 2.774
* Epoch: [20/60]	 Top 1-err 74.150  Top 5-err 42.880	 Test Loss 2.999
* Epoch: [20/60]	 Top 1-err 74.150  Top 5-err 42.880	 Test Loss 2.999
25.85
0.2585
loss: 2.99896548538208
(35, 0, 98) triplet: 0.31499999999999995
(35, 0): 0.0
(35, 98): 0.31499999999999995
not enough sample
* Epoch: [21/60]	 Top 1-err 68.570  Top 5-err 36.632	 Train Loss 2.660
* Epoch: [21/60]	 Top 1-err 74.280  Top 5-err 42.910	 Test Loss 3.040
* Epoch: [21/60]	 Top 1-err 74.280  Top 5-err 42.910	 Test Loss 3.040
25.72
0.2572
loss: 3.0395932275772095
(35, 0, 98) triplet: 0.32499999999999996
(35, 0): 0.0
(35, 98): 0.32499999999999996
* Epoch: [22/60]	 Top 1-err 66.709  Top 5-err 34.188	 Train Loss 2.567
* Epoch: [22/60]	 Top 1-err 72.280  Top 5-err 39.200	 Test Loss 2.884
* Epoch: [22/60]	 Top 1-err 72.280  Top 5-err 39.200	 Test Loss 2.884
27.72
0.2772
loss: 2.8839024772644044
(35, 0, 98) triplet: 0.31
(35, 0): 0.0
(35, 98): 0.31
* Epoch: [23/60]	 Top 1-err 64.620  Top 5-err 31.929	 Train Loss 2.470
* Epoch: [23/60]	 Top 1-err 68.740  Top 5-err 36.170	 Test Loss 2.751
* Epoch: [23/60]	 Top 1-err 68.740  Top 5-err 36.170	 Test Loss 2.751
31.26
0.3126
loss: 2.7514947048187257
(35, 0, 98) triplet: 0.33
(35, 0): 0.005
(35, 98): 0.335
* Epoch: [24/60]	 Top 1-err 62.626  Top 5-err 30.140	 Train Loss 2.376
* Epoch: [24/60]	 Top 1-err 67.720  Top 5-err 35.440	 Test Loss 2.675
* Epoch: [24/60]	 Top 1-err 67.720  Top 5-err 35.440	 Test Loss 2.675
32.28
0.3228
loss: 2.6749772739410402
(35, 0, 98) triplet: 0.24
(35, 0): 0.0
(35, 98): 0.24
* Epoch: [25/60]	 Top 1-err 61.088  Top 5-err 28.523	 Train Loss 2.303
* Epoch: [25/60]	 Top 1-err 72.830  Top 5-err 42.550	 Test Loss 3.054
* Epoch: [25/60]	 Top 1-err 72.830  Top 5-err 42.550	 Test Loss 3.054
27.17
0.2717
loss: 3.0543482604980468
(35, 0, 98) triplet: 0.22999999999999998
(35, 0): 0.0
(35, 98): 0.22999999999999998
* Epoch: [26/60]	 Top 1-err 59.180  Top 5-err 26.596	 Train Loss 2.216
* Epoch: [26/60]	 Top 1-err 67.520  Top 5-err 35.430	 Test Loss 2.713
* Epoch: [26/60]	 Top 1-err 67.520  Top 5-err 35.430	 Test Loss 2.713
32.48
0.3248
loss: 2.713338473510742
(35, 0, 98) triplet: 0.28
(35, 0): 0.0
(35, 98): 0.28
* Epoch: [27/60]	 Top 1-err 57.671  Top 5-err 25.176	 Train Loss 2.142
* Epoch: [27/60]	 Top 1-err 63.040  Top 5-err 30.790	 Test Loss 2.507
* Epoch: [27/60]	 Top 1-err 63.040  Top 5-err 30.790	 Test Loss 2.507
36.96
0.3696
loss: 2.506792551803589
(35, 0, 98) triplet: 0.25
(35, 0): 0.005
(35, 98): 0.255
* Epoch: [28/60]	 Top 1-err 56.196  Top 5-err 24.071	 Train Loss 2.080
* Epoch: [28/60]	 Top 1-err 63.890  Top 5-err 31.750	 Test Loss 2.557
* Epoch: [28/60]	 Top 1-err 63.890  Top 5-err 31.750	 Test Loss 2.557
36.11
0.3611
loss: 2.5570492809295655
(35, 0, 98) triplet: 0.25
(35, 0): 0.0
(35, 98): 0.25
* Epoch: [29/60]	 Top 1-err 54.941  Top 5-err 22.994	 Train Loss 2.022
* Epoch: [29/60]	 Top 1-err 73.080  Top 5-err 43.530	 Test Loss 3.315
* Epoch: [29/60]	 Top 1-err 73.080  Top 5-err 43.530	 Test Loss 3.315
26.92
0.2692
loss: 3.3152118816375733
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
* Epoch: [30/60]	 Top 1-err 50.106  Top 5-err 19.260	 Train Loss 1.831
* Epoch: [30/60]	 Top 1-err 53.030  Top 5-err 21.760	 Test Loss 1.972
* Epoch: [30/60]	 Top 1-err 53.030  Top 5-err 21.760	 Test Loss 1.972
46.97
0.4697
loss: 1.971879179763794
(35, 0, 98) triplet: 0.23500000000000001
(35, 0): 0.0
(35, 98): 0.23500000000000001
* Epoch: [31/60]	 Top 1-err 49.023  Top 5-err 18.478	 Train Loss 1.782
* Epoch: [31/60]	 Top 1-err 52.700  Top 5-err 21.320	 Test Loss 1.951
* Epoch: [31/60]	 Top 1-err 52.700  Top 5-err 21.320	 Test Loss 1.951
47.3
0.473
loss: 1.9513471139907836
(35, 0, 98) triplet: 0.22500000000000003
(35, 0): 0.0
(35, 98): 0.22500000000000003
* Epoch: [32/60]	 Top 1-err 48.491  Top 5-err 18.190	 Train Loss 1.762
* Epoch: [32/60]	 Top 1-err 52.750  Top 5-err 21.090	 Test Loss 1.949
* Epoch: [32/60]	 Top 1-err 52.750  Top 5-err 21.090	 Test Loss 1.949
47.25
0.4725
loss: 1.949325477218628
(35, 0, 98) triplet: 0.27
(35, 0): 0.0
(35, 98): 0.27
* Epoch: [33/60]	 Top 1-err 48.162  Top 5-err 18.009	 Train Loss 1.749
* Epoch: [33/60]	 Top 1-err 53.860  Top 5-err 21.510	 Test Loss 2.000
* Epoch: [33/60]	 Top 1-err 53.860  Top 5-err 21.510	 Test Loss 2.000
46.14
0.4614
loss: 1.9998316249847412
(35, 0, 98) triplet: 0.24
(35, 0): 0.0
(35, 98): 0.24
* Epoch: [34/60]	 Top 1-err 47.917  Top 5-err 17.697	 Train Loss 1.737
* Epoch: [34/60]	 Top 1-err 53.270  Top 5-err 21.450	 Test Loss 1.990
* Epoch: [34/60]	 Top 1-err 53.270  Top 5-err 21.450	 Test Loss 1.990
46.73
0.4673
loss: 1.9898320209503173
(35, 0, 98) triplet: 0.21000000000000002
(35, 0): 0.0
(35, 98): 0.21000000000000002
* Epoch: [35/60]	 Top 1-err 47.686  Top 5-err 17.442	 Train Loss 1.722
* Epoch: [35/60]	 Top 1-err 52.320  Top 5-err 21.030	 Test Loss 1.957
* Epoch: [35/60]	 Top 1-err 52.320  Top 5-err 21.030	 Test Loss 1.957
47.68
0.4768
loss: 1.9573200691223145
(35, 0, 98) triplet: 0.235
(35, 0): 0.0
(35, 98): 0.235
* Epoch: [36/60]	 Top 1-err 47.267  Top 5-err 17.080	 Train Loss 1.710
* Epoch: [36/60]	 Top 1-err 51.680  Top 5-err 20.430	 Test Loss 1.906
* Epoch: [36/60]	 Top 1-err 51.680  Top 5-err 20.430	 Test Loss 1.906
48.32
0.4832
loss: 1.9062271633148193
(35, 0, 98) triplet: 0.20500000000000002
(35, 0): 0.0
(35, 98): 0.20500000000000002
* Epoch: [37/60]	 Top 1-err 46.951  Top 5-err 16.951	 Train Loss 1.696
* Epoch: [37/60]	 Top 1-err 51.640  Top 5-err 20.630	 Test Loss 1.914
* Epoch: [37/60]	 Top 1-err 51.640  Top 5-err 20.630	 Test Loss 1.914
48.36
0.4836
loss: 1.9140254810333253
(35, 0, 98) triplet: 0.21999999999999997
(35, 0): 0.0
(35, 98): 0.21999999999999997
not enough sample
* Epoch: [38/60]	 Top 1-err 46.789  Top 5-err 16.785	 Train Loss 1.689
* Epoch: [38/60]	 Top 1-err 52.070  Top 5-err 20.320	 Test Loss 1.926
* Epoch: [38/60]	 Top 1-err 52.070  Top 5-err 20.320	 Test Loss 1.926
47.93
0.4793
loss: 1.926037089729309
(35, 0, 98) triplet: 0.255
(35, 0): 0.0
(35, 98): 0.255
* Epoch: [39/60]	 Top 1-err 46.611  Top 5-err 16.638	 Train Loss 1.677
* Epoch: [39/60]	 Top 1-err 51.760  Top 5-err 20.650	 Test Loss 1.937
* Epoch: [39/60]	 Top 1-err 51.760  Top 5-err 20.650	 Test Loss 1.937
48.24
0.4824
loss: 1.9366574113845825
(35, 0, 98) triplet: 0.265
(35, 0): 0.0
(35, 98): 0.265
not enough sample
* Epoch: [40/60]	 Top 1-err 46.165  Top 5-err 16.517	 Train Loss 1.668
* Epoch: [40/60]	 Top 1-err 51.300  Top 5-err 20.190	 Test Loss 1.914
* Epoch: [40/60]	 Top 1-err 51.300  Top 5-err 20.190	 Test Loss 1.914
48.7
0.487
loss: 1.9139374584198
(35, 0, 98) triplet: 0.235
(35, 0): 0.0
(35, 98): 0.235
* Epoch: [41/60]	 Top 1-err 46.105  Top 5-err 16.300	 Train Loss 1.662
* Epoch: [41/60]	 Top 1-err 53.200  Top 5-err 21.870	 Test Loss 2.006
* Epoch: [41/60]	 Top 1-err 53.200  Top 5-err 21.870	 Test Loss 2.006
46.8
0.468
loss: 2.006278351020813
(35, 0, 98) triplet: 0.22499999999999998
(35, 0): 0.0
(35, 98): 0.22499999999999998
* Epoch: [42/60]	 Top 1-err 45.882  Top 5-err 16.341	 Train Loss 1.650
* Epoch: [42/60]	 Top 1-err 50.790  Top 5-err 19.560	 Test Loss 1.891
* Epoch: [42/60]	 Top 1-err 50.790  Top 5-err 19.560	 Test Loss 1.891
49.21
0.4921
loss: 1.890833218383789
(35, 0, 98) triplet: 0.235
(35, 0): 0.0
(35, 98): 0.235
* Epoch: [43/60]	 Top 1-err 45.559  Top 5-err 16.009	 Train Loss 1.640
* Epoch: [43/60]	 Top 1-err 51.630  Top 5-err 20.660	 Test Loss 1.937
* Epoch: [43/60]	 Top 1-err 51.630  Top 5-err 20.660	 Test Loss 1.937
48.37
0.4837
loss: 1.936799615097046
(35, 0, 98) triplet: 0.245
(35, 0): 0.0
(35, 98): 0.245
* Epoch: [44/60]	 Top 1-err 45.140  Top 5-err 15.956	 Train Loss 1.627
* Epoch: [44/60]	 Top 1-err 51.110  Top 5-err 20.050	 Test Loss 1.913
* Epoch: [44/60]	 Top 1-err 51.110  Top 5-err 20.050	 Test Loss 1.913
48.89
0.4889
loss: 1.9130909461975099
(35, 0, 98) triplet: 0.245
(35, 0): 0.0
(35, 98): 0.245
* Epoch: [45/60]	 Top 1-err 44.142  Top 5-err 15.072	 Train Loss 1.586
* Epoch: [45/60]	 Top 1-err 49.900  Top 5-err 19.390	 Test Loss 1.848
Current best accuracy (top-1 and 5 error): 49.9 19.39
saving best model...
* Epoch: [45/60]	 Top 1-err 49.900  Top 5-err 19.390	 Test Loss 1.848
50.1
0.501
loss: 1.8478973752975465
(35, 0, 98) triplet: 0.225
(35, 0): 0.0
(35, 98): 0.225
* Epoch: [46/60]	 Top 1-err 43.986  Top 5-err 15.093	 Train Loss 1.579
* Epoch: [46/60]	 Top 1-err 49.990  Top 5-err 19.140	 Test Loss 1.850
Current best accuracy (top-1 and 5 error): 49.9 19.39
* Epoch: [46/60]	 Top 1-err 49.990  Top 5-err 19.140	 Test Loss 1.850
50.01
0.5001
loss: 1.850095623779297
(35, 0, 98) triplet: 0.22499999999999998
(35, 0): 0.0
(35, 98): 0.22499999999999998
not enough sample
* Epoch: [47/60]	 Top 1-err 44.067  Top 5-err 15.151	 Train Loss 1.578
* Epoch: [47/60]	 Top 1-err 50.050  Top 5-err 19.170	 Test Loss 1.845
Current best accuracy (top-1 and 5 error): 49.9 19.39
* Epoch: [47/60]	 Top 1-err 50.050  Top 5-err 19.170	 Test Loss 1.845
49.95
0.4995
loss: 1.8450375743865968
(35, 0, 98) triplet: 0.23
(35, 0): 0.0
(35, 98): 0.23
* Epoch: [48/60]	 Top 1-err 43.963  Top 5-err 15.002	 Train Loss 1.572
* Epoch: [48/60]	 Top 1-err 49.970  Top 5-err 19.320	 Test Loss 1.850
Current best accuracy (top-1 and 5 error): 49.9 19.39
* Epoch: [48/60]	 Top 1-err 49.970  Top 5-err 19.320	 Test Loss 1.850
50.03
0.5003
loss: 1.8497841445922851
(35, 0, 98) triplet: 0.21
(35, 0): 0.0
(35, 98): 0.21
* Epoch: [49/60]	 Top 1-err 43.795  Top 5-err 14.900	 Train Loss 1.568
* Epoch: [49/60]	 Top 1-err 49.870  Top 5-err 19.070	 Test Loss 1.843
Current best accuracy (top-1 and 5 error): 49.87 19.07
saving best model...
* Epoch: [49/60]	 Top 1-err 49.870  Top 5-err 19.070	 Test Loss 1.843
50.13
0.5013
loss: 1.8434608638763428
(35, 0, 98) triplet: 0.21000000000000002
(35, 0): 0.0
(35, 98): 0.21000000000000002
* Epoch: [50/60]	 Top 1-err 43.827  Top 5-err 14.781	 Train Loss 1.570
* Epoch: [50/60]	 Top 1-err 49.900  Top 5-err 19.130	 Test Loss 1.850
Current best accuracy (top-1 and 5 error): 49.87 19.07
* Epoch: [50/60]	 Top 1-err 49.900  Top 5-err 19.130	 Test Loss 1.850
50.1
0.501
loss: 1.8500292209625244
(35, 0, 98) triplet: 0.215
(35, 0): 0.0
(35, 98): 0.215
* Epoch: [51/60]	 Top 1-err 43.823  Top 5-err 14.853	 Train Loss 1.568
* Epoch: [51/60]	 Top 1-err 49.890  Top 5-err 19.160	 Test Loss 1.844
Current best accuracy (top-1 and 5 error): 49.87 19.07
* Epoch: [51/60]	 Top 1-err 49.890  Top 5-err 19.160	 Test Loss 1.844
50.11
0.5011
loss: 1.8439666515350341
(35, 0, 98) triplet: 0.21
(35, 0): 0.0
(35, 98): 0.21
* Epoch: [52/60]	 Top 1-err 43.659  Top 5-err 14.824	 Train Loss 1.563
* Epoch: [52/60]	 Top 1-err 50.070  Top 5-err 19.070	 Test Loss 1.843
Current best accuracy (top-1 and 5 error): 49.87 19.07
* Epoch: [52/60]	 Top 1-err 50.070  Top 5-err 19.070	 Test Loss 1.843
49.93
0.4993
loss: 1.843067293548584
(35, 0, 98) triplet: 0.215
(35, 0): 0.0
(35, 98): 0.215
* Epoch: [53/60]	 Top 1-err 43.657  Top 5-err 14.790	 Train Loss 1.564
* Epoch: [53/60]	 Top 1-err 49.780  Top 5-err 19.250	 Test Loss 1.847
Current best accuracy (top-1 and 5 error): 49.78 19.25
saving best model...
* Epoch: [53/60]	 Top 1-err 49.780  Top 5-err 19.250	 Test Loss 1.847
50.22
0.5022
loss: 1.8473514938354492
(35, 0, 98) triplet: 0.21000000000000002
(35, 0): 0.0
(35, 98): 0.21000000000000002
* Epoch: [54/60]	 Top 1-err 43.555  Top 5-err 14.807	 Train Loss 1.565
* Epoch: [54/60]	 Top 1-err 49.730  Top 5-err 19.360	 Test Loss 1.850
Current best accuracy (top-1 and 5 error): 49.73 19.36
saving best model...
* Epoch: [54/60]	 Top 1-err 49.730  Top 5-err 19.360	 Test Loss 1.850
50.27
0.5027
loss: 1.8502179454803467
(35, 0, 98) triplet: 0.2
(35, 0): 0.0
(35, 98): 0.2
* Epoch: [55/60]	 Top 1-err 43.623  Top 5-err 14.690	 Train Loss 1.563
* Epoch: [55/60]	 Top 1-err 49.680  Top 5-err 18.970	 Test Loss 1.843
Current best accuracy (top-1 and 5 error): 49.68 18.97
saving best model...
* Epoch: [55/60]	 Top 1-err 49.680  Top 5-err 18.970	 Test Loss 1.843
50.32
0.5032
loss: 1.8425407257080078
(35, 0, 98) triplet: 0.21000000000000002
(35, 0): 0.0
(35, 98): 0.21000000000000002
* Epoch: [56/60]	 Top 1-err 43.400  Top 5-err 14.747	 Train Loss 1.559
* Epoch: [56/60]	 Top 1-err 49.710  Top 5-err 19.220	 Test Loss 1.845
Current best accuracy (top-1 and 5 error): 49.68 18.97
* Epoch: [56/60]	 Top 1-err 49.710  Top 5-err 19.220	 Test Loss 1.845
50.29
0.5029
loss: 1.8453317264556885
(35, 0, 98) triplet: 0.225
(35, 0): 0.0
(35, 98): 0.225
* Epoch: [57/60]	 Top 1-err 43.351  Top 5-err 14.853	 Train Loss 1.562
* Epoch: [57/60]	 Top 1-err 49.800  Top 5-err 19.310	 Test Loss 1.848
Current best accuracy (top-1 and 5 error): 49.68 18.97
* Epoch: [57/60]	 Top 1-err 49.800  Top 5-err 19.310	 Test Loss 1.848
50.2
0.502
loss: 1.8476045370101928
(35, 0, 98) triplet: 0.21500000000000002
(35, 0): 0.0
(35, 98): 0.21500000000000002
not enough sample
* Epoch: [58/60]	 Top 1-err 43.585  Top 5-err 14.743	 Train Loss 1.558
* Epoch: [58/60]	 Top 1-err 49.930  Top 5-err 18.990	 Test Loss 1.840
Current best accuracy (top-1 and 5 error): 49.68 18.97
* Epoch: [58/60]	 Top 1-err 49.930  Top 5-err 18.990	 Test Loss 1.840
50.07
0.5007
loss: 1.8403078411102296
(35, 0, 98) triplet: 0.20500000000000002
(35, 0): 0.0
(35, 98): 0.20500000000000002
* Epoch: [59/60]	 Top 1-err 43.461  Top 5-err 14.722	 Train Loss 1.556
* Epoch: [59/60]	 Top 1-err 49.930  Top 5-err 19.150	 Test Loss 1.846
Current best accuracy (top-1 and 5 error): 49.68 18.97
* Epoch: [59/60]	 Top 1-err 49.930  Top 5-err 19.150	 Test Loss 1.846
50.07
0.5007
loss: 1.845504935836792
(35, 0, 98) triplet: 0.2
(35, 0): 0.0
(35, 98): 0.2
Best accuracy (top-1 and 5 error): 49.68 18.97
=> loading checkpoint 'runs/cifar100_resnet_1/model_best.pth.tar'
* Epoch: [-1/60]	 Top 1-err 49.680  Top 5-err 18.970	 Test Loss 1.843
50.32
0.5032
loss: 1.842540748977661
(35, 0, 98) triplet: 0.21000000000000002
(35, 0): 0.0
(35, 98): 0.21000000000000002
python3 cifar100_repair_bias.py --net_type resnet --dataset cifar100 --depth 50 --batch_size 256 --lr 0.1 --expname cifar100_resnet_1 --epochs 60 --beta 1.0 --cutmix_prob 0 --pretrained ./runs/cifar100_resnet_1/model_best.pth.tar --expid 0 --first 35 --second 0 --third 98 --lam 0.5
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
=> creating model 'resnet'
True
=> loading checkpoint './runs/cifar100_resnet_1/model_best.pth.tar'
=> loaded checkpoint './runs/cifar100_resnet_1/model_best.pth.tar'
the number of model parameters: 521716
* Epoch: [0/60]	 Top 1-err 56.766  Top 5-err 23.952	 Train Loss 2.188
* Epoch: [0/60]	 Top 1-err 63.380  Top 5-err 31.230	 Test Loss 2.639
* Epoch: [0/60]	 Top 1-err 63.380  Top 5-err 31.230	 Test Loss 2.639
36.62
0.3662
loss: 2.639309683227539
(35, 0, 98) triplet: 0.21000000000000002
(35, 0): 0.08
(35, 98): 0.29000000000000004
* Epoch: [1/60]	 Top 1-err 54.686  Top 5-err 22.102	 Train Loss 2.066
* Epoch: [1/60]	 Top 1-err 71.530  Top 5-err 39.900	 Test Loss 3.268
* Epoch: [1/60]	 Top 1-err 71.530  Top 5-err 39.900	 Test Loss 3.268
28.47
0.2847
loss: 3.267870955657959
(35, 0, 98) triplet: 0.22499999999999998
(35, 0): 0.07
(35, 98): 0.295
* Epoch: [2/60]	 Top 1-err 53.510  Top 5-err 20.946	 Train Loss 2.006
* Epoch: [2/60]	 Top 1-err 67.710  Top 5-err 36.180	 Test Loss 2.899
* Epoch: [2/60]	 Top 1-err 67.710  Top 5-err 36.180	 Test Loss 2.899
32.29
0.3229
loss: 2.899243726348877
(35, 0, 98) triplet: 0.04000000000000001
(35, 0): 0.06
(35, 98): 0.1
* Epoch: [3/60]	 Top 1-err 52.510  Top 5-err 20.331	 Train Loss 1.968
* Epoch: [3/60]	 Top 1-err 60.410  Top 5-err 28.800	 Test Loss 2.398
* Epoch: [3/60]	 Top 1-err 60.410  Top 5-err 28.800	 Test Loss 2.398
39.59
0.3959
loss: 2.3982086509704588
(35, 0, 98) triplet: 0.030000000000000006
(35, 0): 0.04
(35, 98): 0.07
* Epoch: [4/60]	 Top 1-err 51.250  Top 5-err 19.448	 Train Loss 1.913
* Epoch: [4/60]	 Top 1-err 57.880  Top 5-err 25.580	 Test Loss 2.243
* Epoch: [4/60]	 Top 1-err 57.880  Top 5-err 25.580	 Test Loss 2.243
42.12
0.4212
loss: 2.242923268890381
(35, 0, 98) triplet: 0.020000000000000018
(35, 0): 0.175
(35, 98): 0.195
* Epoch: [5/60]	 Top 1-err 50.383  Top 5-err 18.872	 Train Loss 1.879
* Epoch: [5/60]	 Top 1-err 61.650  Top 5-err 30.270	 Test Loss 2.568
* Epoch: [5/60]	 Top 1-err 61.650  Top 5-err 30.270	 Test Loss 2.568
38.35
0.3835
loss: 2.568399942588806
(35, 0, 98) triplet: 0.19
(35, 0): 0.05
(35, 98): 0.24
* Epoch: [6/60]	 Top 1-err 50.174  Top 5-err 18.402	 Train Loss 1.854
* Epoch: [6/60]	 Top 1-err 56.330  Top 5-err 23.660	 Test Loss 2.120
* Epoch: [6/60]	 Top 1-err 56.330  Top 5-err 23.660	 Test Loss 2.120
43.67
0.4367
loss: 2.1201965265274048
(35, 0, 98) triplet: 0.05000000000000002
(35, 0): 0.08499999999999999
(35, 98): 0.135
* Epoch: [7/60]	 Top 1-err 49.282  Top 5-err 17.588	 Train Loss 1.810
* Epoch: [7/60]	 Top 1-err 59.470  Top 5-err 27.340	 Test Loss 2.441
* Epoch: [7/60]	 Top 1-err 59.470  Top 5-err 27.340	 Test Loss 2.441
40.53
0.4053
loss: 2.4413581176757813
(35, 0, 98) triplet: 0.07500000000000001
(35, 0): 0.049999999999999996
(35, 98): 0.125
* Epoch: [8/60]	 Top 1-err 48.568  Top 5-err 17.465	 Train Loss 1.797
* Epoch: [8/60]	 Top 1-err 60.240  Top 5-err 30.120	 Test Loss 2.599
* Epoch: [8/60]	 Top 1-err 60.240  Top 5-err 30.120	 Test Loss 2.599
39.76
0.3976
loss: 2.5985050857543945
(35, 0, 98) triplet: 0.035
(35, 0): 0.095
(35, 98): 0.06
* Epoch: [9/60]	 Top 1-err 47.813  Top 5-err 16.759	 Train Loss 1.752
* Epoch: [9/60]	 Top 1-err 57.800  Top 5-err 26.580	 Test Loss 2.272
* Epoch: [9/60]	 Top 1-err 57.800  Top 5-err 26.580	 Test Loss 2.272
42.2
0.422
loss: 2.272189670562744
(35, 0, 98) triplet: 0.045
(35, 0): 0.075
(35, 98): 0.12
* Epoch: [10/60]	 Top 1-err 47.008  Top 5-err 16.351	 Train Loss 1.731
* Epoch: [10/60]	 Top 1-err 55.320  Top 5-err 24.460	 Test Loss 2.155
* Epoch: [10/60]	 Top 1-err 55.320  Top 5-err 24.460	 Test Loss 2.155
44.68
0.4468
loss: 2.1552080894470214
(35, 0, 98) triplet: 0.20500000000000002
(35, 0): 0.049999999999999996
(35, 98): 0.255
* Epoch: [11/60]	 Top 1-err 46.462  Top 5-err 15.935	 Train Loss 1.711
* Epoch: [11/60]	 Top 1-err 56.220  Top 5-err 24.210	 Test Loss 2.230
* Epoch: [11/60]	 Top 1-err 56.220  Top 5-err 24.210	 Test Loss 2.230
43.78
0.4378
loss: 2.2300610023498537
(35, 0, 98) triplet: 0.105
(35, 0): 0.115
(35, 98): 0.22
* Epoch: [12/60]	 Top 1-err 45.657  Top 5-err 15.399	 Train Loss 1.681
* Epoch: [12/60]	 Top 1-err 60.410  Top 5-err 28.260	 Test Loss 2.488
* Epoch: [12/60]	 Top 1-err 60.410  Top 5-err 28.260	 Test Loss 2.488
39.59
0.3959
loss: 2.4878116821289065
(35, 0, 98) triplet: 0.22500000000000003
(35, 0): 0.045
(35, 98): 0.27
* Epoch: [13/60]	 Top 1-err 44.934  Top 5-err 15.183	 Train Loss 1.654
* Epoch: [13/60]	 Top 1-err 51.560  Top 5-err 20.990	 Test Loss 1.954
* Epoch: [13/60]	 Top 1-err 51.560  Top 5-err 20.990	 Test Loss 1.954
48.44
0.4844
loss: 1.9536031288146973
(35, 0, 98) triplet: 0.12499999999999999
(35, 0): 0.075
(35, 98): 0.19999999999999998
not enough sample
* Epoch: [14/60]	 Top 1-err 44.407  Top 5-err 14.643	 Train Loss 1.628
* Epoch: [14/60]	 Top 1-err 53.680  Top 5-err 23.380	 Test Loss 2.094
* Epoch: [14/60]	 Top 1-err 53.680  Top 5-err 23.380	 Test Loss 2.094
46.32
0.4632
loss: 2.093518662261963
(35, 0, 98) triplet: 0.16
(35, 0): 0.095
(35, 98): 0.255
* Epoch: [15/60]	 Top 1-err 43.940  Top 5-err 14.328	 Train Loss 1.602
* Epoch: [15/60]	 Top 1-err 51.870  Top 5-err 21.000	 Test Loss 1.985
* Epoch: [15/60]	 Top 1-err 51.870  Top 5-err 21.000	 Test Loss 1.985
48.13
0.4813
loss: 1.985126180267334
(35, 0, 98) triplet: 0.19
(35, 0): 0.065
(35, 98): 0.255
* Epoch: [16/60]	 Top 1-err 43.402  Top 5-err 13.927	 Train Loss 1.577
* Epoch: [16/60]	 Top 1-err 53.080  Top 5-err 21.110	 Test Loss 2.054
* Epoch: [16/60]	 Top 1-err 53.080  Top 5-err 21.110	 Test Loss 2.054
46.92
0.4692
loss: 2.053626830101013
(35, 0, 98) triplet: 0.13
(35, 0): 0.03
(35, 98): 0.16
* Epoch: [17/60]	 Top 1-err 43.053  Top 5-err 13.765	 Train Loss 1.571
* Epoch: [17/60]	 Top 1-err 51.960  Top 5-err 21.630	 Test Loss 2.028
* Epoch: [17/60]	 Top 1-err 51.960  Top 5-err 21.630	 Test Loss 2.028
48.04
0.4804
loss: 2.028161471939087
(35, 0, 98) triplet: 0.0050000000000000044
(35, 0): 0.15
(35, 98): 0.155
* Epoch: [18/60]	 Top 1-err 42.603  Top 5-err 13.479	 Train Loss 1.552
* Epoch: [18/60]	 Top 1-err 49.960  Top 5-err 20.070	 Test Loss 1.913
* Epoch: [18/60]	 Top 1-err 49.960  Top 5-err 20.070	 Test Loss 1.913
50.04
0.5004
loss: 1.913292179107666
(35, 0, 98) triplet: 0.045
(35, 0): 0.105
(35, 98): 0.15
* Epoch: [19/60]	 Top 1-err 41.855  Top 5-err 13.226	 Train Loss 1.529
* Epoch: [19/60]	 Top 1-err 49.490  Top 5-err 19.850	 Test Loss 1.934
* Epoch: [19/60]	 Top 1-err 49.490  Top 5-err 19.850	 Test Loss 1.934
50.51
0.5051
loss: 1.9337657768249512
(35, 0, 98) triplet: 0.205
(35, 0): 0.030000000000000002
(35, 98): 0.235
* Epoch: [20/60]	 Top 1-err 41.672  Top 5-err 13.000	 Train Loss 1.513
* Epoch: [20/60]	 Top 1-err 50.750  Top 5-err 20.220	 Test Loss 1.944
* Epoch: [20/60]	 Top 1-err 50.750  Top 5-err 20.220	 Test Loss 1.944
49.25
0.4925
loss: 1.9436926807403565
(35, 0, 98) triplet: 0.305
(35, 0): 0.02
(35, 98): 0.325
not enough sample
* Epoch: [21/60]	 Top 1-err 41.139  Top 5-err 12.482	 Train Loss 1.493
* Epoch: [21/60]	 Top 1-err 50.640  Top 5-err 21.470	 Test Loss 1.994
* Epoch: [21/60]	 Top 1-err 50.640  Top 5-err 21.470	 Test Loss 1.994
49.36
0.4936
loss: 1.9936520837783813
(35, 0, 98) triplet: 0.145
(35, 0): 0.03
(35, 98): 0.175
* Epoch: [22/60]	 Top 1-err 40.788  Top 5-err 12.365	 Train Loss 1.477
* Epoch: [22/60]	 Top 1-err 50.670  Top 5-err 20.310	 Test Loss 1.979
* Epoch: [22/60]	 Top 1-err 50.670  Top 5-err 20.310	 Test Loss 1.979
49.33
0.4933
loss: 1.979477410888672
(35, 0, 98) triplet: 0.24500000000000005
(35, 0): 0.05
(35, 98): 0.29500000000000004
* Epoch: [23/60]	 Top 1-err 40.697  Top 5-err 12.195	 Train Loss 1.473
* Epoch: [23/60]	 Top 1-err 49.770  Top 5-err 19.380	 Test Loss 1.894
* Epoch: [23/60]	 Top 1-err 49.770  Top 5-err 19.380	 Test Loss 1.894
50.23
0.5023
loss: 1.8943267395019532
(35, 0, 98) triplet: 0.06500000000000002
(35, 0): 0.09000000000000001
(35, 98): 0.15500000000000003
* Epoch: [24/60]	 Top 1-err 40.083  Top 5-err 11.823	 Train Loss 1.452
* Epoch: [24/60]	 Top 1-err 50.770  Top 5-err 20.730	 Test Loss 1.968
* Epoch: [24/60]	 Top 1-err 50.770  Top 5-err 20.730	 Test Loss 1.968
49.23
0.4923
loss: 1.9676424125671386
(35, 0, 98) triplet: 0.24
(35, 0): 0.005
(35, 98): 0.245
* Epoch: [25/60]	 Top 1-err 39.535  Top 5-err 11.851	 Train Loss 1.442
* Epoch: [25/60]	 Top 1-err 48.920  Top 5-err 19.560	 Test Loss 1.866
* Epoch: [25/60]	 Top 1-err 48.920  Top 5-err 19.560	 Test Loss 1.866
51.08
0.5108
loss: 1.8662376159667968
(35, 0, 98) triplet: 0.10500000000000001
(35, 0): 0.034999999999999996
(35, 98): 0.14
* Epoch: [26/60]	 Top 1-err 39.125  Top 5-err 11.685	 Train Loss 1.412
* Epoch: [26/60]	 Top 1-err 47.140  Top 5-err 17.940	 Test Loss 1.785
* Epoch: [26/60]	 Top 1-err 47.140  Top 5-err 17.940	 Test Loss 1.785
52.86
0.5286
loss: 1.785077626800537
(35, 0, 98) triplet: 0.235
(35, 0): 0.005
(35, 98): 0.24
* Epoch: [27/60]	 Top 1-err 38.682  Top 5-err 11.298	 Train Loss 1.408
* Epoch: [27/60]	 Top 1-err 49.600  Top 5-err 19.870	 Test Loss 1.944
* Epoch: [27/60]	 Top 1-err 49.600  Top 5-err 19.870	 Test Loss 1.944
50.4
0.504
loss: 1.9444261505126954
(35, 0, 98) triplet: 0.13499999999999998
(35, 0): 0.03
(35, 98): 0.16499999999999998
* Epoch: [28/60]	 Top 1-err 38.170  Top 5-err 10.769	 Train Loss 1.375
* Epoch: [28/60]	 Top 1-err 48.430  Top 5-err 17.950	 Test Loss 1.858
* Epoch: [28/60]	 Top 1-err 48.430  Top 5-err 17.950	 Test Loss 1.858
51.57
0.5157
loss: 1.857860391998291
(35, 0, 98) triplet: 0.18000000000000002
(35, 0): 0.055
(35, 98): 0.23500000000000001
* Epoch: [29/60]	 Top 1-err 37.890  Top 5-err 10.688	 Train Loss 1.360
* Epoch: [29/60]	 Top 1-err 46.550  Top 5-err 17.660	 Test Loss 1.808
* Epoch: [29/60]	 Top 1-err 46.550  Top 5-err 17.660	 Test Loss 1.808
53.45
0.5345
loss: 1.8075900024414062
(35, 0, 98) triplet: 0.13
(35, 0): 0.015
(35, 98): 0.14500000000000002
* Epoch: [30/60]	 Top 1-err 31.675  Top 5-err 8.081	 Train Loss 1.142
* Epoch: [30/60]	 Top 1-err 39.430  Top 5-err 13.260	 Test Loss 1.470
* Epoch: [30/60]	 Top 1-err 39.430  Top 5-err 13.260	 Test Loss 1.470
60.57
0.6057
loss: 1.470459935760498
(35, 0, 98) triplet: 0.13000000000000003
(35, 0): 0.02
(35, 98): 0.15000000000000002
* Epoch: [31/60]	 Top 1-err 29.637  Top 5-err 7.376	 Train Loss 1.076
* Epoch: [31/60]	 Top 1-err 38.770  Top 5-err 12.930	 Test Loss 1.434
* Epoch: [31/60]	 Top 1-err 38.770  Top 5-err 12.930	 Test Loss 1.434
61.23
0.6123
loss: 1.4340088490486145
(35, 0, 98) triplet: 0.11500000000000002
(35, 0): 0.025
(35, 98): 0.14
* Epoch: [32/60]	 Top 1-err 29.209  Top 5-err 7.025	 Train Loss 1.053
* Epoch: [32/60]	 Top 1-err 38.810  Top 5-err 12.950	 Test Loss 1.458
* Epoch: [32/60]	 Top 1-err 38.810  Top 5-err 12.950	 Test Loss 1.458
61.19
0.6119
loss: 1.458206706237793
(35, 0, 98) triplet: 0.16
(35, 0): 0.02
(35, 98): 0.18
* Epoch: [33/60]	 Top 1-err 28.602  Top 5-err 6.794	 Train Loss 1.030
* Epoch: [33/60]	 Top 1-err 39.080  Top 5-err 12.930	 Test Loss 1.449
* Epoch: [33/60]	 Top 1-err 39.080  Top 5-err 12.930	 Test Loss 1.449
60.92
0.6092
loss: 1.449038514328003
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.02
(35, 98): 0.165
* Epoch: [34/60]	 Top 1-err 28.396  Top 5-err 6.715	 Train Loss 1.023
* Epoch: [34/60]	 Top 1-err 38.810  Top 5-err 12.880	 Test Loss 1.462
* Epoch: [34/60]	 Top 1-err 38.810  Top 5-err 12.880	 Test Loss 1.462
61.19
0.6119
loss: 1.462272043609619
(35, 0, 98) triplet: 0.15
(35, 0): 0.01
(35, 98): 0.16
* Epoch: [35/60]	 Top 1-err 28.075  Top 5-err 6.643	 Train Loss 1.011
* Epoch: [35/60]	 Top 1-err 38.750  Top 5-err 12.950	 Test Loss 1.454
* Epoch: [35/60]	 Top 1-err 38.750  Top 5-err 12.950	 Test Loss 1.454
61.25
0.6125
loss: 1.4537864046096802
(35, 0, 98) triplet: 0.15999999999999998
(35, 0): 0.01
(35, 98): 0.16999999999999998
* Epoch: [36/60]	 Top 1-err 27.771  Top 5-err 6.415	 Train Loss 0.997
* Epoch: [36/60]	 Top 1-err 38.740  Top 5-err 12.700	 Test Loss 1.445
* Epoch: [36/60]	 Top 1-err 38.740  Top 5-err 12.700	 Test Loss 1.445
61.26
0.6126
loss: 1.4448231462478638
(35, 0, 98) triplet: 0.13999999999999999
(35, 0): 0.01
(35, 98): 0.15
* Epoch: [37/60]	 Top 1-err 27.469  Top 5-err 6.347	 Train Loss 0.986
* Epoch: [37/60]	 Top 1-err 38.890  Top 5-err 13.110	 Test Loss 1.484
* Epoch: [37/60]	 Top 1-err 38.890  Top 5-err 13.110	 Test Loss 1.484
61.11
0.6111
loss: 1.4844002855300904
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.015
(35, 98): 0.18
not enough sample
* Epoch: [38/60]	 Top 1-err 27.401  Top 5-err 6.464	 Train Loss 0.988
* Epoch: [38/60]	 Top 1-err 38.690  Top 5-err 12.800	 Test Loss 1.465
* Epoch: [38/60]	 Top 1-err 38.690  Top 5-err 12.800	 Test Loss 1.465
61.31
0.6131
loss: 1.4645432050704956
(35, 0, 98) triplet: 0.185
(35, 0): 0.01
(35, 98): 0.195
* Epoch: [39/60]	 Top 1-err 26.976  Top 5-err 6.177	 Train Loss 0.976
* Epoch: [39/60]	 Top 1-err 38.820  Top 5-err 12.810	 Test Loss 1.477
* Epoch: [39/60]	 Top 1-err 38.820  Top 5-err 12.810	 Test Loss 1.477
61.18
0.6118
loss: 1.4767350036621094
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.01
(35, 98): 0.175
not enough sample
* Epoch: [40/60]	 Top 1-err 26.706  Top 5-err 6.124	 Train Loss 0.963
* Epoch: [40/60]	 Top 1-err 38.360  Top 5-err 12.600	 Test Loss 1.460
* Epoch: [40/60]	 Top 1-err 38.360  Top 5-err 12.600	 Test Loss 1.460
61.64
0.6164
loss: 1.4599023878097535
(35, 0, 98) triplet: 0.18
(35, 0): 0.01
(35, 98): 0.19
* Epoch: [41/60]	 Top 1-err 26.947  Top 5-err 6.071	 Train Loss 0.961
* Epoch: [41/60]	 Top 1-err 38.530  Top 5-err 12.990	 Test Loss 1.467
* Epoch: [41/60]	 Top 1-err 38.530  Top 5-err 12.990	 Test Loss 1.467
61.47
0.6147
loss: 1.4666609104156494
(35, 0, 98) triplet: 0.15500000000000003
(35, 0): 0.015
(35, 98): 0.17
* Epoch: [42/60]	 Top 1-err 26.664  Top 5-err 5.859	 Train Loss 0.952
* Epoch: [42/60]	 Top 1-err 38.670  Top 5-err 12.950	 Test Loss 1.465
* Epoch: [42/60]	 Top 1-err 38.670  Top 5-err 12.950	 Test Loss 1.465
61.33
0.6133
loss: 1.4651079978942871
(35, 0, 98) triplet: 0.2
(35, 0): 0.01
(35, 98): 0.21000000000000002
* Epoch: [43/60]	 Top 1-err 26.311  Top 5-err 6.011	 Train Loss 0.944
* Epoch: [43/60]	 Top 1-err 38.930  Top 5-err 13.220	 Test Loss 1.502
* Epoch: [43/60]	 Top 1-err 38.930  Top 5-err 13.220	 Test Loss 1.502
61.07
0.6107
loss: 1.5019163349151612
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.01
(35, 98): 0.175
* Epoch: [44/60]	 Top 1-err 25.994  Top 5-err 5.910	 Train Loss 0.935
* Epoch: [44/60]	 Top 1-err 39.050  Top 5-err 12.760	 Test Loss 1.478
* Epoch: [44/60]	 Top 1-err 39.050  Top 5-err 12.760	 Test Loss 1.478
60.95
0.6095
loss: 1.477639888381958
(35, 0, 98) triplet: 0.185
(35, 0): 0.005
(35, 98): 0.19
* Epoch: [45/60]	 Top 1-err 25.094  Top 5-err 5.465	 Train Loss 0.902
* Epoch: [45/60]	 Top 1-err 38.020  Top 5-err 12.310	 Test Loss 1.443
Current best accuracy (top-1 and 5 error): 38.02 12.31
saving best model...
* Epoch: [45/60]	 Top 1-err 38.020  Top 5-err 12.310	 Test Loss 1.443
61.98
0.6198
loss: 1.44292359085083
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.015
(35, 98): 0.16
* Epoch: [46/60]	 Top 1-err 24.934  Top 5-err 5.365	 Train Loss 0.891
* Epoch: [46/60]	 Top 1-err 37.920  Top 5-err 12.480	 Test Loss 1.441
Current best accuracy (top-1 and 5 error): 37.92 12.48
saving best model...
* Epoch: [46/60]	 Top 1-err 37.920  Top 5-err 12.480	 Test Loss 1.441
62.08
0.6208
loss: 1.4405974245071411
(35, 0, 98) triplet: 0.15
(35, 0): 0.01
(35, 98): 0.16
not enough sample
* Epoch: [47/60]	 Top 1-err 24.431  Top 5-err 5.221	 Train Loss 0.880
* Epoch: [47/60]	 Top 1-err 37.900  Top 5-err 12.410	 Test Loss 1.441
Current best accuracy (top-1 and 5 error): 37.9 12.41
saving best model...
* Epoch: [47/60]	 Top 1-err 37.900  Top 5-err 12.410	 Test Loss 1.441
62.1
0.621
loss: 1.4407873847961425
(35, 0, 98) triplet: 0.13
(35, 0): 0.015
(35, 98): 0.14500000000000002
* Epoch: [48/60]	 Top 1-err 24.702  Top 5-err 5.195	 Train Loss 0.882
* Epoch: [48/60]	 Top 1-err 37.930  Top 5-err 12.510	 Test Loss 1.442
Current best accuracy (top-1 and 5 error): 37.9 12.41
* Epoch: [48/60]	 Top 1-err 37.930  Top 5-err 12.510	 Test Loss 1.442
62.07
0.6207
loss: 1.442238048171997
(35, 0, 98) triplet: 0.15999999999999998
(35, 0): 0.01
(35, 98): 0.16999999999999998
* Epoch: [49/60]	 Top 1-err 24.377  Top 5-err 5.263	 Train Loss 0.881
* Epoch: [49/60]	 Top 1-err 37.810  Top 5-err 12.460	 Test Loss 1.442
Current best accuracy (top-1 and 5 error): 37.81 12.46
saving best model...
* Epoch: [49/60]	 Top 1-err 37.810  Top 5-err 12.460	 Test Loss 1.442
62.19
0.6219
loss: 1.4417921081542968
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.01
(35, 98): 0.15500000000000003
* Epoch: [50/60]	 Top 1-err 24.499  Top 5-err 5.417	 Train Loss 0.879
* Epoch: [50/60]	 Top 1-err 37.920  Top 5-err 12.500	 Test Loss 1.445
Current best accuracy (top-1 and 5 error): 37.81 12.46
* Epoch: [50/60]	 Top 1-err 37.920  Top 5-err 12.500	 Test Loss 1.445
62.08
0.6208
loss: 1.444949769592285
(35, 0, 98) triplet: 0.135
(35, 0): 0.015
(35, 98): 0.15000000000000002
* Epoch: [51/60]	 Top 1-err 24.411  Top 5-err 5.227	 Train Loss 0.874
* Epoch: [51/60]	 Top 1-err 38.020  Top 5-err 12.400	 Test Loss 1.446
Current best accuracy (top-1 and 5 error): 37.81 12.46
* Epoch: [51/60]	 Top 1-err 38.020  Top 5-err 12.400	 Test Loss 1.446
61.98
0.6198
loss: 1.4461843545913697
(35, 0, 98) triplet: 0.15000000000000002
(35, 0): 0.015
(35, 98): 0.165
* Epoch: [52/60]	 Top 1-err 24.345  Top 5-err 5.234	 Train Loss 0.874
* Epoch: [52/60]	 Top 1-err 37.890  Top 5-err 12.550	 Test Loss 1.448
Current best accuracy (top-1 and 5 error): 37.81 12.46
* Epoch: [52/60]	 Top 1-err 37.890  Top 5-err 12.550	 Test Loss 1.448
62.11
0.6211
loss: 1.4481304761886598
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.015
(35, 98): 0.16
* Epoch: [53/60]	 Top 1-err 24.233  Top 5-err 5.255	 Train Loss 0.873
* Epoch: [53/60]	 Top 1-err 37.800  Top 5-err 12.530	 Test Loss 1.455
Current best accuracy (top-1 and 5 error): 37.8 12.53
saving best model...
* Epoch: [53/60]	 Top 1-err 37.800  Top 5-err 12.530	 Test Loss 1.455
62.2
0.622
loss: 1.454809569168091
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.01
(35, 98): 0.18
* Epoch: [54/60]	 Top 1-err 24.341  Top 5-err 5.268	 Train Loss 0.873
* Epoch: [54/60]	 Top 1-err 37.830  Top 5-err 12.540	 Test Loss 1.449
Current best accuracy (top-1 and 5 error): 37.8 12.53
* Epoch: [54/60]	 Top 1-err 37.830  Top 5-err 12.540	 Test Loss 1.449
62.17
0.6217
loss: 1.4485464851379395
(35, 0, 98) triplet: 0.14999999999999997
(35, 0): 0.015
(35, 98): 0.16499999999999998
* Epoch: [55/60]	 Top 1-err 24.367  Top 5-err 5.383	 Train Loss 0.872
* Epoch: [55/60]	 Top 1-err 37.740  Top 5-err 12.640	 Test Loss 1.448
Current best accuracy (top-1 and 5 error): 37.74 12.64
saving best model...
* Epoch: [55/60]	 Top 1-err 37.740  Top 5-err 12.640	 Test Loss 1.448
62.26
0.6226
loss: 1.4481771440505982
(35, 0, 98) triplet: 0.14
(35, 0): 0.015
(35, 98): 0.155
* Epoch: [56/60]	 Top 1-err 24.392  Top 5-err 5.266	 Train Loss 0.871
* Epoch: [56/60]	 Top 1-err 37.720  Top 5-err 12.640	 Test Loss 1.449
Current best accuracy (top-1 and 5 error): 37.72 12.64
saving best model...
* Epoch: [56/60]	 Top 1-err 37.720  Top 5-err 12.640	 Test Loss 1.449
62.28
0.6228
loss: 1.4490197563171388
(35, 0, 98) triplet: 0.15499999999999997
(35, 0): 0.01
(35, 98): 0.16499999999999998
* Epoch: [57/60]	 Top 1-err 24.594  Top 5-err 5.206	 Train Loss 0.870
* Epoch: [57/60]	 Top 1-err 37.820  Top 5-err 12.470	 Test Loss 1.448
Current best accuracy (top-1 and 5 error): 37.72 12.64
* Epoch: [57/60]	 Top 1-err 37.820  Top 5-err 12.470	 Test Loss 1.448
62.18
0.6218
loss: 1.4483066414833068
(35, 0, 98) triplet: 0.11
(35, 0): 0.015
(35, 98): 0.125
not enough sample
* Epoch: [58/60]	 Top 1-err 24.042  Top 5-err 5.213	 Train Loss 0.867
* Epoch: [58/60]	 Top 1-err 37.860  Top 5-err 12.480	 Test Loss 1.449
Current best accuracy (top-1 and 5 error): 37.72 12.64
* Epoch: [58/60]	 Top 1-err 37.860  Top 5-err 12.480	 Test Loss 1.449
62.14
0.6214
loss: 1.44864498462677
(35, 0, 98) triplet: 0.155
(35, 0): 0.01
(35, 98): 0.165
* Epoch: [59/60]	 Top 1-err 24.220  Top 5-err 5.200	 Train Loss 0.866
* Epoch: [59/60]	 Top 1-err 37.680  Top 5-err 12.480	 Test Loss 1.451
Current best accuracy (top-1 and 5 error): 37.68 12.48
saving best model...
* Epoch: [59/60]	 Top 1-err 37.680  Top 5-err 12.480	 Test Loss 1.451
62.32
0.6232
loss: 1.4507539848327637
(35, 0, 98) triplet: 0.15
(35, 0): 0.01
(35, 98): 0.16
Best accuracy (top-1 and 5 error): 37.68 12.48
=> loading checkpoint 'runs/cifar100_resnet_1/model_best.pth.tar'
* Epoch: [-1/60]	 Top 1-err 37.680  Top 5-err 12.480	 Test Loss 1.451
62.32
0.6232
loss: 1.4507539934158324
(35, 0, 98) triplet: 0.15
(35, 0): 0.01
(35, 98): 0.16
python3 cifar100_repair_bias.py --net_type resnet --dataset cifar100 --depth 50 --batch_size 256 --lr 0.1 --expname cifar100_resnet_1 --epochs 60 --beta 1.0 --cutmix_prob 0 --pretrained ./runs/cifar100_resnet_1/model_best.pth.tar --expid 0 --first 35 --second 0 --third 98 --lam 1
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
=> creating model 'resnet'
True
=> loading checkpoint './runs/cifar100_resnet_1/model_best.pth.tar'
=> loaded checkpoint './runs/cifar100_resnet_1/model_best.pth.tar'
the number of model parameters: 521716
* Epoch: [0/60]	 Top 1-err 44.537  Top 5-err 14.405	 Train Loss 1.661
* Epoch: [0/60]	 Top 1-err 50.540  Top 5-err 20.730	 Test Loss 2.028
* Epoch: [0/60]	 Top 1-err 50.540  Top 5-err 20.730	 Test Loss 2.028
49.46
0.4946
loss: 2.0279517236709594
(35, 0, 98) triplet: 0.065
(35, 0): 0.07
(35, 98): 0.135
* Epoch: [1/60]	 Top 1-err 41.175  Top 5-err 12.546	 Train Loss 1.508
* Epoch: [1/60]	 Top 1-err 47.580  Top 5-err 18.820	 Test Loss 1.855
* Epoch: [1/60]	 Top 1-err 47.580  Top 5-err 18.820	 Test Loss 1.855
52.42
0.5242
loss: 1.8551837812423706
(35, 0, 98) triplet: 0.060000000000000026
(35, 0): 0.15000000000000002
(35, 98): 0.09
* Epoch: [2/60]	 Top 1-err 39.858  Top 5-err 11.647	 Train Loss 1.449
* Epoch: [2/60]	 Top 1-err 53.470  Top 5-err 23.120	 Test Loss 2.196
* Epoch: [2/60]	 Top 1-err 53.470  Top 5-err 23.120	 Test Loss 2.196
46.53
0.4653
loss: 2.1957076892852783
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
* Epoch: [3/60]	 Top 1-err 38.853  Top 5-err 11.120	 Train Loss 1.408
* Epoch: [3/60]	 Top 1-err 55.890  Top 5-err 24.960	 Test Loss 2.307
* Epoch: [3/60]	 Top 1-err 55.890  Top 5-err 24.960	 Test Loss 2.307
44.11
0.4411
loss: 2.3066670082092284
(35, 0, 98) triplet: 0.08000000000000002
(35, 0): 0.055
(35, 98): 0.135
* Epoch: [4/60]	 Top 1-err 39.152  Top 5-err 11.194	 Train Loss 1.412
* Epoch: [4/60]	 Top 1-err 57.430  Top 5-err 26.300	 Test Loss 2.502
* Epoch: [4/60]	 Top 1-err 57.430  Top 5-err 26.300	 Test Loss 2.502
42.57
0.4257
loss: 2.501753678512573
(35, 0, 98) triplet: 0.035
(35, 0): 0.08
(35, 98): 0.115
* Epoch: [5/60]	 Top 1-err 38.810  Top 5-err 10.825	 Train Loss 1.398
* Epoch: [5/60]	 Top 1-err 48.670  Top 5-err 19.140	 Test Loss 1.904
* Epoch: [5/60]	 Top 1-err 48.670  Top 5-err 19.140	 Test Loss 1.904
51.33
0.5133
loss: 1.9037946086883546
(35, 0, 98) triplet: 0.13
(35, 0): 0.065
(35, 98): 0.195
* Epoch: [6/60]	 Top 1-err 38.206  Top 5-err 10.977	 Train Loss 1.380
* Epoch: [6/60]	 Top 1-err 50.970  Top 5-err 20.430	 Test Loss 2.001
* Epoch: [6/60]	 Top 1-err 50.970  Top 5-err 20.430	 Test Loss 2.001
49.03
0.4903
loss: 2.001173650741577
(35, 0, 98) triplet: 0.03
(35, 0): 0.065
(35, 98): 0.095
* Epoch: [7/60]	 Top 1-err 38.183  Top 5-err 10.873	 Train Loss 1.394
* Epoch: [7/60]	 Top 1-err 48.050  Top 5-err 18.710	 Test Loss 1.888
* Epoch: [7/60]	 Top 1-err 48.050  Top 5-err 18.710	 Test Loss 1.888
51.95
0.5195
loss: 1.8876176519393921
(35, 0, 98) triplet: 0.095
(35, 0): 0.025
(35, 98): 0.12000000000000001
* Epoch: [8/60]	 Top 1-err 37.080  Top 5-err 9.943	 Train Loss 1.333
* Epoch: [8/60]	 Top 1-err 46.640  Top 5-err 17.680	 Test Loss 1.837
* Epoch: [8/60]	 Top 1-err 46.640  Top 5-err 17.680	 Test Loss 1.837
53.36
0.5336
loss: 1.8369845378875733
(35, 0, 98) triplet: 0.09999999999999999
(35, 0): 0.05
(35, 98): 0.15
* Epoch: [9/60]	 Top 1-err 36.896  Top 5-err 9.800	 Train Loss 1.312
* Epoch: [9/60]	 Top 1-err 49.110  Top 5-err 19.680	 Test Loss 1.867
* Epoch: [9/60]	 Top 1-err 49.110  Top 5-err 19.680	 Test Loss 1.867
50.89
0.5089
loss: 1.8666241039276124
(35, 0, 98) triplet: 0.325
(35, 0): 0.0
(35, 98): 0.325
* Epoch: [10/60]	 Top 1-err 36.528  Top 5-err 9.671	 Train Loss 1.303
* Epoch: [10/60]	 Top 1-err 47.540  Top 5-err 18.220	 Test Loss 1.866
* Epoch: [10/60]	 Top 1-err 47.540  Top 5-err 18.220	 Test Loss 1.866
52.46
0.5246
loss: 1.8656219673156738
(35, 0, 98) triplet: 0.21500000000000002
(35, 0): 0.02
(35, 98): 0.23500000000000001
* Epoch: [11/60]	 Top 1-err 36.692  Top 5-err 9.730	 Train Loss 1.306
* Epoch: [11/60]	 Top 1-err 45.300  Top 5-err 16.560	 Test Loss 1.763
* Epoch: [11/60]	 Top 1-err 45.300  Top 5-err 16.560	 Test Loss 1.763
54.7
0.547
loss: 1.7625272956848144
(35, 0, 98) triplet: 0.15000000000000002
(35, 0): 0.02
(35, 98): 0.17
* Epoch: [12/60]	 Top 1-err 36.328  Top 5-err 9.770	 Train Loss 1.312
* Epoch: [12/60]	 Top 1-err 48.450  Top 5-err 18.230	 Test Loss 1.871
* Epoch: [12/60]	 Top 1-err 48.450  Top 5-err 18.230	 Test Loss 1.871
51.55
0.5155
loss: 1.8710207088470459
(35, 0, 98) triplet: 0.045
(35, 0): 0.01
(35, 98): 0.055
* Epoch: [13/60]	 Top 1-err 35.060  Top 5-err 9.292	 Train Loss 1.257
* Epoch: [13/60]	 Top 1-err 48.250  Top 5-err 19.770	 Test Loss 1.908
* Epoch: [13/60]	 Top 1-err 48.250  Top 5-err 19.770	 Test Loss 1.908
51.75
0.5175
loss: 1.9081689567565918
(35, 0, 98) triplet: 0.08499999999999999
(35, 0): 0.034999999999999996
(35, 98): 0.12
not enough sample
* Epoch: [14/60]	 Top 1-err 36.575  Top 5-err 9.694	 Train Loss 1.311
* Epoch: [14/60]	 Top 1-err 46.470  Top 5-err 18.510	 Test Loss 1.798
* Epoch: [14/60]	 Top 1-err 46.470  Top 5-err 18.510	 Test Loss 1.798
53.53
0.5353
loss: 1.798381439590454
(35, 0, 98) triplet: 0.020000000000000004
(35, 0): 0.05
(35, 98): 0.07
* Epoch: [15/60]	 Top 1-err 35.725  Top 5-err 9.424	 Train Loss 1.281
* Epoch: [15/60]	 Top 1-err 47.930  Top 5-err 18.550	 Test Loss 1.889
* Epoch: [15/60]	 Top 1-err 47.930  Top 5-err 18.550	 Test Loss 1.889
52.07
0.5207
loss: 1.8893923721313477
(35, 0, 98) triplet: 0.125
(35, 0): 0.03
(35, 98): 0.155
* Epoch: [16/60]	 Top 1-err 34.974  Top 5-err 8.857	 Train Loss 1.236
* Epoch: [16/60]	 Top 1-err 45.230  Top 5-err 16.840	 Test Loss 1.767
* Epoch: [16/60]	 Top 1-err 45.230  Top 5-err 16.840	 Test Loss 1.767
54.77
0.5477
loss: 1.7670635818481446
(35, 0, 98) triplet: 0.3
(35, 0): 0.005
(35, 98): 0.305
* Epoch: [17/60]	 Top 1-err 34.362  Top 5-err 8.523	 Train Loss 1.222
* Epoch: [17/60]	 Top 1-err 44.550  Top 5-err 16.090	 Test Loss 1.723
* Epoch: [17/60]	 Top 1-err 44.550  Top 5-err 16.090	 Test Loss 1.723
55.45
0.5545
loss: 1.7226795219421387
(35, 0, 98) triplet: 0.165
(35, 0): 0.035
(35, 98): 0.2
* Epoch: [18/60]	 Top 1-err 33.952  Top 5-err 8.334	 Train Loss 1.203
* Epoch: [18/60]	 Top 1-err 46.720  Top 5-err 17.280	 Test Loss 1.826
* Epoch: [18/60]	 Top 1-err 46.720  Top 5-err 17.280	 Test Loss 1.826
53.28
0.5328
loss: 1.8264501777648925
(35, 0, 98) triplet: 0.2
(35, 0): 0.005
(35, 98): 0.20500000000000002
* Epoch: [19/60]	 Top 1-err 33.948  Top 5-err 8.347	 Train Loss 1.201
* Epoch: [19/60]	 Top 1-err 44.860  Top 5-err 16.590	 Test Loss 1.703
* Epoch: [19/60]	 Top 1-err 44.860  Top 5-err 16.590	 Test Loss 1.703
55.14
0.5514
loss: 1.7030893224716186
(35, 0, 98) triplet: 0.15999999999999998
(35, 0): 0.0
(35, 98): 0.15999999999999998
* Epoch: [20/60]	 Top 1-err 33.544  Top 5-err 8.343	 Train Loss 1.193
* Epoch: [20/60]	 Top 1-err 46.620  Top 5-err 18.350	 Test Loss 1.881
* Epoch: [20/60]	 Top 1-err 46.620  Top 5-err 18.350	 Test Loss 1.881
53.38
0.5338
loss: 1.881286137008667
(35, 0, 98) triplet: 0.28
(35, 0): 0.005
(35, 98): 0.28500000000000003
not enough sample
* Epoch: [21/60]	 Top 1-err 33.249  Top 5-err 8.041	 Train Loss 1.180
* Epoch: [21/60]	 Top 1-err 45.230  Top 5-err 17.330	 Test Loss 1.822
* Epoch: [21/60]	 Top 1-err 45.230  Top 5-err 17.330	 Test Loss 1.822
54.77
0.5477
loss: 1.822247946548462
(35, 0, 98) triplet: 0.175
(35, 0): 0.04
(35, 98): 0.215
* Epoch: [22/60]	 Top 1-err 33.419  Top 5-err 8.172	 Train Loss 1.192
* Epoch: [22/60]	 Top 1-err 45.070  Top 5-err 16.960	 Test Loss 1.783
* Epoch: [22/60]	 Top 1-err 45.070  Top 5-err 16.960	 Test Loss 1.783
54.93
0.5493
loss: 1.7828242729187012
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.065
(35, 98): 0.22999999999999998
* Epoch: [23/60]	 Top 1-err 32.907  Top 5-err 7.752	 Train Loss 1.164
* Epoch: [23/60]	 Top 1-err 50.310  Top 5-err 19.930	 Test Loss 2.042
* Epoch: [23/60]	 Top 1-err 50.310  Top 5-err 19.930	 Test Loss 2.042
49.69
0.4969
loss: 2.041900358581543
(35, 0, 98) triplet: 0.135
(35, 0): 0.195
(35, 98): 0.060000000000000005
* Epoch: [24/60]	 Top 1-err 32.735  Top 5-err 7.769	 Train Loss 1.150
* Epoch: [24/60]	 Top 1-err 44.010  Top 5-err 15.130	 Test Loss 1.662
* Epoch: [24/60]	 Top 1-err 44.010  Top 5-err 15.130	 Test Loss 1.662
55.99
0.5599
loss: 1.6617750253677368
(35, 0, 98) triplet: 0.27999999999999997
(35, 0): 0.01
(35, 98): 0.29
* Epoch: [25/60]	 Top 1-err 32.490  Top 5-err 7.867	 Train Loss 1.166
* Epoch: [25/60]	 Top 1-err 45.960  Top 5-err 16.380	 Test Loss 1.761
* Epoch: [25/60]	 Top 1-err 45.960  Top 5-err 16.380	 Test Loss 1.761
54.04
0.5404
loss: 1.760577038192749
(35, 0, 98) triplet: 0.175
(35, 0): 0.0
(35, 98): 0.175
* Epoch: [26/60]	 Top 1-err 32.660  Top 5-err 7.714	 Train Loss 1.152
* Epoch: [26/60]	 Top 1-err 45.030  Top 5-err 16.480	 Test Loss 1.733
* Epoch: [26/60]	 Top 1-err 45.030  Top 5-err 16.480	 Test Loss 1.733
54.97
0.5497
loss: 1.7331414360046387
(35, 0, 98) triplet: 0.115
(35, 0): 0.01
(35, 98): 0.125
* Epoch: [27/60]	 Top 1-err 32.227  Top 5-err 7.607	 Train Loss 1.139
* Epoch: [27/60]	 Top 1-err 50.600  Top 5-err 20.960	 Test Loss 2.123
* Epoch: [27/60]	 Top 1-err 50.600  Top 5-err 20.960	 Test Loss 2.123
49.4
0.494
loss: 2.1231146781921386
(35, 0, 98) triplet: 0.14
(35, 0): 0.049999999999999996
(35, 98): 0.19
* Epoch: [28/60]	 Top 1-err 31.417  Top 5-err 7.218	 Train Loss 1.100
* Epoch: [28/60]	 Top 1-err 56.700  Top 5-err 26.270	 Test Loss 2.772
* Epoch: [28/60]	 Top 1-err 56.700  Top 5-err 26.270	 Test Loss 2.772
43.3
0.433
loss: 2.771645798873901
(35, 0, 98) triplet: 0.08
(35, 0): 0.11
(35, 98): 0.03
* Epoch: [29/60]	 Top 1-err 32.176  Top 5-err 7.219	 Train Loss 1.110
* Epoch: [29/60]	 Top 1-err 45.790  Top 5-err 17.520	 Test Loss 1.784
* Epoch: [29/60]	 Top 1-err 45.790  Top 5-err 17.520	 Test Loss 1.784
54.21
0.5421
loss: 1.7844691474914551
(35, 0, 98) triplet: 0.0050000000000000044
(35, 0): 0.085
(35, 98): 0.08
* Epoch: [30/60]	 Top 1-err 26.434  Top 5-err 5.533	 Train Loss 0.942
* Epoch: [30/60]	 Top 1-err 36.290  Top 5-err 11.450	 Test Loss 1.360
* Epoch: [30/60]	 Top 1-err 36.290  Top 5-err 11.450	 Test Loss 1.360
63.71
0.6371
loss: 1.3604903835296631
(35, 0, 98) triplet: 0.13499999999999998
(35, 0): 0.01
(35, 98): 0.145
* Epoch: [31/60]	 Top 1-err 23.070  Top 5-err 4.433	 Train Loss 0.832
* Epoch: [31/60]	 Top 1-err 35.580  Top 5-err 11.210	 Test Loss 1.339
* Epoch: [31/60]	 Top 1-err 35.580  Top 5-err 11.210	 Test Loss 1.339
64.42
0.6442
loss: 1.338710740852356
(35, 0, 98) triplet: 0.115
(35, 0): 0.01
(35, 98): 0.125
* Epoch: [32/60]	 Top 1-err 22.038  Top 5-err 4.178	 Train Loss 0.798
* Epoch: [32/60]	 Top 1-err 35.470  Top 5-err 11.100	 Test Loss 1.341
* Epoch: [32/60]	 Top 1-err 35.470  Top 5-err 11.100	 Test Loss 1.341
64.53
0.6453
loss: 1.3413355052947997
(35, 0, 98) triplet: 0.12000000000000001
(35, 0): 0.01
(35, 98): 0.13
* Epoch: [33/60]	 Top 1-err 21.171  Top 5-err 3.906	 Train Loss 0.769
* Epoch: [33/60]	 Top 1-err 35.090  Top 5-err 11.160	 Test Loss 1.337
* Epoch: [33/60]	 Top 1-err 35.090  Top 5-err 11.160	 Test Loss 1.337
64.91
0.6491
loss: 1.3368323554992676
(35, 0, 98) triplet: 0.10500000000000001
(35, 0): 0.01
(35, 98): 0.115
* Epoch: [34/60]	 Top 1-err 20.858  Top 5-err 3.833	 Train Loss 0.756
* Epoch: [34/60]	 Top 1-err 35.550  Top 5-err 11.070	 Test Loss 1.348
* Epoch: [34/60]	 Top 1-err 35.550  Top 5-err 11.070	 Test Loss 1.348
64.45
0.6445
loss: 1.3479396337509155
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.005
(35, 98): 0.15000000000000002
* Epoch: [35/60]	 Top 1-err 20.521  Top 5-err 3.716	 Train Loss 0.745
* Epoch: [35/60]	 Top 1-err 35.110  Top 5-err 11.080	 Test Loss 1.351
* Epoch: [35/60]	 Top 1-err 35.110  Top 5-err 11.080	 Test Loss 1.351
64.89
0.6489
loss: 1.3512295022964478
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.005
(35, 98): 0.15000000000000002
* Epoch: [36/60]	 Top 1-err 19.992  Top 5-err 3.600	 Train Loss 0.732
* Epoch: [36/60]	 Top 1-err 35.250  Top 5-err 11.210	 Test Loss 1.356
* Epoch: [36/60]	 Top 1-err 35.250  Top 5-err 11.210	 Test Loss 1.356
64.75
0.6475
loss: 1.3563837871551514
(35, 0, 98) triplet: 0.08
(35, 0): 0.01
(35, 98): 0.09
* Epoch: [37/60]	 Top 1-err 19.720  Top 5-err 3.508	 Train Loss 0.721
* Epoch: [37/60]	 Top 1-err 35.090  Top 5-err 10.970	 Test Loss 1.345
* Epoch: [37/60]	 Top 1-err 35.090  Top 5-err 10.970	 Test Loss 1.345
64.91
0.6491
loss: 1.3454466395378113
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.005
(35, 98): 0.175
not enough sample
* Epoch: [38/60]	 Top 1-err 19.637  Top 5-err 3.428	 Train Loss 0.714
* Epoch: [38/60]	 Top 1-err 35.340  Top 5-err 11.030	 Test Loss 1.372
* Epoch: [38/60]	 Top 1-err 35.340  Top 5-err 11.030	 Test Loss 1.372
64.66
0.6466
loss: 1.3719103197097777
(35, 0, 98) triplet: 0.14
(35, 0): 0.005
(35, 98): 0.14500000000000002
* Epoch: [39/60]	 Top 1-err 19.394  Top 5-err 3.351	 Train Loss 0.706
* Epoch: [39/60]	 Top 1-err 35.080  Top 5-err 11.090	 Test Loss 1.366
* Epoch: [39/60]	 Top 1-err 35.080  Top 5-err 11.090	 Test Loss 1.366
64.92
0.6492
loss: 1.366192342376709
(35, 0, 98) triplet: 0.13999999999999999
(35, 0): 0.005
(35, 98): 0.145
not enough sample
* Epoch: [40/60]	 Top 1-err 19.373  Top 5-err 3.292	 Train Loss 0.698
* Epoch: [40/60]	 Top 1-err 35.020  Top 5-err 11.010	 Test Loss 1.363
* Epoch: [40/60]	 Top 1-err 35.020  Top 5-err 11.010	 Test Loss 1.363
64.98
0.6498
loss: 1.3633882274627687
(35, 0, 98) triplet: 0.08499999999999999
(35, 0): 0.005
(35, 98): 0.09
* Epoch: [41/60]	 Top 1-err 18.884  Top 5-err 3.207	 Train Loss 0.692
* Epoch: [41/60]	 Top 1-err 35.630  Top 5-err 11.390	 Test Loss 1.394
* Epoch: [41/60]	 Top 1-err 35.630  Top 5-err 11.390	 Test Loss 1.394
64.37
0.6437
loss: 1.394358886909485
(35, 0, 98) triplet: 0.085
(35, 0): 0.01
(35, 98): 0.095
* Epoch: [42/60]	 Top 1-err 18.818  Top 5-err 3.111	 Train Loss 0.685
* Epoch: [42/60]	 Top 1-err 35.300  Top 5-err 11.300	 Test Loss 1.385
* Epoch: [42/60]	 Top 1-err 35.300  Top 5-err 11.300	 Test Loss 1.385
64.7
0.647
loss: 1.384809230518341
(35, 0, 98) triplet: 0.155
(35, 0): 0.005
(35, 98): 0.16
* Epoch: [43/60]	 Top 1-err 18.634  Top 5-err 3.209	 Train Loss 0.682
* Epoch: [43/60]	 Top 1-err 35.280  Top 5-err 11.270	 Test Loss 1.390
* Epoch: [43/60]	 Top 1-err 35.280  Top 5-err 11.270	 Test Loss 1.390
64.72
0.6472
loss: 1.3897017906188964
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
* Epoch: [44/60]	 Top 1-err 18.351  Top 5-err 3.037	 Train Loss 0.673
* Epoch: [44/60]	 Top 1-err 35.720  Top 5-err 11.240	 Test Loss 1.406
* Epoch: [44/60]	 Top 1-err 35.720  Top 5-err 11.240	 Test Loss 1.406
64.28
0.6428
loss: 1.4061099016189575
(35, 0, 98) triplet: 0.165
(35, 0): 0.0
(35, 98): 0.165
* Epoch: [45/60]	 Top 1-err 17.663  Top 5-err 2.839	 Train Loss 0.646
* Epoch: [45/60]	 Top 1-err 34.770  Top 5-err 10.860	 Test Loss 1.372
Current best accuracy (top-1 and 5 error): 34.77 10.86
saving best model...
* Epoch: [45/60]	 Top 1-err 34.770  Top 5-err 10.860	 Test Loss 1.372
65.23
0.6523
loss: 1.3723233528137206
(35, 0, 98) triplet: 0.12
(35, 0): 0.005
(35, 98): 0.125
* Epoch: [46/60]	 Top 1-err 17.246  Top 5-err 2.764	 Train Loss 0.630
* Epoch: [46/60]	 Top 1-err 34.740  Top 5-err 10.810	 Test Loss 1.369
Current best accuracy (top-1 and 5 error): 34.74 10.81
saving best model...
* Epoch: [46/60]	 Top 1-err 34.740  Top 5-err 10.810	 Test Loss 1.369
65.26
0.6526
loss: 1.3688732257843017
(35, 0, 98) triplet: 0.125
(35, 0): 0.005
(35, 98): 0.13
not enough sample
* Epoch: [47/60]	 Top 1-err 17.015  Top 5-err 2.696	 Train Loss 0.625
* Epoch: [47/60]	 Top 1-err 34.670  Top 5-err 10.850	 Test Loss 1.368
Current best accuracy (top-1 and 5 error): 34.67 10.85
saving best model...
* Epoch: [47/60]	 Top 1-err 34.670  Top 5-err 10.850	 Test Loss 1.368
65.33
0.6533
loss: 1.3683608768463136
(35, 0, 98) triplet: 0.12
(35, 0): 0.005
(35, 98): 0.125
* Epoch: [48/60]	 Top 1-err 17.066  Top 5-err 2.773	 Train Loss 0.628
* Epoch: [48/60]	 Top 1-err 34.650  Top 5-err 10.590	 Test Loss 1.365
Current best accuracy (top-1 and 5 error): 34.65 10.59
saving best model...
* Epoch: [48/60]	 Top 1-err 34.650  Top 5-err 10.590	 Test Loss 1.365
65.35
0.6535
loss: 1.3650957530975343
(35, 0, 98) triplet: 0.12
(35, 0): 0.005
(35, 98): 0.125
* Epoch: [49/60]	 Top 1-err 16.917  Top 5-err 2.796	 Train Loss 0.623
* Epoch: [49/60]	 Top 1-err 34.630  Top 5-err 10.670	 Test Loss 1.365
Current best accuracy (top-1 and 5 error): 34.63 10.67
saving best model...
* Epoch: [49/60]	 Top 1-err 34.630  Top 5-err 10.670	 Test Loss 1.365
65.37
0.6537
loss: 1.3649054157257081
(35, 0, 98) triplet: 0.11
(35, 0): 0.005
(35, 98): 0.115
* Epoch: [50/60]	 Top 1-err 17.038  Top 5-err 2.739	 Train Loss 0.622
* Epoch: [50/60]	 Top 1-err 34.770  Top 5-err 10.740	 Test Loss 1.366
Current best accuracy (top-1 and 5 error): 34.63 10.67
* Epoch: [50/60]	 Top 1-err 34.770  Top 5-err 10.740	 Test Loss 1.366
65.23
0.6523
loss: 1.3664724533081054
(35, 0, 98) triplet: 0.12
(35, 0): 0.005
(35, 98): 0.125
* Epoch: [51/60]	 Top 1-err 16.700  Top 5-err 2.697	 Train Loss 0.618
* Epoch: [51/60]	 Top 1-err 34.790  Top 5-err 10.800	 Test Loss 1.368
Current best accuracy (top-1 and 5 error): 34.63 10.67
* Epoch: [51/60]	 Top 1-err 34.790  Top 5-err 10.800	 Test Loss 1.368
65.21
0.6521
loss: 1.3682117486953735
(35, 0, 98) triplet: 0.10999999999999999
(35, 0): 0.005
(35, 98): 0.11499999999999999
* Epoch: [52/60]	 Top 1-err 16.687  Top 5-err 2.741	 Train Loss 0.615
* Epoch: [52/60]	 Top 1-err 34.750  Top 5-err 10.690	 Test Loss 1.371
Current best accuracy (top-1 and 5 error): 34.63 10.67
* Epoch: [52/60]	 Top 1-err 34.750  Top 5-err 10.690	 Test Loss 1.371
65.25
0.6525
loss: 1.3710834632873534
(35, 0, 98) triplet: 0.13
(35, 0): 0.005
(35, 98): 0.135
* Epoch: [53/60]	 Top 1-err 16.721  Top 5-err 2.705	 Train Loss 0.619
* Epoch: [53/60]	 Top 1-err 34.880  Top 5-err 10.750	 Test Loss 1.377
Current best accuracy (top-1 and 5 error): 34.63 10.67
* Epoch: [53/60]	 Top 1-err 34.880  Top 5-err 10.750	 Test Loss 1.377
65.12
0.6512
loss: 1.3770443588256835
(35, 0, 98) triplet: 0.11499999999999999
(35, 0): 0.005
(35, 98): 0.12
* Epoch: [54/60]	 Top 1-err 16.827  Top 5-err 2.660	 Train Loss 0.617
* Epoch: [54/60]	 Top 1-err 34.760  Top 5-err 10.740	 Test Loss 1.370
Current best accuracy (top-1 and 5 error): 34.63 10.67
* Epoch: [54/60]	 Top 1-err 34.760  Top 5-err 10.740	 Test Loss 1.370
65.24
0.6524
loss: 1.3700024742126464
(35, 0, 98) triplet: 0.125
(35, 0): 0.005
(35, 98): 0.13
* Epoch: [55/60]	 Top 1-err 16.849  Top 5-err 2.622	 Train Loss 0.617
* Epoch: [55/60]	 Top 1-err 34.700  Top 5-err 10.730	 Test Loss 1.368
Current best accuracy (top-1 and 5 error): 34.63 10.67
* Epoch: [55/60]	 Top 1-err 34.700  Top 5-err 10.730	 Test Loss 1.368
65.3
0.653
loss: 1.3682437738418578
(35, 0, 98) triplet: 0.12
(35, 0): 0.005
(35, 98): 0.125
* Epoch: [56/60]	 Top 1-err 16.617  Top 5-err 2.688	 Train Loss 0.613
* Epoch: [56/60]	 Top 1-err 34.810  Top 5-err 10.700	 Test Loss 1.371
Current best accuracy (top-1 and 5 error): 34.63 10.67
* Epoch: [56/60]	 Top 1-err 34.810  Top 5-err 10.700	 Test Loss 1.371
65.19
0.6519
loss: 1.371495902633667
(35, 0, 98) triplet: 0.13
(35, 0): 0.005
(35, 98): 0.135
* Epoch: [57/60]	 Top 1-err 16.777  Top 5-err 2.607	 Train Loss 0.614
* Epoch: [57/60]	 Top 1-err 34.700  Top 5-err 10.730	 Test Loss 1.371
Current best accuracy (top-1 and 5 error): 34.63 10.67
* Epoch: [57/60]	 Top 1-err 34.700  Top 5-err 10.730	 Test Loss 1.371
65.3
0.653
loss: 1.3706134527206422
(35, 0, 98) triplet: 0.115
(35, 0): 0.005
(35, 98): 0.12000000000000001
not enough sample
* Epoch: [58/60]	 Top 1-err 16.704  Top 5-err 2.588	 Train Loss 0.612
* Epoch: [58/60]	 Top 1-err 34.600  Top 5-err 10.720	 Test Loss 1.368
Current best accuracy (top-1 and 5 error): 34.6 10.72
saving best model...
* Epoch: [58/60]	 Top 1-err 34.600  Top 5-err 10.720	 Test Loss 1.368
65.4
0.654
loss: 1.368164482688904
(35, 0, 98) triplet: 0.12
(35, 0): 0.005
(35, 98): 0.125
* Epoch: [59/60]	 Top 1-err 16.811  Top 5-err 2.660	 Train Loss 0.611
* Epoch: [59/60]	 Top 1-err 34.790  Top 5-err 10.800	 Test Loss 1.373
Current best accuracy (top-1 and 5 error): 34.6 10.72
* Epoch: [59/60]	 Top 1-err 34.790  Top 5-err 10.800	 Test Loss 1.373
65.21
0.6521
loss: 1.3728609043121338
(35, 0, 98) triplet: 0.135
(35, 0): 0.005
(35, 98): 0.14
Best accuracy (top-1 and 5 error): 34.6 10.72
=> loading checkpoint 'runs/cifar100_resnet_1/model_best.pth.tar'
* Epoch: [-1/60]	 Top 1-err 34.600  Top 5-err 10.720	 Test Loss 1.368
65.4
0.654
loss: 1.3681644729614257
(35, 0, 98) triplet: 0.12
(35, 0): 0.005
(35, 98): 0.125
python3 cifar100_repair_bias.py --net_type resnet --dataset cifar100 --depth 50 --batch_size 256 --lr 0.1 --expname cifar100_resnet_1 --epochs 60 --beta 1.0 --cutmix_prob 0 --pretrained ./runs/cifar100_resnet_1/model_best.pth.tar --expid 0 --first 35 --second 0 --third 98 --lam 2
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
=> creating model 'resnet'
True
=> loading checkpoint './runs/cifar100_resnet_1/model_best.pth.tar'
=> loaded checkpoint './runs/cifar100_resnet_1/model_best.pth.tar'
the number of model parameters: 521716
* Epoch: [0/60]	 Top 1-err 43.648  Top 5-err 14.263	 Train Loss 1.689
* Epoch: [0/60]	 Top 1-err 53.070  Top 5-err 22.150	 Test Loss 2.089
* Epoch: [0/60]	 Top 1-err 53.070  Top 5-err 22.150	 Test Loss 2.089
46.93
0.4693
loss: 2.0894076782226563
(35, 0, 98) triplet: 0.195
(35, 0): 0.005
(35, 98): 0.2
* Epoch: [1/60]	 Top 1-err 43.889  Top 5-err 14.093	 Train Loss 1.668
* Epoch: [1/60]	 Top 1-err 50.720  Top 5-err 20.590	 Test Loss 2.001
* Epoch: [1/60]	 Top 1-err 50.720  Top 5-err 20.590	 Test Loss 2.001
49.28
0.4928
loss: 2.001178561592102
(35, 0, 98) triplet: 0.19999999999999998
(35, 0): 0.005
(35, 98): 0.205
* Epoch: [2/60]	 Top 1-err 38.982  Top 5-err 10.824	 Train Loss 1.436
* Epoch: [2/60]	 Top 1-err 57.240  Top 5-err 27.050	 Test Loss 2.466
* Epoch: [2/60]	 Top 1-err 57.240  Top 5-err 27.050	 Test Loss 2.466
42.76
0.4276
loss: 2.4656313875198363
(35, 0, 98) triplet: 0.16
(35, 0): 0.04
(35, 98): 0.2
* Epoch: [3/60]	 Top 1-err 38.923  Top 5-err 11.215	 Train Loss 1.439
* Epoch: [3/60]	 Top 1-err 50.920  Top 5-err 20.750	 Test Loss 2.008
* Epoch: [3/60]	 Top 1-err 50.920  Top 5-err 20.750	 Test Loss 2.008
49.08
0.4908
loss: 2.0080708126068116
(35, 0, 98) triplet: 0.1
(35, 0): 0.0
(35, 98): 0.1
* Epoch: [4/60]	 Top 1-err 36.912  Top 5-err 9.921	 Train Loss 1.351
* Epoch: [4/60]	 Top 1-err 51.240  Top 5-err 21.100	 Test Loss 2.057
* Epoch: [4/60]	 Top 1-err 51.240  Top 5-err 21.100	 Test Loss 2.057
48.76
0.4876
loss: 2.0571746253967285
(35, 0, 98) triplet: 0.05
(35, 0): 0.125
(35, 98): 0.075
* Epoch: [5/60]	 Top 1-err 36.109  Top 5-err 9.411	 Train Loss 1.306
* Epoch: [5/60]	 Top 1-err 48.560  Top 5-err 19.690	 Test Loss 1.927
* Epoch: [5/60]	 Top 1-err 48.560  Top 5-err 19.690	 Test Loss 1.927
51.44
0.5144
loss: 1.9269767881393434
(35, 0, 98) triplet: 0.030000000000000002
(35, 0): 0.005
(35, 98): 0.035
* Epoch: [6/60]	 Top 1-err 34.352  Top 5-err 8.676	 Train Loss 1.242
* Epoch: [6/60]	 Top 1-err 44.510  Top 5-err 16.230	 Test Loss 1.739
* Epoch: [6/60]	 Top 1-err 44.510  Top 5-err 16.230	 Test Loss 1.739
55.49
0.5549
loss: 1.7388221912384034
(35, 0, 98) triplet: 0.20500000000000002
(35, 0): 0.015
(35, 98): 0.22
* Epoch: [7/60]	 Top 1-err 34.375  Top 5-err 8.464	 Train Loss 1.231
* Epoch: [7/60]	 Top 1-err 43.810  Top 5-err 15.520	 Test Loss 1.662
* Epoch: [7/60]	 Top 1-err 43.810  Top 5-err 15.520	 Test Loss 1.662
56.19
0.5619
loss: 1.662299541091919
(35, 0, 98) triplet: 0.29000000000000004
(35, 0): 0.05
(35, 98): 0.34
* Epoch: [8/60]	 Top 1-err 34.458  Top 5-err 8.517	 Train Loss 1.228
* Epoch: [8/60]	 Top 1-err 48.050  Top 5-err 19.370	 Test Loss 2.026
* Epoch: [8/60]	 Top 1-err 48.050  Top 5-err 19.370	 Test Loss 2.026
51.95
0.5195
loss: 2.0260425159454347
(35, 0, 98) triplet: 0.010000000000000002
(35, 0): 0.045
(35, 98): 0.055
* Epoch: [9/60]	 Top 1-err 34.246  Top 5-err 8.330	 Train Loss 1.215
* Epoch: [9/60]	 Top 1-err 51.430  Top 5-err 21.250	 Test Loss 2.083
* Epoch: [9/60]	 Top 1-err 51.430  Top 5-err 21.250	 Test Loss 2.083
48.57
0.4857
loss: 2.082876671218872
(35, 0, 98) triplet: 0.24
(35, 0): 0.0
(35, 98): 0.24
* Epoch: [10/60]	 Top 1-err 34.924  Top 5-err 9.033	 Train Loss 1.270
* Epoch: [10/60]	 Top 1-err 45.230  Top 5-err 16.780	 Test Loss 1.790
* Epoch: [10/60]	 Top 1-err 45.230  Top 5-err 16.780	 Test Loss 1.790
54.77
0.5477
loss: 1.789574513244629
(35, 0, 98) triplet: 0.165
(35, 0): 0.035
(35, 98): 0.2
* Epoch: [11/60]	 Top 1-err 33.546  Top 5-err 8.102	 Train Loss 1.201
* Epoch: [11/60]	 Top 1-err 42.620  Top 5-err 15.650	 Test Loss 1.670
* Epoch: [11/60]	 Top 1-err 42.620  Top 5-err 15.650	 Test Loss 1.670
57.38
0.5738
loss: 1.670245002746582
(35, 0, 98) triplet: 0.02500000000000001
(35, 0): 0.1
(35, 98): 0.075
* Epoch: [12/60]	 Top 1-err 33.190  Top 5-err 7.705	 Train Loss 1.172
* Epoch: [12/60]	 Top 1-err 45.540  Top 5-err 16.880	 Test Loss 1.778
* Epoch: [12/60]	 Top 1-err 45.540  Top 5-err 16.880	 Test Loss 1.778
54.46
0.5446
loss: 1.7783782577514649
(35, 0, 98) triplet: 0.275
(35, 0): 0.02
(35, 98): 0.29500000000000004
* Epoch: [13/60]	 Top 1-err 34.362  Top 5-err 8.632	 Train Loss 1.234
* Epoch: [13/60]	 Top 1-err 44.250  Top 5-err 16.600	 Test Loss 1.700
* Epoch: [13/60]	 Top 1-err 44.250  Top 5-err 16.600	 Test Loss 1.700
55.75
0.5575
loss: 1.699507912826538
(35, 0, 98) triplet: 0.135
(35, 0): 0.15
(35, 98): 0.015
not enough sample
* Epoch: [14/60]	 Top 1-err 34.397  Top 5-err 8.757	 Train Loss 1.244
* Epoch: [14/60]	 Top 1-err 46.750  Top 5-err 18.070	 Test Loss 1.961
* Epoch: [14/60]	 Top 1-err 46.750  Top 5-err 18.070	 Test Loss 1.961
53.25
0.5325
loss: 1.9608044855117799
(35, 0, 98) triplet: 0.045
(35, 0): 0.075
(35, 98): 0.12
* Epoch: [15/60]	 Top 1-err 33.487  Top 5-err 8.162	 Train Loss 1.198
* Epoch: [15/60]	 Top 1-err 45.400  Top 5-err 16.890	 Test Loss 1.770
* Epoch: [15/60]	 Top 1-err 45.400  Top 5-err 16.890	 Test Loss 1.770
54.6
0.546
loss: 1.7702013530731202
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.015
(35, 98): 0.185
* Epoch: [16/60]	 Top 1-err 32.743  Top 5-err 7.425	 Train Loss 1.156
* Epoch: [16/60]	 Top 1-err 41.560  Top 5-err 14.970	 Test Loss 1.588
* Epoch: [16/60]	 Top 1-err 41.560  Top 5-err 14.970	 Test Loss 1.588
58.44
0.5844
loss: 1.5882881187438964
(35, 0, 98) triplet: 0.21999999999999997
(35, 0): 0.005
(35, 98): 0.22499999999999998
* Epoch: [17/60]	 Top 1-err 31.171  Top 5-err 6.776	 Train Loss 1.086
* Epoch: [17/60]	 Top 1-err 46.410  Top 5-err 17.130	 Test Loss 1.871
* Epoch: [17/60]	 Top 1-err 46.410  Top 5-err 17.130	 Test Loss 1.871
53.59
0.5359
loss: 1.8712828205108643
(35, 0, 98) triplet: 0.305
(35, 0): 0.0
(35, 98): 0.305
* Epoch: [18/60]	 Top 1-err 32.144  Top 5-err 7.269	 Train Loss 1.143
* Epoch: [18/60]	 Top 1-err 45.630  Top 5-err 18.090	 Test Loss 1.844
* Epoch: [18/60]	 Top 1-err 45.630  Top 5-err 18.090	 Test Loss 1.844
54.37
0.5437
loss: 1.8438384765625
(35, 0, 98) triplet: 0.05500000000000001
(35, 0): 0.115
(35, 98): 0.06
* Epoch: [19/60]	 Top 1-err 33.948  Top 5-err 8.274	 Train Loss 1.217
* Epoch: [19/60]	 Top 1-err 44.380  Top 5-err 16.200	 Test Loss 1.732
* Epoch: [19/60]	 Top 1-err 44.380  Top 5-err 16.200	 Test Loss 1.732
55.62
0.5562
loss: 1.7322364692687988
(35, 0, 98) triplet: 0.0
(35, 0): 0.02
(35, 98): 0.02
* Epoch: [20/60]	 Top 1-err 31.747  Top 5-err 7.333	 Train Loss 1.121
* Epoch: [20/60]	 Top 1-err 45.490  Top 5-err 16.750	 Test Loss 1.840
* Epoch: [20/60]	 Top 1-err 45.490  Top 5-err 16.750	 Test Loss 1.840
54.51
0.5451
loss: 1.8402226831436157
(35, 0, 98) triplet: 0.135
(35, 0): 0.01
(35, 98): 0.14500000000000002
not enough sample
* Epoch: [21/60]	 Top 1-err 33.236  Top 5-err 7.760	 Train Loss 1.183
* Epoch: [21/60]	 Top 1-err 44.580  Top 5-err 16.210	 Test Loss 1.780
* Epoch: [21/60]	 Top 1-err 44.580  Top 5-err 16.210	 Test Loss 1.780
55.42
0.5542
loss: 1.779803699493408
(35, 0, 98) triplet: 0.28500000000000003
(35, 0): 0.01
(35, 98): 0.29500000000000004
* Epoch: [22/60]	 Top 1-err 31.026  Top 5-err 6.940	 Train Loss 1.098
* Epoch: [22/60]	 Top 1-err 45.610  Top 5-err 17.080	 Test Loss 1.855
* Epoch: [22/60]	 Top 1-err 45.610  Top 5-err 17.080	 Test Loss 1.855
54.39
0.5439
loss: 1.855364114379883
(35, 0, 98) triplet: 0.01999999999999999
(35, 0): 0.1
(35, 98): 0.12
* Epoch: [23/60]	 Top 1-err 32.161  Top 5-err 7.376	 Train Loss 1.145
* Epoch: [23/60]	 Top 1-err 46.600  Top 5-err 18.030	 Test Loss 1.883
* Epoch: [23/60]	 Top 1-err 46.600  Top 5-err 18.030	 Test Loss 1.883
53.4
0.534
loss: 1.8827188388824463
(35, 0, 98) triplet: 0.09
(35, 0): 0.155
(35, 98): 0.065
* Epoch: [24/60]	 Top 1-err 31.405  Top 5-err 7.112	 Train Loss 1.130
* Epoch: [24/60]	 Top 1-err 48.680  Top 5-err 19.220	 Test Loss 2.051
* Epoch: [24/60]	 Top 1-err 48.680  Top 5-err 19.220	 Test Loss 2.051
51.32
0.5132
loss: 2.0513455055236816
(35, 0, 98) triplet: 0.165
(35, 0): 0.0
(35, 98): 0.165
* Epoch: [25/60]	 Top 1-err 29.052  Top 5-err 6.309	 Train Loss 1.036
* Epoch: [25/60]	 Top 1-err 44.810  Top 5-err 16.980	 Test Loss 1.819
* Epoch: [25/60]	 Top 1-err 44.810  Top 5-err 16.980	 Test Loss 1.819
55.19
0.5519
loss: 1.818505909729004
(35, 0, 98) triplet: 0.12499999999999999
(35, 0): 0.02
(35, 98): 0.145
* Epoch: [26/60]	 Top 1-err 30.637  Top 5-err 6.896	 Train Loss 1.080
* Epoch: [26/60]	 Top 1-err 43.720  Top 5-err 15.880	 Test Loss 1.726
* Epoch: [26/60]	 Top 1-err 43.720  Top 5-err 15.880	 Test Loss 1.726
56.28
0.5628
loss: 1.7260278972625733
(35, 0, 98) triplet: 0.12
(35, 0): 0.005
(35, 98): 0.125
* Epoch: [27/60]	 Top 1-err 31.305  Top 5-err 7.329	 Train Loss 1.134
* Epoch: [27/60]	 Top 1-err 44.770  Top 5-err 16.940	 Test Loss 1.828
* Epoch: [27/60]	 Top 1-err 44.770  Top 5-err 16.940	 Test Loss 1.828
55.23
0.5523
loss: 1.8281170471191406
(35, 0, 98) triplet: 0.125
(35, 0): 0.005
(35, 98): 0.13
* Epoch: [28/60]	 Top 1-err 30.321  Top 5-err 6.624	 Train Loss 1.068
* Epoch: [28/60]	 Top 1-err 41.580  Top 5-err 14.930	 Test Loss 1.667
* Epoch: [28/60]	 Top 1-err 41.580  Top 5-err 14.930	 Test Loss 1.667
58.42
0.5842
loss: 1.666582698059082
(35, 0, 98) triplet: 0.03
(35, 0): 0.055
(35, 98): 0.025
* Epoch: [29/60]	 Top 1-err 30.344  Top 5-err 6.517	 Train Loss 1.074
* Epoch: [29/60]	 Top 1-err 44.990  Top 5-err 17.230	 Test Loss 1.814
* Epoch: [29/60]	 Top 1-err 44.990  Top 5-err 17.230	 Test Loss 1.814
55.01
0.5501
loss: 1.8136126914978028
(35, 0, 98) triplet: 0.004999999999999977
(35, 0): 0.165
(35, 98): 0.16999999999999998
* Epoch: [30/60]	 Top 1-err 25.096  Top 5-err 5.017	 Train Loss 0.905
* Epoch: [30/60]	 Top 1-err 35.300  Top 5-err 11.130	 Test Loss 1.340
* Epoch: [30/60]	 Top 1-err 35.300  Top 5-err 11.130	 Test Loss 1.340
64.7
0.647
loss: 1.3399998306274414
(35, 0, 98) triplet: 0.12499999999999999
(35, 0): 0.015
(35, 98): 0.13999999999999999
* Epoch: [31/60]	 Top 1-err 21.318  Top 5-err 3.746	 Train Loss 0.777
* Epoch: [31/60]	 Top 1-err 34.270  Top 5-err 10.770	 Test Loss 1.315
* Epoch: [31/60]	 Top 1-err 34.270  Top 5-err 10.770	 Test Loss 1.315
65.73
0.6573
loss: 1.3150190324783326
(35, 0, 98) triplet: 0.12000000000000001
(35, 0): 0.015
(35, 98): 0.135
* Epoch: [32/60]	 Top 1-err 20.259  Top 5-err 3.411	 Train Loss 0.739
* Epoch: [32/60]	 Top 1-err 34.290  Top 5-err 10.630	 Test Loss 1.323
* Epoch: [32/60]	 Top 1-err 34.290  Top 5-err 10.630	 Test Loss 1.323
65.71
0.6571
loss: 1.323257848739624
(35, 0, 98) triplet: 0.07
(35, 0): 0.01
(35, 98): 0.08
* Epoch: [33/60]	 Top 1-err 19.418  Top 5-err 3.128	 Train Loss 0.713
* Epoch: [33/60]	 Top 1-err 33.800  Top 5-err 10.790	 Test Loss 1.311
* Epoch: [33/60]	 Top 1-err 33.800  Top 5-err 10.790	 Test Loss 1.311
66.2
0.662
loss: 1.3106964210510255
(35, 0, 98) triplet: 0.10500000000000001
(35, 0): 0.015
(35, 98): 0.12000000000000001
* Epoch: [34/60]	 Top 1-err 18.831  Top 5-err 3.139	 Train Loss 0.694
* Epoch: [34/60]	 Top 1-err 33.940  Top 5-err 10.690	 Test Loss 1.321
* Epoch: [34/60]	 Top 1-err 33.940  Top 5-err 10.690	 Test Loss 1.321
66.06
0.6606
loss: 1.3205660144805909
(35, 0, 98) triplet: 0.10999999999999999
(35, 0): 0.005
(35, 98): 0.11499999999999999
* Epoch: [35/60]	 Top 1-err 18.544  Top 5-err 2.945	 Train Loss 0.681
* Epoch: [35/60]	 Top 1-err 33.680  Top 5-err 10.590	 Test Loss 1.321
* Epoch: [35/60]	 Top 1-err 33.680  Top 5-err 10.590	 Test Loss 1.321
66.32
0.6632
loss: 1.3212636661529542
(35, 0, 98) triplet: 0.105
(35, 0): 0.015
(35, 98): 0.12
* Epoch: [36/60]	 Top 1-err 17.883  Top 5-err 2.752	 Train Loss 0.668
* Epoch: [36/60]	 Top 1-err 33.910  Top 5-err 10.690	 Test Loss 1.335
* Epoch: [36/60]	 Top 1-err 33.910  Top 5-err 10.690	 Test Loss 1.335
66.09
0.6609
loss: 1.334523363494873
(35, 0, 98) triplet: 0.125
(35, 0): 0.0
(35, 98): 0.125
* Epoch: [37/60]	 Top 1-err 17.809  Top 5-err 2.813	 Train Loss 0.661
* Epoch: [37/60]	 Top 1-err 33.580  Top 5-err 10.620	 Test Loss 1.320
* Epoch: [37/60]	 Top 1-err 33.580  Top 5-err 10.620	 Test Loss 1.320
66.42
0.6642
loss: 1.320199593925476
(35, 0, 98) triplet: 0.11
(35, 0): 0.015
(35, 98): 0.125
not enough sample
* Epoch: [38/60]	 Top 1-err 17.728  Top 5-err 2.665	 Train Loss 0.653
* Epoch: [38/60]	 Top 1-err 34.150  Top 5-err 10.540	 Test Loss 1.343
* Epoch: [38/60]	 Top 1-err 34.150  Top 5-err 10.540	 Test Loss 1.343
65.85
0.6585
loss: 1.3429260808944703
(35, 0, 98) triplet: 0.06999999999999999
(35, 0): 0.015
(35, 98): 0.08499999999999999
* Epoch: [39/60]	 Top 1-err 17.286  Top 5-err 2.543	 Train Loss 0.637
* Epoch: [39/60]	 Top 1-err 33.910  Top 5-err 10.640	 Test Loss 1.344
* Epoch: [39/60]	 Top 1-err 33.910  Top 5-err 10.640	 Test Loss 1.344
66.09
0.6609
loss: 1.3440261695861817
(35, 0, 98) triplet: 0.09500000000000001
(35, 0): 0.01
(35, 98): 0.10500000000000001
not enough sample
* Epoch: [40/60]	 Top 1-err 17.061  Top 5-err 2.493	 Train Loss 0.629
* Epoch: [40/60]	 Top 1-err 33.570  Top 5-err 10.580	 Test Loss 1.341
* Epoch: [40/60]	 Top 1-err 33.570  Top 5-err 10.580	 Test Loss 1.341
66.43
0.6643
loss: 1.3412890907287598
(35, 0, 98) triplet: 0.11
(35, 0): 0.015
(35, 98): 0.125
* Epoch: [41/60]	 Top 1-err 16.781  Top 5-err 2.478	 Train Loss 0.629
* Epoch: [41/60]	 Top 1-err 33.870  Top 5-err 10.180	 Test Loss 1.346
* Epoch: [41/60]	 Top 1-err 33.870  Top 5-err 10.180	 Test Loss 1.346
66.13
0.6613
loss: 1.3459714848518372
(35, 0, 98) triplet: 0.13
(35, 0): 0.005
(35, 98): 0.135
* Epoch: [42/60]	 Top 1-err 16.590  Top 5-err 2.403	 Train Loss 0.615
* Epoch: [42/60]	 Top 1-err 33.890  Top 5-err 10.500	 Test Loss 1.353
* Epoch: [42/60]	 Top 1-err 33.890  Top 5-err 10.500	 Test Loss 1.353
66.11
0.6611
loss: 1.3529003833770752
(35, 0, 98) triplet: 0.165
(35, 0): 0.005
(35, 98): 0.17
* Epoch: [43/60]	 Top 1-err 16.623  Top 5-err 2.450	 Train Loss 0.611
* Epoch: [43/60]	 Top 1-err 33.990  Top 5-err 10.620	 Test Loss 1.358
* Epoch: [43/60]	 Top 1-err 33.990  Top 5-err 10.620	 Test Loss 1.358
66.01
0.6601
loss: 1.3583394428253175
(35, 0, 98) triplet: 0.18
(35, 0): 0.005
(35, 98): 0.185
* Epoch: [44/60]	 Top 1-err 16.143  Top 5-err 2.286	 Train Loss 0.603
* Epoch: [44/60]	 Top 1-err 34.060  Top 5-err 10.650	 Test Loss 1.368
* Epoch: [44/60]	 Top 1-err 34.060  Top 5-err 10.650	 Test Loss 1.368
65.94
0.6594
loss: 1.3681532720565797
(35, 0, 98) triplet: 0.11
(35, 0): 0.005
(35, 98): 0.115
* Epoch: [45/60]	 Top 1-err 15.544  Top 5-err 2.235	 Train Loss 0.580
* Epoch: [45/60]	 Top 1-err 33.430  Top 5-err 10.390	 Test Loss 1.343
Current best accuracy (top-1 and 5 error): 33.43 10.39
saving best model...
* Epoch: [45/60]	 Top 1-err 33.430  Top 5-err 10.390	 Test Loss 1.343
66.57
0.6657
loss: 1.3434888967514038
(35, 0, 98) triplet: 0.12
(35, 0): 0.005
(35, 98): 0.125
* Epoch: [46/60]	 Top 1-err 15.164  Top 5-err 2.223	 Train Loss 0.572
* Epoch: [46/60]	 Top 1-err 33.350  Top 5-err 10.390	 Test Loss 1.342
Current best accuracy (top-1 and 5 error): 33.35 10.39
saving best model...
* Epoch: [46/60]	 Top 1-err 33.350  Top 5-err 10.390	 Test Loss 1.342
66.65
0.6665
loss: 1.341692664527893
(35, 0, 98) triplet: 0.125
(35, 0): 0.005
(35, 98): 0.13
not enough sample
* Epoch: [47/60]	 Top 1-err 15.000  Top 5-err 2.072	 Train Loss 0.565
* Epoch: [47/60]	 Top 1-err 33.200  Top 5-err 10.430	 Test Loss 1.344
Current best accuracy (top-1 and 5 error): 33.2 10.43
saving best model...
* Epoch: [47/60]	 Top 1-err 33.200  Top 5-err 10.430	 Test Loss 1.344
66.8
0.668
loss: 1.3437126066207885
(35, 0, 98) triplet: 0.115
(35, 0): 0.005
(35, 98): 0.12000000000000001
* Epoch: [48/60]	 Top 1-err 15.089  Top 5-err 2.095	 Train Loss 0.566
* Epoch: [48/60]	 Top 1-err 33.420  Top 5-err 10.440	 Test Loss 1.342
Current best accuracy (top-1 and 5 error): 33.2 10.43
* Epoch: [48/60]	 Top 1-err 33.420  Top 5-err 10.440	 Test Loss 1.342
66.58
0.6658
loss: 1.3422829242706298
(35, 0, 98) triplet: 0.12
(35, 0): 0.005
(35, 98): 0.125
* Epoch: [49/60]	 Top 1-err 14.962  Top 5-err 2.093	 Train Loss 0.566
* Epoch: [49/60]	 Top 1-err 33.550  Top 5-err 10.450	 Test Loss 1.341
Current best accuracy (top-1 and 5 error): 33.2 10.43
* Epoch: [49/60]	 Top 1-err 33.550  Top 5-err 10.450	 Test Loss 1.341
66.45
0.6645
loss: 1.3411525276184082
(35, 0, 98) triplet: 0.11499999999999999
(35, 0): 0.005
(35, 98): 0.12
* Epoch: [50/60]	 Top 1-err 14.730  Top 5-err 2.044	 Train Loss 0.559
* Epoch: [50/60]	 Top 1-err 33.340  Top 5-err 10.430	 Test Loss 1.344
Current best accuracy (top-1 and 5 error): 33.2 10.43
* Epoch: [50/60]	 Top 1-err 33.340  Top 5-err 10.430	 Test Loss 1.344
66.66
0.6666
loss: 1.3444466369628907
(35, 0, 98) triplet: 0.125
(35, 0): 0.005
(35, 98): 0.13
* Epoch: [51/60]	 Top 1-err 14.571  Top 5-err 2.087	 Train Loss 0.556
* Epoch: [51/60]	 Top 1-err 33.540  Top 5-err 10.410	 Test Loss 1.346
Current best accuracy (top-1 and 5 error): 33.2 10.43
* Epoch: [51/60]	 Top 1-err 33.540  Top 5-err 10.410	 Test Loss 1.346
66.46
0.6646
loss: 1.3461495670318604
(35, 0, 98) triplet: 0.11499999999999999
(35, 0): 0.005
(35, 98): 0.12
* Epoch: [52/60]	 Top 1-err 14.513  Top 5-err 2.155	 Train Loss 0.557
* Epoch: [52/60]	 Top 1-err 33.330  Top 5-err 10.480	 Test Loss 1.350
Current best accuracy (top-1 and 5 error): 33.2 10.43
* Epoch: [52/60]	 Top 1-err 33.330  Top 5-err 10.480	 Test Loss 1.350
66.67
0.6667
loss: 1.3502276329040528
(35, 0, 98) triplet: 0.13
(35, 0): 0.005
(35, 98): 0.135
* Epoch: [53/60]	 Top 1-err 14.807  Top 5-err 2.082	 Train Loss 0.558
* Epoch: [53/60]	 Top 1-err 33.380  Top 5-err 10.560	 Test Loss 1.356
Current best accuracy (top-1 and 5 error): 33.2 10.43
* Epoch: [53/60]	 Top 1-err 33.380  Top 5-err 10.560	 Test Loss 1.356
66.62
0.6662
loss: 1.3556903896331787
(35, 0, 98) triplet: 0.125
(35, 0): 0.005
(35, 98): 0.13
* Epoch: [54/60]	 Top 1-err 14.641  Top 5-err 2.121	 Train Loss 0.559
* Epoch: [54/60]	 Top 1-err 33.450  Top 5-err 10.640	 Test Loss 1.347
Current best accuracy (top-1 and 5 error): 33.2 10.43
* Epoch: [54/60]	 Top 1-err 33.450  Top 5-err 10.640	 Test Loss 1.347
66.55
0.6655
loss: 1.3473882190704345
(35, 0, 98) triplet: 0.125
(35, 0): 0.005
(35, 98): 0.13
* Epoch: [55/60]	 Top 1-err 14.688  Top 5-err 2.053	 Train Loss 0.558
* Epoch: [55/60]	 Top 1-err 33.620  Top 5-err 10.500	 Test Loss 1.348
Current best accuracy (top-1 and 5 error): 33.2 10.43
* Epoch: [55/60]	 Top 1-err 33.620  Top 5-err 10.500	 Test Loss 1.348
66.38
0.6638
loss: 1.34838909034729
(35, 0, 98) triplet: 0.135
(35, 0): 0.005
(35, 98): 0.14
* Epoch: [56/60]	 Top 1-err 14.634  Top 5-err 2.000	 Train Loss 0.554
* Epoch: [56/60]	 Top 1-err 33.430  Top 5-err 10.400	 Test Loss 1.351
Current best accuracy (top-1 and 5 error): 33.2 10.43
* Epoch: [56/60]	 Top 1-err 33.430  Top 5-err 10.400	 Test Loss 1.351
66.57
0.6657
loss: 1.350559601211548
(35, 0, 98) triplet: 0.1
(35, 0): 0.005
(35, 98): 0.10500000000000001
* Epoch: [57/60]	 Top 1-err 14.686  Top 5-err 2.099	 Train Loss 0.551
* Epoch: [57/60]	 Top 1-err 33.450  Top 5-err 10.460	 Test Loss 1.352
Current best accuracy (top-1 and 5 error): 33.2 10.43
* Epoch: [57/60]	 Top 1-err 33.450  Top 5-err 10.460	 Test Loss 1.352
66.55
0.6655
loss: 1.3516759502410889
(35, 0, 98) triplet: 0.12
(35, 0): 0.005
(35, 98): 0.125
not enough sample
* Epoch: [58/60]	 Top 1-err 14.371  Top 5-err 2.008	 Train Loss 0.549
* Epoch: [58/60]	 Top 1-err 33.270  Top 5-err 10.380	 Test Loss 1.346
Current best accuracy (top-1 and 5 error): 33.2 10.43
* Epoch: [58/60]	 Top 1-err 33.270  Top 5-err 10.380	 Test Loss 1.346
66.73
0.6673
loss: 1.3455592332839965
(35, 0, 98) triplet: 0.125
(35, 0): 0.01
(35, 98): 0.135
* Epoch: [59/60]	 Top 1-err 14.558  Top 5-err 2.002	 Train Loss 0.550
* Epoch: [59/60]	 Top 1-err 33.450  Top 5-err 10.260	 Test Loss 1.354
Current best accuracy (top-1 and 5 error): 33.2 10.43
* Epoch: [59/60]	 Top 1-err 33.450  Top 5-err 10.260	 Test Loss 1.354
66.55
0.6655
loss: 1.3536575038909913
(35, 0, 98) triplet: 0.13
(35, 0): 0.005
(35, 98): 0.135
Best accuracy (top-1 and 5 error): 33.2 10.43
=> loading checkpoint 'runs/cifar100_resnet_1/model_best.pth.tar'
* Epoch: [-1/60]	 Top 1-err 33.200  Top 5-err 10.430	 Test Loss 1.344
66.8
0.668
loss: 1.343712601852417
(35, 0, 98) triplet: 0.115
(35, 0): 0.005
(35, 98): 0.12000000000000001
python3 cifar100_repair_bias.py --net_type resnet --dataset cifar100 --depth 50 --batch_size 256 --lr 0.1 --expname cifar100_resnet_1 --epochs 60 --beta 1.0 --cutmix_prob 1 --pretrained ./runs/cifar100_resnet_1/model_best.pth.tar --expid 0 --first 35 --second 0 --third 98 --lam 0
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
=> creating model 'resnet'
True
=> loading checkpoint './runs/cifar100_resnet_1/model_best.pth.tar'
=> loaded checkpoint './runs/cifar100_resnet_1/model_best.pth.tar'
the number of model parameters: 521716
* Epoch: [0/60]	 Top 1-err 56.976  Top 5-err 31.356	 Train Loss 2.678
* Epoch: [0/60]	 Top 1-err 39.630  Top 5-err 13.910	 Test Loss 1.461
* Epoch: [0/60]	 Top 1-err 39.630  Top 5-err 13.910	 Test Loss 1.461
60.37
0.6037
loss: 1.4610192165374756
(35, 0, 98) triplet: 0.19
(35, 0): 0.0
(35, 98): 0.19
* Epoch: [1/60]	 Top 1-err 58.695  Top 5-err 33.740	 Train Loss 2.630
* Epoch: [1/60]	 Top 1-err 39.480  Top 5-err 13.680	 Test Loss 1.452
* Epoch: [1/60]	 Top 1-err 39.480  Top 5-err 13.680	 Test Loss 1.452
60.52
0.6052
loss: 1.4522732120513917
(35, 0, 98) triplet: 0.245
(35, 0): 0.0
(35, 98): 0.245
* Epoch: [2/60]	 Top 1-err 55.880  Top 5-err 29.577	 Train Loss 2.531
* Epoch: [2/60]	 Top 1-err 38.090  Top 5-err 12.980	 Test Loss 1.396
* Epoch: [2/60]	 Top 1-err 38.090  Top 5-err 12.980	 Test Loss 1.396
61.91
0.6191
loss: 1.3964756580352784
(35, 0, 98) triplet: 0.19
(35, 0): 0.0
(35, 98): 0.19
* Epoch: [3/60]	 Top 1-err 55.927  Top 5-err 30.123	 Train Loss 2.531
* Epoch: [3/60]	 Top 1-err 40.270  Top 5-err 14.190	 Test Loss 1.503
* Epoch: [3/60]	 Top 1-err 40.270  Top 5-err 14.190	 Test Loss 1.503
59.73
0.5973
loss: 1.502779309463501
(35, 0, 98) triplet: 0.22999999999999998
(35, 0): 0.0
(35, 98): 0.22999999999999998
not enough sample
not enough sample
* Epoch: [4/60]	 Top 1-err 56.910  Top 5-err 30.701	 Train Loss 2.587
* Epoch: [4/60]	 Top 1-err 41.290  Top 5-err 14.120	 Test Loss 1.539
* Epoch: [4/60]	 Top 1-err 41.290  Top 5-err 14.120	 Test Loss 1.539
58.71
0.5871
loss: 1.5389992862701416
(35, 0, 98) triplet: 0.18
(35, 0): 0.0
(35, 98): 0.18
* Epoch: [5/60]	 Top 1-err 52.371  Top 5-err 26.188	 Train Loss 2.446
* Epoch: [5/60]	 Top 1-err 41.050  Top 5-err 14.130	 Test Loss 1.509
* Epoch: [5/60]	 Top 1-err 41.050  Top 5-err 14.130	 Test Loss 1.509
58.95
0.5895
loss: 1.508697756767273
(35, 0, 98) triplet: 0.20500000000000002
(35, 0): 0.0
(35, 98): 0.20500000000000002
* Epoch: [6/60]	 Top 1-err 53.649  Top 5-err 27.340	 Train Loss 2.486
* Epoch: [6/60]	 Top 1-err 38.970  Top 5-err 13.310	 Test Loss 1.432
* Epoch: [6/60]	 Top 1-err 38.970  Top 5-err 13.310	 Test Loss 1.432
61.03
0.6103
loss: 1.432493355178833
(35, 0, 98) triplet: 0.19
(35, 0): 0.0
(35, 98): 0.19
* Epoch: [7/60]	 Top 1-err 52.518  Top 5-err 27.068	 Train Loss 2.386
* Epoch: [7/60]	 Top 1-err 38.900  Top 5-err 13.610	 Test Loss 1.445
* Epoch: [7/60]	 Top 1-err 38.900  Top 5-err 13.610	 Test Loss 1.445
61.1
0.611
loss: 1.4445974758148192
(35, 0, 98) triplet: 0.2
(35, 0): 0.0
(35, 98): 0.2
* Epoch: [8/60]	 Top 1-err 53.525  Top 5-err 27.918	 Train Loss 2.408
* Epoch: [8/60]	 Top 1-err 39.600  Top 5-err 13.420	 Test Loss 1.444
* Epoch: [8/60]	 Top 1-err 39.600  Top 5-err 13.420	 Test Loss 1.444
60.4
0.604
loss: 1.4443025981903077
(35, 0, 98) triplet: 0.19
(35, 0): 0.0
(35, 98): 0.19
* Epoch: [9/60]	 Top 1-err 54.093  Top 5-err 28.500	 Train Loss 2.461
* Epoch: [9/60]	 Top 1-err 39.970  Top 5-err 13.940	 Test Loss 1.465
* Epoch: [9/60]	 Top 1-err 39.970  Top 5-err 13.940	 Test Loss 1.465
60.03
0.6003
loss: 1.4653078357696534
(35, 0, 98) triplet: 0.185
(35, 0): 0.0
(35, 98): 0.185
* Epoch: [10/60]	 Top 1-err 54.237  Top 5-err 28.162	 Train Loss 2.475
* Epoch: [10/60]	 Top 1-err 41.010  Top 5-err 14.190	 Test Loss 1.511
* Epoch: [10/60]	 Top 1-err 41.010  Top 5-err 14.190	 Test Loss 1.511
58.99
0.5899
loss: 1.5114144821166993
(35, 0, 98) triplet: 0.15000000000000002
(35, 0): 0.0
(35, 98): 0.15000000000000002
* Epoch: [11/60]	 Top 1-err 54.647  Top 5-err 28.054	 Train Loss 2.511
* Epoch: [11/60]	 Top 1-err 39.310  Top 5-err 13.160	 Test Loss 1.446
* Epoch: [11/60]	 Top 1-err 39.310  Top 5-err 13.160	 Test Loss 1.446
60.69
0.6069
loss: 1.4460058162689209
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.0
(35, 98): 0.16999999999999998
not enough sample
* Epoch: [12/60]	 Top 1-err 55.482  Top 5-err 29.422	 Train Loss 2.504
* Epoch: [12/60]	 Top 1-err 40.830  Top 5-err 14.370	 Test Loss 1.511
* Epoch: [12/60]	 Top 1-err 40.830  Top 5-err 14.370	 Test Loss 1.511
59.17
0.5917
loss: 1.5111033016204833
(35, 0, 98) triplet: 0.18
(35, 0): 0.01
(35, 98): 0.19
* Epoch: [13/60]	 Top 1-err 54.107  Top 5-err 27.869	 Train Loss 2.479
* Epoch: [13/60]	 Top 1-err 40.760  Top 5-err 14.120	 Test Loss 1.516
* Epoch: [13/60]	 Top 1-err 40.760  Top 5-err 14.120	 Test Loss 1.516
59.24
0.5924
loss: 1.5158737964630127
(35, 0, 98) triplet: 0.26
(35, 0): 0.0
(35, 98): 0.26
* Epoch: [14/60]	 Top 1-err 54.352  Top 5-err 27.796	 Train Loss 2.486
* Epoch: [14/60]	 Top 1-err 40.680  Top 5-err 14.080	 Test Loss 1.504
* Epoch: [14/60]	 Top 1-err 40.680  Top 5-err 14.080	 Test Loss 1.504
59.32
0.5932
loss: 1.5037863899230957
(35, 0, 98) triplet: 0.225
(35, 0): 0.0
(35, 98): 0.225
* Epoch: [15/60]	 Top 1-err 57.393  Top 5-err 30.839	 Train Loss 2.532
* Epoch: [15/60]	 Top 1-err 40.580  Top 5-err 14.370	 Test Loss 1.503
* Epoch: [15/60]	 Top 1-err 40.580  Top 5-err 14.370	 Test Loss 1.503
59.42
0.5942
loss: 1.5028772731781006
(35, 0, 98) triplet: 0.17
(35, 0): 0.0
(35, 98): 0.17
* Epoch: [16/60]	 Top 1-err 53.122  Top 5-err 26.471	 Train Loss 2.455
* Epoch: [16/60]	 Top 1-err 40.390  Top 5-err 14.370	 Test Loss 1.509
* Epoch: [16/60]	 Top 1-err 40.390  Top 5-err 14.370	 Test Loss 1.509
59.61
0.5961
loss: 1.509317189025879
(35, 0, 98) triplet: 0.195
(35, 0): 0.005
(35, 98): 0.2
not enough sample
* Epoch: [17/60]	 Top 1-err 54.397  Top 5-err 28.204	 Train Loss 2.420
* Epoch: [17/60]	 Top 1-err 42.020  Top 5-err 15.950	 Test Loss 1.591
* Epoch: [17/60]	 Top 1-err 42.020  Top 5-err 15.950	 Test Loss 1.591
57.98
0.5798
loss: 1.5909674629211426
(35, 0, 98) triplet: 0.19999999999999998
(35, 0): 0.0
(35, 98): 0.19999999999999998
* Epoch: [18/60]	 Top 1-err 55.438  Top 5-err 28.211	 Train Loss 2.465
* Epoch: [18/60]	 Top 1-err 39.510  Top 5-err 13.160	 Test Loss 1.461
* Epoch: [18/60]	 Top 1-err 39.510  Top 5-err 13.160	 Test Loss 1.461
60.49
0.6049
loss: 1.4605223461151122
(35, 0, 98) triplet: 0.23
(35, 0): 0.0
(35, 98): 0.23
* Epoch: [19/60]	 Top 1-err 54.052  Top 5-err 28.032	 Train Loss 2.400
* Epoch: [19/60]	 Top 1-err 40.260  Top 5-err 13.520	 Test Loss 1.502
* Epoch: [19/60]	 Top 1-err 40.260  Top 5-err 13.520	 Test Loss 1.502
59.74
0.5974
loss: 1.5017007085800171
(35, 0, 98) triplet: 0.18
(35, 0): 0.005
(35, 98): 0.185
* Epoch: [20/60]	 Top 1-err 51.058  Top 5-err 24.777	 Train Loss 2.344
* Epoch: [20/60]	 Top 1-err 42.490  Top 5-err 15.450	 Test Loss 1.590
* Epoch: [20/60]	 Top 1-err 42.490  Top 5-err 15.450	 Test Loss 1.590
57.51
0.5751
loss: 1.589763916015625
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [21/60]	 Top 1-err 54.777  Top 5-err 28.563	 Train Loss 2.433
* Epoch: [21/60]	 Top 1-err 39.710  Top 5-err 13.930	 Test Loss 1.461
* Epoch: [21/60]	 Top 1-err 39.710  Top 5-err 13.930	 Test Loss 1.461
60.29
0.6029
loss: 1.4605451025009155
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
not enough sample
* Epoch: [22/60]	 Top 1-err 50.591  Top 5-err 23.999	 Train Loss 2.386
* Epoch: [22/60]	 Top 1-err 40.420  Top 5-err 14.110	 Test Loss 1.495
* Epoch: [22/60]	 Top 1-err 40.420  Top 5-err 14.110	 Test Loss 1.495
59.58
0.5958
loss: 1.4954817436218262
(35, 0, 98) triplet: 0.25
(35, 0): 0.0
(35, 98): 0.25
* Epoch: [23/60]	 Top 1-err 53.002  Top 5-err 26.560	 Train Loss 2.399
* Epoch: [23/60]	 Top 1-err 41.170  Top 5-err 14.150	 Test Loss 1.541
* Epoch: [23/60]	 Top 1-err 41.170  Top 5-err 14.150	 Test Loss 1.541
58.83
0.5883
loss: 1.540737922668457
(35, 0, 98) triplet: 0.24
(35, 0): 0.0
(35, 98): 0.24
* Epoch: [24/60]	 Top 1-err 53.336  Top 5-err 26.630	 Train Loss 2.422
* Epoch: [24/60]	 Top 1-err 39.420  Top 5-err 12.790	 Test Loss 1.436
* Epoch: [24/60]	 Top 1-err 39.420  Top 5-err 12.790	 Test Loss 1.436
60.58
0.6058
loss: 1.4359166877746583
(35, 0, 98) triplet: 0.19999999999999998
(35, 0): 0.005
(35, 98): 0.205
* Epoch: [25/60]	 Top 1-err 52.491  Top 5-err 25.501	 Train Loss 2.421
* Epoch: [25/60]	 Top 1-err 39.560  Top 5-err 13.230	 Test Loss 1.453
* Epoch: [25/60]	 Top 1-err 39.560  Top 5-err 13.230	 Test Loss 1.453
60.44
0.6044
loss: 1.4532633974075317
(35, 0, 98) triplet: 0.195
(35, 0): 0.005
(35, 98): 0.2
* Epoch: [26/60]	 Top 1-err 52.535  Top 5-err 25.174	 Train Loss 2.426
* Epoch: [26/60]	 Top 1-err 40.770  Top 5-err 14.350	 Test Loss 1.533
* Epoch: [26/60]	 Top 1-err 40.770  Top 5-err 14.350	 Test Loss 1.533
59.23
0.5923
loss: 1.5329434997558593
(35, 0, 98) triplet: 0.36
(35, 0): 0.0
(35, 98): 0.36
* Epoch: [27/60]	 Top 1-err 52.815  Top 5-err 26.113	 Train Loss 2.435
* Epoch: [27/60]	 Top 1-err 39.630  Top 5-err 13.610	 Test Loss 1.490
* Epoch: [27/60]	 Top 1-err 39.630  Top 5-err 13.610	 Test Loss 1.490
60.37
0.6037
loss: 1.4903727813720704
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [28/60]	 Top 1-err 53.808  Top 5-err 27.454	 Train Loss 2.427
* Epoch: [28/60]	 Top 1-err 39.710  Top 5-err 13.600	 Test Loss 1.488
* Epoch: [28/60]	 Top 1-err 39.710  Top 5-err 13.600	 Test Loss 1.488
60.29
0.6029
loss: 1.4883780197143555
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [29/60]	 Top 1-err 52.731  Top 5-err 25.825	 Train Loss 2.407
* Epoch: [29/60]	 Top 1-err 39.570  Top 5-err 12.980	 Test Loss 1.442
* Epoch: [29/60]	 Top 1-err 39.570  Top 5-err 12.980	 Test Loss 1.442
60.43
0.6043
loss: 1.4422056270599366
(35, 0, 98) triplet: 0.18
(35, 0): 0.005
(35, 98): 0.185
* Epoch: [30/60]	 Top 1-err 50.119  Top 5-err 24.218	 Train Loss 2.299
* Epoch: [30/60]	 Top 1-err 33.210  Top 5-err 9.840	 Test Loss 1.213
* Epoch: [30/60]	 Top 1-err 33.210  Top 5-err 9.840	 Test Loss 1.213
66.79
0.6679
loss: 1.2131732181549073
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
* Epoch: [31/60]	 Top 1-err 48.863  Top 5-err 23.729	 Train Loss 2.225
* Epoch: [31/60]	 Top 1-err 33.210  Top 5-err 9.930	 Test Loss 1.219
* Epoch: [31/60]	 Top 1-err 33.210  Top 5-err 9.930	 Test Loss 1.219
66.79
0.6679
loss: 1.2187765350341797
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [32/60]	 Top 1-err 45.984  Top 5-err 21.026	 Train Loss 2.225
* Epoch: [32/60]	 Top 1-err 33.120  Top 5-err 9.770	 Test Loss 1.217
* Epoch: [32/60]	 Top 1-err 33.120  Top 5-err 9.770	 Test Loss 1.217
66.88
0.6688
loss: 1.2173673364639281
(35, 0, 98) triplet: 0.165
(35, 0): 0.0
(35, 98): 0.165
not enough sample
* Epoch: [33/60]	 Top 1-err 49.133  Top 5-err 23.215	 Train Loss 2.266
* Epoch: [33/60]	 Top 1-err 32.980  Top 5-err 9.890	 Test Loss 1.221
* Epoch: [33/60]	 Top 1-err 32.980  Top 5-err 9.890	 Test Loss 1.221
67.02
0.6702
loss: 1.2214890739440918
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [34/60]	 Top 1-err 46.046  Top 5-err 20.867	 Train Loss 2.193
* Epoch: [34/60]	 Top 1-err 33.010  Top 5-err 9.790	 Test Loss 1.222
* Epoch: [34/60]	 Top 1-err 33.010  Top 5-err 9.790	 Test Loss 1.222
66.99
0.6699
loss: 1.2220646480560302
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [35/60]	 Top 1-err 48.750  Top 5-err 23.449	 Train Loss 2.260
* Epoch: [35/60]	 Top 1-err 32.510  Top 5-err 10.000	 Test Loss 1.201
* Epoch: [35/60]	 Top 1-err 32.510  Top 5-err 10.000	 Test Loss 1.201
67.49
0.6749
loss: 1.2012031648635864
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [36/60]	 Top 1-err 45.523  Top 5-err 20.697	 Train Loss 2.172
* Epoch: [36/60]	 Top 1-err 33.130  Top 5-err 9.870	 Test Loss 1.232
* Epoch: [36/60]	 Top 1-err 33.130  Top 5-err 9.870	 Test Loss 1.232
66.87
0.6687
loss: 1.2319754098892213
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.0
(35, 98): 0.16999999999999998
not enough sample
* Epoch: [37/60]	 Top 1-err 44.603  Top 5-err 20.113	 Train Loss 2.148
* Epoch: [37/60]	 Top 1-err 32.340  Top 5-err 9.600	 Test Loss 1.178
* Epoch: [37/60]	 Top 1-err 32.340  Top 5-err 9.600	 Test Loss 1.178
67.66
0.6766
loss: 1.177995160484314
(35, 0, 98) triplet: 0.18
(35, 0): 0.0
(35, 98): 0.18
* Epoch: [38/60]	 Top 1-err 46.766  Top 5-err 21.581	 Train Loss 2.203
* Epoch: [38/60]	 Top 1-err 32.630  Top 5-err 9.600	 Test Loss 1.199
* Epoch: [38/60]	 Top 1-err 32.630  Top 5-err 9.600	 Test Loss 1.199
67.37
0.6737
loss: 1.1988331043243408
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [39/60]	 Top 1-err 48.950  Top 5-err 22.803	 Train Loss 2.261
* Epoch: [39/60]	 Top 1-err 32.920  Top 5-err 9.750	 Test Loss 1.209
* Epoch: [39/60]	 Top 1-err 32.920  Top 5-err 9.750	 Test Loss 1.209
67.08
0.6708
loss: 1.2094270338058473
(35, 0, 98) triplet: 0.145
(35, 0): 0.0
(35, 98): 0.145
not enough sample
* Epoch: [40/60]	 Top 1-err 47.218  Top 5-err 22.121	 Train Loss 2.221
* Epoch: [40/60]	 Top 1-err 32.620  Top 5-err 9.740	 Test Loss 1.212
* Epoch: [40/60]	 Top 1-err 32.620  Top 5-err 9.740	 Test Loss 1.212
67.38
0.6738
loss: 1.212259635734558
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [41/60]	 Top 1-err 46.184  Top 5-err 21.249	 Train Loss 2.200
* Epoch: [41/60]	 Top 1-err 32.150  Top 5-err 9.370	 Test Loss 1.175
* Epoch: [41/60]	 Top 1-err 32.150  Top 5-err 9.370	 Test Loss 1.175
67.85
0.6785
loss: 1.174996248626709
(35, 0, 98) triplet: 0.125
(35, 0): 0.0
(35, 98): 0.125
* Epoch: [42/60]	 Top 1-err 45.757  Top 5-err 21.536	 Train Loss 2.169
* Epoch: [42/60]	 Top 1-err 32.520  Top 5-err 9.510	 Test Loss 1.188
* Epoch: [42/60]	 Top 1-err 32.520  Top 5-err 9.510	 Test Loss 1.188
67.48
0.6748
loss: 1.1877936365127564
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [43/60]	 Top 1-err 45.958  Top 5-err 21.007	 Train Loss 2.165
* Epoch: [43/60]	 Top 1-err 32.250  Top 5-err 9.500	 Test Loss 1.183
* Epoch: [43/60]	 Top 1-err 32.250  Top 5-err 9.500	 Test Loss 1.183
67.75
0.6775
loss: 1.1827306602478027
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.0
(35, 98): 0.16499999999999998
* Epoch: [44/60]	 Top 1-err 47.236  Top 5-err 22.361	 Train Loss 2.173
* Epoch: [44/60]	 Top 1-err 31.940  Top 5-err 9.550	 Test Loss 1.169
* Epoch: [44/60]	 Top 1-err 31.940  Top 5-err 9.550	 Test Loss 1.169
68.06
0.6806
loss: 1.1694670402526854
(35, 0, 98) triplet: 0.175
(35, 0): 0.0
(35, 98): 0.175
not enough sample
* Epoch: [45/60]	 Top 1-err 44.156  Top 5-err 19.673	 Train Loss 2.141
* Epoch: [45/60]	 Top 1-err 32.010  Top 5-err 9.560	 Test Loss 1.192
Current best accuracy (top-1 and 5 error): 32.01 9.56
saving best model...
* Epoch: [45/60]	 Top 1-err 32.010  Top 5-err 9.560	 Test Loss 1.192
67.99
0.6799
loss: 1.192143363571167
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [46/60]	 Top 1-err 45.557  Top 5-err 21.220	 Train Loss 2.116
* Epoch: [46/60]	 Top 1-err 31.890  Top 5-err 9.530	 Test Loss 1.178
Current best accuracy (top-1 and 5 error): 31.89 9.53
saving best model...
* Epoch: [46/60]	 Top 1-err 31.890  Top 5-err 9.530	 Test Loss 1.178
68.11
0.6811
loss: 1.1781341957092286
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [47/60]	 Top 1-err 48.126  Top 5-err 23.060	 Train Loss 2.200
* Epoch: [47/60]	 Top 1-err 31.850  Top 5-err 9.580	 Test Loss 1.172
Current best accuracy (top-1 and 5 error): 31.85 9.58
saving best model...
* Epoch: [47/60]	 Top 1-err 31.850  Top 5-err 9.580	 Test Loss 1.172
68.15
0.6815
loss: 1.1724819334983825
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [48/60]	 Top 1-err 44.569  Top 5-err 20.130	 Train Loss 2.135
* Epoch: [48/60]	 Top 1-err 32.100  Top 5-err 9.430	 Test Loss 1.178
Current best accuracy (top-1 and 5 error): 31.85 9.58
* Epoch: [48/60]	 Top 1-err 32.100  Top 5-err 9.430	 Test Loss 1.178
67.9
0.679
loss: 1.1776532995223998
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [49/60]	 Top 1-err 45.729  Top 5-err 22.076	 Train Loss 2.103
* Epoch: [49/60]	 Top 1-err 31.900  Top 5-err 9.540	 Test Loss 1.174
Current best accuracy (top-1 and 5 error): 31.85 9.58
* Epoch: [49/60]	 Top 1-err 31.900  Top 5-err 9.540	 Test Loss 1.174
68.1
0.681
loss: 1.1738872396469116
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [50/60]	 Top 1-err 44.894  Top 5-err 20.446	 Train Loss 2.142
* Epoch: [50/60]	 Top 1-err 32.050  Top 5-err 9.400	 Test Loss 1.173
Current best accuracy (top-1 and 5 error): 31.85 9.58
* Epoch: [50/60]	 Top 1-err 32.050  Top 5-err 9.400	 Test Loss 1.173
67.95
0.6795
loss: 1.1727805324554443
(35, 0, 98) triplet: 0.15000000000000002
(35, 0): 0.0
(35, 98): 0.15000000000000002
* Epoch: [51/60]	 Top 1-err 48.504  Top 5-err 23.151	 Train Loss 2.230
* Epoch: [51/60]	 Top 1-err 31.960  Top 5-err 9.440	 Test Loss 1.179
Current best accuracy (top-1 and 5 error): 31.85 9.58
* Epoch: [51/60]	 Top 1-err 31.960  Top 5-err 9.440	 Test Loss 1.179
68.04
0.6804
loss: 1.1788125722885132
(35, 0, 98) triplet: 0.15500000000000003
(35, 0): 0.0
(35, 98): 0.15500000000000003
* Epoch: [52/60]	 Top 1-err 45.478  Top 5-err 20.453	 Train Loss 2.172
* Epoch: [52/60]	 Top 1-err 31.860  Top 5-err 9.350	 Test Loss 1.172
Current best accuracy (top-1 and 5 error): 31.85 9.58
* Epoch: [52/60]	 Top 1-err 31.860  Top 5-err 9.350	 Test Loss 1.172
68.14
0.6814
loss: 1.1716713562011718
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [53/60]	 Top 1-err 42.752  Top 5-err 18.438	 Train Loss 2.126
* Epoch: [53/60]	 Top 1-err 31.870  Top 5-err 9.400	 Test Loss 1.175
Current best accuracy (top-1 and 5 error): 31.85 9.58
* Epoch: [53/60]	 Top 1-err 31.870  Top 5-err 9.400	 Test Loss 1.175
68.13
0.6813
loss: 1.1748318373680116
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
not enough sample
* Epoch: [54/60]	 Top 1-err 45.200  Top 5-err 20.412	 Train Loss 2.170
* Epoch: [54/60]	 Top 1-err 32.260  Top 5-err 9.620	 Test Loss 1.204
Current best accuracy (top-1 and 5 error): 31.85 9.58
* Epoch: [54/60]	 Top 1-err 32.260  Top 5-err 9.620	 Test Loss 1.204
67.74
0.6774
loss: 1.204358427810669
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [55/60]	 Top 1-err 45.047  Top 5-err 20.686	 Train Loss 2.125
* Epoch: [55/60]	 Top 1-err 31.680  Top 5-err 9.210	 Test Loss 1.160
Current best accuracy (top-1 and 5 error): 31.68 9.21
saving best model...
* Epoch: [55/60]	 Top 1-err 31.680  Top 5-err 9.210	 Test Loss 1.160
68.32
0.6832
loss: 1.160138384437561
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
not enough sample
* Epoch: [56/60]	 Top 1-err 44.626  Top 5-err 19.541	 Train Loss 2.156
* Epoch: [56/60]	 Top 1-err 32.200  Top 5-err 9.490	 Test Loss 1.185
Current best accuracy (top-1 and 5 error): 31.68 9.21
* Epoch: [56/60]	 Top 1-err 32.200  Top 5-err 9.490	 Test Loss 1.185
67.8
0.678
loss: 1.1846013782501221
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
* Epoch: [57/60]	 Top 1-err 45.907  Top 5-err 20.856	 Train Loss 2.179
* Epoch: [57/60]	 Top 1-err 31.740  Top 5-err 9.360	 Test Loss 1.160
Current best accuracy (top-1 and 5 error): 31.68 9.21
* Epoch: [57/60]	 Top 1-err 31.740  Top 5-err 9.360	 Test Loss 1.160
68.26
0.6826
loss: 1.1604176873207093
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
* Epoch: [58/60]	 Top 1-err 45.074  Top 5-err 20.293	 Train Loss 2.102
* Epoch: [58/60]	 Top 1-err 31.820  Top 5-err 9.340	 Test Loss 1.154
Current best accuracy (top-1 and 5 error): 31.68 9.21
* Epoch: [58/60]	 Top 1-err 31.820  Top 5-err 9.340	 Test Loss 1.154
68.18
0.6818
loss: 1.1544273475646973
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
* Epoch: [59/60]	 Top 1-err 43.646  Top 5-err 19.430	 Train Loss 2.099
* Epoch: [59/60]	 Top 1-err 31.920  Top 5-err 9.350	 Test Loss 1.173
Current best accuracy (top-1 and 5 error): 31.68 9.21
* Epoch: [59/60]	 Top 1-err 31.920  Top 5-err 9.350	 Test Loss 1.173
68.08
0.6808
loss: 1.17345032787323
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
Best accuracy (top-1 and 5 error): 31.68 9.21
=> loading checkpoint 'runs/cifar100_resnet_1/model_best.pth.tar'
* Epoch: [-1/60]	 Top 1-err 31.680  Top 5-err 9.210	 Test Loss 1.160
68.32
0.6832
loss: 1.1601383861541747
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
python3 cifar100_repair_bias.py --net_type resnet --dataset cifar100 --depth 50 --batch_size 256 --lr 0.1 --expname cifar100_resnet_1 --epochs 60 --beta 1.0 --cutmix_prob 1 --pretrained ./runs/cifar100_resnet_1/model_best.pth.tar --expid 0 --first 35 --second 0 --third 98 --lam 0.5
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
=> creating model 'resnet'
True
=> loading checkpoint './runs/cifar100_resnet_1/model_best.pth.tar'
=> loaded checkpoint './runs/cifar100_resnet_1/model_best.pth.tar'
the number of model parameters: 521716
* Epoch: [0/60]	 Top 1-err 53.757  Top 5-err 26.301	 Train Loss 2.471
* Epoch: [0/60]	 Top 1-err 41.290  Top 5-err 15.230	 Test Loss 1.591
* Epoch: [0/60]	 Top 1-err 41.290  Top 5-err 15.230	 Test Loss 1.591
58.71
0.5871
loss: 1.5912355518341064
(35, 0, 98) triplet: 0.27499999999999997
(35, 0): 0.02
(35, 98): 0.295
* Epoch: [1/60]	 Top 1-err 56.604  Top 5-err 29.890	 Train Loss 2.535
* Epoch: [1/60]	 Top 1-err 43.260  Top 5-err 15.840	 Test Loss 1.645
* Epoch: [1/60]	 Top 1-err 43.260  Top 5-err 15.840	 Test Loss 1.645
56.74
0.5674
loss: 1.6448114196777344
(35, 0, 98) triplet: 0.185
(35, 0): 0.07
(35, 98): 0.255
* Epoch: [2/60]	 Top 1-err 54.156  Top 5-err 27.165	 Train Loss 2.473
* Epoch: [2/60]	 Top 1-err 41.830  Top 5-err 15.380	 Test Loss 1.578
* Epoch: [2/60]	 Top 1-err 41.830  Top 5-err 15.380	 Test Loss 1.578
58.17
0.5817
loss: 1.5783564613342285
(35, 0, 98) triplet: 0.20500000000000002
(35, 0): 0.025
(35, 98): 0.23
* Epoch: [3/60]	 Top 1-err 54.983  Top 5-err 27.969	 Train Loss 2.491
* Epoch: [3/60]	 Top 1-err 43.660  Top 5-err 16.420	 Test Loss 1.678
* Epoch: [3/60]	 Top 1-err 43.660  Top 5-err 16.420	 Test Loss 1.678
56.34
0.5634
loss: 1.6779402793884277
(35, 0, 98) triplet: 0.16
(35, 0): 0.01
(35, 98): 0.17
not enough sample
not enough sample
* Epoch: [4/60]	 Top 1-err 56.213  Top 5-err 28.449	 Train Loss 2.554
* Epoch: [4/60]	 Top 1-err 43.280  Top 5-err 15.870	 Test Loss 1.674
* Epoch: [4/60]	 Top 1-err 43.280  Top 5-err 15.870	 Test Loss 1.674
56.72
0.5672
loss: 1.6737875999450684
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.01
(35, 98): 0.175
* Epoch: [5/60]	 Top 1-err 52.184  Top 5-err 25.025	 Train Loss 2.440
* Epoch: [5/60]	 Top 1-err 43.130  Top 5-err 15.550	 Test Loss 1.629
* Epoch: [5/60]	 Top 1-err 43.130  Top 5-err 15.550	 Test Loss 1.629
56.87
0.5687
loss: 1.629313177871704
(35, 0, 98) triplet: 0.22999999999999998
(35, 0): 0.01
(35, 98): 0.24
* Epoch: [6/60]	 Top 1-err 53.432  Top 5-err 26.109	 Train Loss 2.474
* Epoch: [6/60]	 Top 1-err 42.370  Top 5-err 15.470	 Test Loss 1.612
* Epoch: [6/60]	 Top 1-err 42.370  Top 5-err 15.470	 Test Loss 1.612
57.63
0.5763
loss: 1.6116689737319947
(35, 0, 98) triplet: 0.195
(35, 0): 0.01
(35, 98): 0.20500000000000002
* Epoch: [7/60]	 Top 1-err 52.626  Top 5-err 26.105	 Train Loss 2.395
* Epoch: [7/60]	 Top 1-err 39.820  Top 5-err 13.410	 Test Loss 1.482
* Epoch: [7/60]	 Top 1-err 39.820  Top 5-err 13.410	 Test Loss 1.482
60.18
0.6018
loss: 1.4815876167297364
(35, 0, 98) triplet: 0.18
(35, 0): 0.01
(35, 98): 0.19
* Epoch: [8/60]	 Top 1-err 53.797  Top 5-err 26.874	 Train Loss 2.421
* Epoch: [8/60]	 Top 1-err 39.810  Top 5-err 13.570	 Test Loss 1.471
* Epoch: [8/60]	 Top 1-err 39.810  Top 5-err 13.570	 Test Loss 1.471
60.19
0.6019
loss: 1.471050354385376
(35, 0, 98) triplet: 0.175
(35, 0): 0.005
(35, 98): 0.18
* Epoch: [9/60]	 Top 1-err 54.350  Top 5-err 27.425	 Train Loss 2.473
* Epoch: [9/60]	 Top 1-err 43.900  Top 5-err 16.410	 Test Loss 1.648
* Epoch: [9/60]	 Top 1-err 43.900  Top 5-err 16.410	 Test Loss 1.648
56.1
0.561
loss: 1.6476219972610473
(35, 0, 98) triplet: 0.325
(35, 0): 0.005
(35, 98): 0.33
* Epoch: [10/60]	 Top 1-err 54.025  Top 5-err 26.930	 Train Loss 2.475
* Epoch: [10/60]	 Top 1-err 41.670  Top 5-err 14.560	 Test Loss 1.561
* Epoch: [10/60]	 Top 1-err 41.670  Top 5-err 14.560	 Test Loss 1.561
58.33
0.5833
loss: 1.561191102218628
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [11/60]	 Top 1-err 54.620  Top 5-err 26.966	 Train Loss 2.521
* Epoch: [11/60]	 Top 1-err 42.900  Top 5-err 15.790	 Test Loss 1.618
* Epoch: [11/60]	 Top 1-err 42.900  Top 5-err 15.790	 Test Loss 1.618
57.1
0.571
loss: 1.6179080154418946
(35, 0, 98) triplet: 0.13
(35, 0): 0.015
(35, 98): 0.14500000000000002
not enough sample
* Epoch: [12/60]	 Top 1-err 55.440  Top 5-err 28.413	 Train Loss 2.516
* Epoch: [12/60]	 Top 1-err 42.190  Top 5-err 15.290	 Test Loss 1.586
* Epoch: [12/60]	 Top 1-err 42.190  Top 5-err 15.290	 Test Loss 1.586
57.81
0.5781
loss: 1.5857501153945923
(35, 0, 98) triplet: 0.19
(35, 0): 0.0
(35, 98): 0.19
* Epoch: [13/60]	 Top 1-err 54.001  Top 5-err 26.704	 Train Loss 2.483
* Epoch: [13/60]	 Top 1-err 39.580  Top 5-err 13.880	 Test Loss 1.485
* Epoch: [13/60]	 Top 1-err 39.580  Top 5-err 13.880	 Test Loss 1.485
60.42
0.6042
loss: 1.484833800315857
(35, 0, 98) triplet: 0.24
(35, 0): 0.0
(35, 98): 0.24
* Epoch: [14/60]	 Top 1-err 54.307  Top 5-err 27.031	 Train Loss 2.499
* Epoch: [14/60]	 Top 1-err 41.020  Top 5-err 14.350	 Test Loss 1.525
* Epoch: [14/60]	 Top 1-err 41.020  Top 5-err 14.350	 Test Loss 1.525
58.98
0.5898
loss: 1.5251325187683105
(35, 0, 98) triplet: 0.23
(35, 0): 0.0
(35, 98): 0.23
* Epoch: [15/60]	 Top 1-err 57.342  Top 5-err 29.709	 Train Loss 2.539
* Epoch: [15/60]	 Top 1-err 40.770  Top 5-err 14.280	 Test Loss 1.548
* Epoch: [15/60]	 Top 1-err 40.770  Top 5-err 14.280	 Test Loss 1.548
59.23
0.5923
loss: 1.5483733011245728
(35, 0, 98) triplet: 0.245
(35, 0): 0.0
(35, 98): 0.245
* Epoch: [16/60]	 Top 1-err 53.334  Top 5-err 25.733	 Train Loss 2.480
* Epoch: [16/60]	 Top 1-err 43.570  Top 5-err 16.620	 Test Loss 1.682
* Epoch: [16/60]	 Top 1-err 43.570  Top 5-err 16.620	 Test Loss 1.682
56.43
0.5643
loss: 1.68155223903656
(35, 0, 98) triplet: 0.17
(35, 0): 0.02
(35, 98): 0.19
not enough sample
* Epoch: [17/60]	 Top 1-err 54.620  Top 5-err 27.363	 Train Loss 2.450
* Epoch: [17/60]	 Top 1-err 44.040  Top 5-err 17.160	 Test Loss 1.687
* Epoch: [17/60]	 Top 1-err 44.040  Top 5-err 17.160	 Test Loss 1.687
55.96
0.5596
loss: 1.6872513912200928
(35, 0, 98) triplet: 0.21500000000000002
(35, 0): 0.05
(35, 98): 0.265
* Epoch: [18/60]	 Top 1-err 55.032  Top 5-err 27.524	 Train Loss 2.482
* Epoch: [18/60]	 Top 1-err 40.360  Top 5-err 14.150	 Test Loss 1.549
* Epoch: [18/60]	 Top 1-err 40.360  Top 5-err 14.150	 Test Loss 1.549
59.64
0.5964
loss: 1.5488170728683472
(35, 0, 98) triplet: 0.2
(35, 0): 0.0
(35, 98): 0.2
* Epoch: [19/60]	 Top 1-err 54.363  Top 5-err 27.369	 Train Loss 2.425
* Epoch: [19/60]	 Top 1-err 43.770  Top 5-err 16.430	 Test Loss 1.715
* Epoch: [19/60]	 Top 1-err 43.770  Top 5-err 16.430	 Test Loss 1.715
56.23
0.5623
loss: 1.715482493209839
(35, 0, 98) triplet: 0.305
(35, 0): 0.0
(35, 98): 0.305
* Epoch: [20/60]	 Top 1-err 51.296  Top 5-err 24.163	 Train Loss 2.376
* Epoch: [20/60]	 Top 1-err 40.110  Top 5-err 13.950	 Test Loss 1.510
* Epoch: [20/60]	 Top 1-err 40.110  Top 5-err 13.950	 Test Loss 1.510
59.89
0.5989
loss: 1.510495856666565
(35, 0, 98) triplet: 0.16
(35, 0): 0.055
(35, 98): 0.215
* Epoch: [21/60]	 Top 1-err 54.841  Top 5-err 27.935	 Train Loss 2.462
* Epoch: [21/60]	 Top 1-err 39.950  Top 5-err 14.110	 Test Loss 1.508
* Epoch: [21/60]	 Top 1-err 39.950  Top 5-err 14.110	 Test Loss 1.508
60.05
0.6005
loss: 1.507791240310669
(35, 0, 98) triplet: 0.175
(35, 0): 0.0
(35, 98): 0.175
not enough sample
* Epoch: [22/60]	 Top 1-err 50.699  Top 5-err 23.462	 Train Loss 2.410
* Epoch: [22/60]	 Top 1-err 40.910  Top 5-err 14.070	 Test Loss 1.532
* Epoch: [22/60]	 Top 1-err 40.910  Top 5-err 14.070	 Test Loss 1.532
59.09
0.5909
loss: 1.532263452911377
(35, 0, 98) triplet: 0.27499999999999997
(35, 0): 0.005
(35, 98): 0.27999999999999997
* Epoch: [23/60]	 Top 1-err 53.232  Top 5-err 26.156	 Train Loss 2.429
* Epoch: [23/60]	 Top 1-err 41.670  Top 5-err 14.990	 Test Loss 1.586
* Epoch: [23/60]	 Top 1-err 41.670  Top 5-err 14.990	 Test Loss 1.586
58.33
0.5833
loss: 1.5861956233978272
(35, 0, 98) triplet: 0.165
(35, 0): 0.02
(35, 98): 0.185
* Epoch: [24/60]	 Top 1-err 53.366  Top 5-err 26.135	 Train Loss 2.444
* Epoch: [24/60]	 Top 1-err 40.550  Top 5-err 14.000	 Test Loss 1.520
* Epoch: [24/60]	 Top 1-err 40.550  Top 5-err 14.000	 Test Loss 1.520
59.45
0.5945
loss: 1.519671978378296
(35, 0, 98) triplet: 0.18
(35, 0): 0.01
(35, 98): 0.19
* Epoch: [25/60]	 Top 1-err 52.807  Top 5-err 25.178	 Train Loss 2.453
* Epoch: [25/60]	 Top 1-err 38.680  Top 5-err 12.870	 Test Loss 1.445
* Epoch: [25/60]	 Top 1-err 38.680  Top 5-err 12.870	 Test Loss 1.445
61.32
0.6132
loss: 1.445117246246338
(35, 0, 98) triplet: 0.22
(35, 0): 0.025
(35, 98): 0.245
* Epoch: [26/60]	 Top 1-err 52.531  Top 5-err 24.654	 Train Loss 2.453
* Epoch: [26/60]	 Top 1-err 40.640  Top 5-err 14.360	 Test Loss 1.544
* Epoch: [26/60]	 Top 1-err 40.640  Top 5-err 14.360	 Test Loss 1.544
59.36
0.5936
loss: 1.544288064956665
(35, 0, 98) triplet: 0.31999999999999995
(35, 0): 0.01
(35, 98): 0.32999999999999996
* Epoch: [27/60]	 Top 1-err 52.805  Top 5-err 25.408	 Train Loss 2.455
* Epoch: [27/60]	 Top 1-err 38.710  Top 5-err 13.190	 Test Loss 1.455
* Epoch: [27/60]	 Top 1-err 38.710  Top 5-err 13.190	 Test Loss 1.455
61.29
0.6129
loss: 1.4552815717697143
(35, 0, 98) triplet: 0.1
(35, 0): 0.025
(35, 98): 0.125
* Epoch: [28/60]	 Top 1-err 53.795  Top 5-err 27.176	 Train Loss 2.459
* Epoch: [28/60]	 Top 1-err 42.670  Top 5-err 15.620	 Test Loss 1.627
* Epoch: [28/60]	 Top 1-err 42.670  Top 5-err 15.620	 Test Loss 1.627
57.33
0.5733
loss: 1.6270774299621582
(35, 0, 98) triplet: 0.07500000000000001
(35, 0): 0.11499999999999999
(35, 98): 0.19
* Epoch: [29/60]	 Top 1-err 53.036  Top 5-err 25.572	 Train Loss 2.445
* Epoch: [29/60]	 Top 1-err 39.780  Top 5-err 13.400	 Test Loss 1.472
* Epoch: [29/60]	 Top 1-err 39.780  Top 5-err 13.400	 Test Loss 1.472
60.22
0.6022
loss: 1.4724996074676513
(35, 0, 98) triplet: 0.105
(35, 0): 0.01
(35, 98): 0.11499999999999999
* Epoch: [30/60]	 Top 1-err 49.741  Top 5-err 23.389	 Train Loss 2.310
* Epoch: [30/60]	 Top 1-err 32.530  Top 5-err 9.520	 Test Loss 1.204
* Epoch: [30/60]	 Top 1-err 32.530  Top 5-err 9.520	 Test Loss 1.204
67.47
0.6747
loss: 1.2043777541160583
(35, 0, 98) triplet: 0.18
(35, 0): 0.0
(35, 98): 0.18
* Epoch: [31/60]	 Top 1-err 48.149  Top 5-err 22.920	 Train Loss 2.223
* Epoch: [31/60]	 Top 1-err 31.880  Top 5-err 9.530	 Test Loss 1.206
* Epoch: [31/60]	 Top 1-err 31.880  Top 5-err 9.530	 Test Loss 1.206
68.12
0.6812
loss: 1.20570844039917
(35, 0, 98) triplet: 0.18000000000000002
(35, 0): 0.0
(35, 98): 0.18000000000000002
* Epoch: [32/60]	 Top 1-err 45.240  Top 5-err 20.170	 Train Loss 2.226
* Epoch: [32/60]	 Top 1-err 32.020  Top 5-err 9.370	 Test Loss 1.213
* Epoch: [32/60]	 Top 1-err 32.020  Top 5-err 9.370	 Test Loss 1.213
67.98
0.6798
loss: 1.2133345649719238
(35, 0, 98) triplet: 0.195
(35, 0): 0.0
(35, 98): 0.195
not enough sample
* Epoch: [33/60]	 Top 1-err 48.194  Top 5-err 22.046	 Train Loss 2.257
* Epoch: [33/60]	 Top 1-err 32.250  Top 5-err 9.320	 Test Loss 1.211
* Epoch: [33/60]	 Top 1-err 32.250  Top 5-err 9.320	 Test Loss 1.211
67.75
0.6775
loss: 1.2113362163543702
(35, 0, 98) triplet: 0.19
(35, 0): 0.0
(35, 98): 0.19
* Epoch: [34/60]	 Top 1-err 45.166  Top 5-err 20.042	 Train Loss 2.184
* Epoch: [34/60]	 Top 1-err 31.970  Top 5-err 9.270	 Test Loss 1.207
* Epoch: [34/60]	 Top 1-err 31.970  Top 5-err 9.270	 Test Loss 1.207
68.03
0.6803
loss: 1.2068987466812133
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.005
(35, 98): 0.175
* Epoch: [35/60]	 Top 1-err 47.811  Top 5-err 22.371	 Train Loss 2.250
* Epoch: [35/60]	 Top 1-err 31.550  Top 5-err 9.250	 Test Loss 1.186
* Epoch: [35/60]	 Top 1-err 31.550  Top 5-err 9.250	 Test Loss 1.186
68.45
0.6845
loss: 1.1859087348937989
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
* Epoch: [36/60]	 Top 1-err 44.407  Top 5-err 19.543	 Train Loss 2.161
* Epoch: [36/60]	 Top 1-err 31.880  Top 5-err 9.200	 Test Loss 1.219
* Epoch: [36/60]	 Top 1-err 31.880  Top 5-err 9.200	 Test Loss 1.219
68.12
0.6812
loss: 1.2193235362052917
(35, 0, 98) triplet: 0.19999999999999998
(35, 0): 0.0
(35, 98): 0.19999999999999998
not enough sample
* Epoch: [37/60]	 Top 1-err 43.744  Top 5-err 18.997	 Train Loss 2.141
* Epoch: [37/60]	 Top 1-err 31.300  Top 5-err 8.840	 Test Loss 1.166
* Epoch: [37/60]	 Top 1-err 31.300  Top 5-err 8.840	 Test Loss 1.166
68.7
0.687
loss: 1.165783596611023
(35, 0, 98) triplet: 0.165
(35, 0): 0.0
(35, 98): 0.165
* Epoch: [38/60]	 Top 1-err 45.693  Top 5-err 20.527	 Train Loss 2.193
* Epoch: [38/60]	 Top 1-err 31.120  Top 5-err 8.990	 Test Loss 1.179
* Epoch: [38/60]	 Top 1-err 31.120  Top 5-err 8.990	 Test Loss 1.179
68.88
0.6888
loss: 1.1794260593414307
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [39/60]	 Top 1-err 47.618  Top 5-err 21.643	 Train Loss 2.244
* Epoch: [39/60]	 Top 1-err 31.370  Top 5-err 9.120	 Test Loss 1.186
* Epoch: [39/60]	 Top 1-err 31.370  Top 5-err 9.120	 Test Loss 1.186
68.63
0.6863
loss: 1.186412919807434
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.0
(35, 98): 0.16499999999999998
not enough sample
* Epoch: [40/60]	 Top 1-err 46.111  Top 5-err 21.012	 Train Loss 2.203
* Epoch: [40/60]	 Top 1-err 31.370  Top 5-err 9.180	 Test Loss 1.188
* Epoch: [40/60]	 Top 1-err 31.370  Top 5-err 9.180	 Test Loss 1.188
68.63
0.6863
loss: 1.1884214547157288
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [41/60]	 Top 1-err 45.308  Top 5-err 19.972	 Train Loss 2.185
* Epoch: [41/60]	 Top 1-err 30.920  Top 5-err 9.050	 Test Loss 1.162
* Epoch: [41/60]	 Top 1-err 30.920  Top 5-err 9.050	 Test Loss 1.162
69.08
0.6908
loss: 1.1624524604797364
(35, 0, 98) triplet: 0.13
(35, 0): 0.0
(35, 98): 0.13
* Epoch: [42/60]	 Top 1-err 44.766  Top 5-err 20.665	 Train Loss 2.155
* Epoch: [42/60]	 Top 1-err 30.880  Top 5-err 8.930	 Test Loss 1.164
* Epoch: [42/60]	 Top 1-err 30.880  Top 5-err 8.930	 Test Loss 1.164
69.12
0.6912
loss: 1.1642948871612548
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
* Epoch: [43/60]	 Top 1-err 44.824  Top 5-err 19.751	 Train Loss 2.150
* Epoch: [43/60]	 Top 1-err 31.170  Top 5-err 9.100	 Test Loss 1.174
* Epoch: [43/60]	 Top 1-err 31.170  Top 5-err 9.100	 Test Loss 1.174
68.83
0.6883
loss: 1.174289573287964
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.005
(35, 98): 0.16999999999999998
* Epoch: [44/60]	 Top 1-err 46.275  Top 5-err 21.345	 Train Loss 2.160
* Epoch: [44/60]	 Top 1-err 31.030  Top 5-err 8.880	 Test Loss 1.164
* Epoch: [44/60]	 Top 1-err 31.030  Top 5-err 8.880	 Test Loss 1.164
68.97
0.6897
loss: 1.1642643436431885
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
not enough sample
* Epoch: [45/60]	 Top 1-err 43.338  Top 5-err 18.889	 Train Loss 2.125
* Epoch: [45/60]	 Top 1-err 30.990  Top 5-err 8.940	 Test Loss 1.178
Current best accuracy (top-1 and 5 error): 30.99 8.94
saving best model...
* Epoch: [45/60]	 Top 1-err 30.990  Top 5-err 8.940	 Test Loss 1.178
69.01
0.6901
loss: 1.1778099267959594
(35, 0, 98) triplet: 0.165
(35, 0): 0.0
(35, 98): 0.165
* Epoch: [46/60]	 Top 1-err 44.702  Top 5-err 20.159	 Train Loss 2.097
* Epoch: [46/60]	 Top 1-err 30.660  Top 5-err 8.790	 Test Loss 1.161
Current best accuracy (top-1 and 5 error): 30.66 8.79
saving best model...
* Epoch: [46/60]	 Top 1-err 30.660  Top 5-err 8.790	 Test Loss 1.161
69.34
0.6934
loss: 1.161481422996521
(35, 0, 98) triplet: 0.15
(35, 0): 0.0
(35, 98): 0.15
* Epoch: [47/60]	 Top 1-err 47.104  Top 5-err 21.708	 Train Loss 2.181
* Epoch: [47/60]	 Top 1-err 30.710  Top 5-err 8.670	 Test Loss 1.155
Current best accuracy (top-1 and 5 error): 30.66 8.79
* Epoch: [47/60]	 Top 1-err 30.710  Top 5-err 8.670	 Test Loss 1.155
69.29
0.6929
loss: 1.1547833267688752
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [48/60]	 Top 1-err 43.515  Top 5-err 18.793	 Train Loss 2.116
* Epoch: [48/60]	 Top 1-err 30.720  Top 5-err 8.830	 Test Loss 1.160
Current best accuracy (top-1 and 5 error): 30.66 8.79
* Epoch: [48/60]	 Top 1-err 30.720  Top 5-err 8.830	 Test Loss 1.160
69.28
0.6928
loss: 1.1602345893859862
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [49/60]	 Top 1-err 44.866  Top 5-err 21.086	 Train Loss 2.082
* Epoch: [49/60]	 Top 1-err 30.470  Top 5-err 8.650	 Test Loss 1.155
Current best accuracy (top-1 and 5 error): 30.47 8.65
saving best model...
* Epoch: [49/60]	 Top 1-err 30.470  Top 5-err 8.650	 Test Loss 1.155
69.53
0.6953
loss: 1.1553238914489745
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [50/60]	 Top 1-err 43.929  Top 5-err 19.271	 Train Loss 2.121
* Epoch: [50/60]	 Top 1-err 30.570  Top 5-err 8.790	 Test Loss 1.154
Current best accuracy (top-1 and 5 error): 30.47 8.65
* Epoch: [50/60]	 Top 1-err 30.570  Top 5-err 8.790	 Test Loss 1.154
69.43
0.6943
loss: 1.1541456231117249
(35, 0, 98) triplet: 0.13
(35, 0): 0.0
(35, 98): 0.13
* Epoch: [51/60]	 Top 1-err 47.308  Top 5-err 21.632	 Train Loss 2.204
* Epoch: [51/60]	 Top 1-err 30.720  Top 5-err 8.620	 Test Loss 1.160
Current best accuracy (top-1 and 5 error): 30.47 8.65
* Epoch: [51/60]	 Top 1-err 30.720  Top 5-err 8.620	 Test Loss 1.160
69.28
0.6928
loss: 1.160411203289032
(35, 0, 98) triplet: 0.155
(35, 0): 0.0
(35, 98): 0.155
* Epoch: [52/60]	 Top 1-err 44.447  Top 5-err 19.284	 Train Loss 2.148
* Epoch: [52/60]	 Top 1-err 30.360  Top 5-err 8.610	 Test Loss 1.158
Current best accuracy (top-1 and 5 error): 30.36 8.61
saving best model...
* Epoch: [52/60]	 Top 1-err 30.360  Top 5-err 8.610	 Test Loss 1.158
69.64
0.6964
loss: 1.1580608572006226
(35, 0, 98) triplet: 0.13
(35, 0): 0.0
(35, 98): 0.13
* Epoch: [53/60]	 Top 1-err 41.713  Top 5-err 17.044	 Train Loss 2.106
* Epoch: [53/60]	 Top 1-err 30.480  Top 5-err 8.850	 Test Loss 1.157
Current best accuracy (top-1 and 5 error): 30.36 8.61
* Epoch: [53/60]	 Top 1-err 30.480  Top 5-err 8.850	 Test Loss 1.157
69.52
0.6952
loss: 1.1566090232849122
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
not enough sample
* Epoch: [54/60]	 Top 1-err 44.052  Top 5-err 19.333	 Train Loss 2.148
* Epoch: [54/60]	 Top 1-err 31.110  Top 5-err 8.920	 Test Loss 1.195
Current best accuracy (top-1 and 5 error): 30.36 8.61
* Epoch: [54/60]	 Top 1-err 31.110  Top 5-err 8.920	 Test Loss 1.195
68.89
0.6889
loss: 1.194870132637024
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.0
(35, 98): 0.14500000000000002
* Epoch: [55/60]	 Top 1-err 43.921  Top 5-err 19.465	 Train Loss 2.102
* Epoch: [55/60]	 Top 1-err 30.480  Top 5-err 8.660	 Test Loss 1.142
Current best accuracy (top-1 and 5 error): 30.36 8.61
* Epoch: [55/60]	 Top 1-err 30.480  Top 5-err 8.660	 Test Loss 1.142
69.52
0.6952
loss: 1.1422557804107667
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
not enough sample
* Epoch: [56/60]	 Top 1-err 43.366  Top 5-err 18.296	 Train Loss 2.134
* Epoch: [56/60]	 Top 1-err 30.840  Top 5-err 8.720	 Test Loss 1.168
Current best accuracy (top-1 and 5 error): 30.36 8.61
* Epoch: [56/60]	 Top 1-err 30.840  Top 5-err 8.720	 Test Loss 1.168
69.16
0.6916
loss: 1.1676544651031495
(35, 0, 98) triplet: 0.13
(35, 0): 0.0
(35, 98): 0.13
* Epoch: [57/60]	 Top 1-err 44.921  Top 5-err 19.647	 Train Loss 2.155
* Epoch: [57/60]	 Top 1-err 30.590  Top 5-err 8.700	 Test Loss 1.142
Current best accuracy (top-1 and 5 error): 30.36 8.61
* Epoch: [57/60]	 Top 1-err 30.590  Top 5-err 8.700	 Test Loss 1.142
69.41
0.6941
loss: 1.1417608699798585
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [58/60]	 Top 1-err 43.821  Top 5-err 19.188	 Train Loss 2.084
* Epoch: [58/60]	 Top 1-err 30.380  Top 5-err 8.650	 Test Loss 1.135
Current best accuracy (top-1 and 5 error): 30.36 8.61
* Epoch: [58/60]	 Top 1-err 30.380  Top 5-err 8.650	 Test Loss 1.135
69.62
0.6962
loss: 1.1352988691329957
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [59/60]	 Top 1-err 42.344  Top 5-err 18.221	 Train Loss 2.074
* Epoch: [59/60]	 Top 1-err 30.570  Top 5-err 8.760	 Test Loss 1.155
Current best accuracy (top-1 and 5 error): 30.36 8.61
* Epoch: [59/60]	 Top 1-err 30.570  Top 5-err 8.760	 Test Loss 1.155
69.43
0.6943
loss: 1.1554835668563843
(35, 0, 98) triplet: 0.135
(35, 0): 0.0
(35, 98): 0.135
Best accuracy (top-1 and 5 error): 30.36 8.61
=> loading checkpoint 'runs/cifar100_resnet_1/model_best.pth.tar'
* Epoch: [-1/60]	 Top 1-err 30.360  Top 5-err 8.610	 Test Loss 1.158
69.64
0.6964
loss: 1.1580608724594117
(35, 0, 98) triplet: 0.13
(35, 0): 0.0
(35, 98): 0.13
python3 cifar100_repair_bias.py --net_type resnet --dataset cifar100 --depth 50 --batch_size 256 --lr 0.1 --expname cifar100_resnet_1 --epochs 60 --beta 1.0 --cutmix_prob 1 --pretrained ./runs/cifar100_resnet_1/model_best.pth.tar --expid 0 --first 35 --second 0 --third 98 --lam 1
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
=> creating model 'resnet'
True
=> loading checkpoint './runs/cifar100_resnet_1/model_best.pth.tar'
=> loaded checkpoint './runs/cifar100_resnet_1/model_best.pth.tar'
the number of model parameters: 521716
* Epoch: [0/60]	 Top 1-err 54.031  Top 5-err 26.179	 Train Loss 2.488
* Epoch: [0/60]	 Top 1-err 41.300  Top 5-err 15.070	 Test Loss 1.569
* Epoch: [0/60]	 Top 1-err 41.300  Top 5-err 15.070	 Test Loss 1.569
58.7
0.587
loss: 1.569481374168396
(35, 0, 98) triplet: 0.22999999999999998
(35, 0): 0.005
(35, 98): 0.235
* Epoch: [1/60]	 Top 1-err 57.690  Top 5-err 30.232	 Train Loss 2.575
* Epoch: [1/60]	 Top 1-err 44.150  Top 5-err 17.020	 Test Loss 1.689
* Epoch: [1/60]	 Top 1-err 44.150  Top 5-err 17.020	 Test Loss 1.689
55.85
0.5585
loss: 1.6888537017822265
(35, 0, 98) triplet: 0.13
(35, 0): 0.015
(35, 98): 0.14500000000000002
* Epoch: [2/60]	 Top 1-err 54.858  Top 5-err 27.184	 Train Loss 2.500
* Epoch: [2/60]	 Top 1-err 43.190  Top 5-err 15.900	 Test Loss 1.623
* Epoch: [2/60]	 Top 1-err 43.190  Top 5-err 15.900	 Test Loss 1.623
56.81
0.5681
loss: 1.6230532718658448
(35, 0, 98) triplet: 0.1
(35, 0): 0.025
(35, 98): 0.125
* Epoch: [3/60]	 Top 1-err 55.240  Top 5-err 27.737	 Train Loss 2.512
* Epoch: [3/60]	 Top 1-err 43.150  Top 5-err 15.700	 Test Loss 1.658
* Epoch: [3/60]	 Top 1-err 43.150  Top 5-err 15.700	 Test Loss 1.658
56.85
0.5685
loss: 1.6584179634094238
(35, 0, 98) triplet: 0.075
(35, 0): 0.045
(35, 98): 0.12
not enough sample
not enough sample
* Epoch: [4/60]	 Top 1-err 56.951  Top 5-err 28.466	 Train Loss 2.586
* Epoch: [4/60]	 Top 1-err 45.630  Top 5-err 17.550	 Test Loss 1.790
* Epoch: [4/60]	 Top 1-err 45.630  Top 5-err 17.550	 Test Loss 1.790
54.37
0.5437
loss: 1.7901946838378906
(35, 0, 98) triplet: 0.085
(35, 0): 0.03
(35, 98): 0.115
* Epoch: [5/60]	 Top 1-err 52.195  Top 5-err 24.890	 Train Loss 2.457
* Epoch: [5/60]	 Top 1-err 43.570  Top 5-err 15.800	 Test Loss 1.635
* Epoch: [5/60]	 Top 1-err 43.570  Top 5-err 15.800	 Test Loss 1.635
56.43
0.5643
loss: 1.6345946258544921
(35, 0, 98) triplet: 0.03
(35, 0): 0.18
(35, 98): 0.15
* Epoch: [6/60]	 Top 1-err 53.835  Top 5-err 26.386	 Train Loss 2.503
* Epoch: [6/60]	 Top 1-err 44.790  Top 5-err 17.880	 Test Loss 1.781
* Epoch: [6/60]	 Top 1-err 44.790  Top 5-err 17.880	 Test Loss 1.781
55.21
0.5521
loss: 1.7807251066207885
(35, 0, 98) triplet: 0.19500000000000003
(35, 0): 0.02
(35, 98): 0.21500000000000002
* Epoch: [7/60]	 Top 1-err 53.952  Top 5-err 26.691	 Train Loss 2.443
* Epoch: [7/60]	 Top 1-err 41.720  Top 5-err 14.650	 Test Loss 1.578
* Epoch: [7/60]	 Top 1-err 41.720  Top 5-err 14.650	 Test Loss 1.578
58.28
0.5828
loss: 1.577588159942627
(35, 0, 98) triplet: 0.10499999999999998
(35, 0): 0.04
(35, 98): 0.145
* Epoch: [8/60]	 Top 1-err 54.715  Top 5-err 27.176	 Train Loss 2.474
* Epoch: [8/60]	 Top 1-err 42.810  Top 5-err 16.230	 Test Loss 1.627
* Epoch: [8/60]	 Top 1-err 42.810  Top 5-err 16.230	 Test Loss 1.627
57.19
0.5719
loss: 1.627214716720581
(35, 0, 98) triplet: 0.019999999999999997
(35, 0): 0.01
(35, 98): 0.03
* Epoch: [9/60]	 Top 1-err 55.055  Top 5-err 27.624	 Train Loss 2.509
* Epoch: [9/60]	 Top 1-err 39.830  Top 5-err 13.410	 Test Loss 1.477
* Epoch: [9/60]	 Top 1-err 39.830  Top 5-err 13.410	 Test Loss 1.477
60.17
0.6017
loss: 1.476776950454712
(35, 0, 98) triplet: 0.245
(35, 0): 0.0
(35, 98): 0.245
* Epoch: [10/60]	 Top 1-err 55.444  Top 5-err 27.565	 Train Loss 2.536
* Epoch: [10/60]	 Top 1-err 42.200  Top 5-err 14.460	 Test Loss 1.566
* Epoch: [10/60]	 Top 1-err 42.200  Top 5-err 14.460	 Test Loss 1.566
57.8
0.578
loss: 1.5661421546936034
(35, 0, 98) triplet: 0.1
(35, 0): 0.01
(35, 98): 0.11
* Epoch: [11/60]	 Top 1-err 55.527  Top 5-err 27.510	 Train Loss 2.560
* Epoch: [11/60]	 Top 1-err 44.330  Top 5-err 17.090	 Test Loss 1.703
* Epoch: [11/60]	 Top 1-err 44.330  Top 5-err 17.090	 Test Loss 1.703
55.67
0.5567
loss: 1.7031881221771241
(35, 0, 98) triplet: 0.125
(35, 0): 0.055
(35, 98): 0.18
not enough sample
* Epoch: [12/60]	 Top 1-err 57.346  Top 5-err 29.413	 Train Loss 2.585
* Epoch: [12/60]	 Top 1-err 42.080  Top 5-err 14.070	 Test Loss 1.564
* Epoch: [12/60]	 Top 1-err 42.080  Top 5-err 14.070	 Test Loss 1.564
57.92
0.5792
loss: 1.5642194496154784
(35, 0, 98) triplet: 0.16
(35, 0): 0.005
(35, 98): 0.165
* Epoch: [13/60]	 Top 1-err 54.707  Top 5-err 27.004	 Train Loss 2.515
* Epoch: [13/60]	 Top 1-err 40.830  Top 5-err 14.570	 Test Loss 1.559
* Epoch: [13/60]	 Top 1-err 40.830  Top 5-err 14.570	 Test Loss 1.559
59.17
0.5917
loss: 1.559470753479004
(35, 0, 98) triplet: 0.14
(35, 0): 0.055
(35, 98): 0.195
* Epoch: [14/60]	 Top 1-err 54.586  Top 5-err 27.029	 Train Loss 2.519
* Epoch: [14/60]	 Top 1-err 38.050  Top 5-err 12.430	 Test Loss 1.409
* Epoch: [14/60]	 Top 1-err 38.050  Top 5-err 12.430	 Test Loss 1.409
61.95
0.6195
loss: 1.4090500453948975
(35, 0, 98) triplet: 0.10500000000000001
(35, 0): 0.035
(35, 98): 0.14
* Epoch: [15/60]	 Top 1-err 57.901  Top 5-err 29.849	 Train Loss 2.567
* Epoch: [15/60]	 Top 1-err 41.080  Top 5-err 14.520	 Test Loss 1.575
* Epoch: [15/60]	 Top 1-err 41.080  Top 5-err 14.520	 Test Loss 1.575
58.92
0.5892
loss: 1.5751934707641602
(35, 0, 98) triplet: 0.14499999999999996
(35, 0): 0.075
(35, 98): 0.21999999999999997
* Epoch: [16/60]	 Top 1-err 53.266  Top 5-err 25.546	 Train Loss 2.488
* Epoch: [16/60]	 Top 1-err 40.430  Top 5-err 13.880	 Test Loss 1.527
* Epoch: [16/60]	 Top 1-err 40.430  Top 5-err 13.880	 Test Loss 1.527
59.57
0.5957
loss: 1.5271552974700928
(35, 0, 98) triplet: 0.095
(35, 0): 0.03
(35, 98): 0.125
not enough sample
* Epoch: [17/60]	 Top 1-err 55.402  Top 5-err 27.907	 Train Loss 2.495
* Epoch: [17/60]	 Top 1-err 41.170  Top 5-err 14.890	 Test Loss 1.566
* Epoch: [17/60]	 Top 1-err 41.170  Top 5-err 14.890	 Test Loss 1.566
58.83
0.5883
loss: 1.5660934913635254
(35, 0, 98) triplet: 0.06999999999999999
(35, 0): 0.015
(35, 98): 0.08499999999999999
* Epoch: [18/60]	 Top 1-err 55.808  Top 5-err 27.798	 Train Loss 2.521
* Epoch: [18/60]	 Top 1-err 40.590  Top 5-err 14.530	 Test Loss 1.555
* Epoch: [18/60]	 Top 1-err 40.590  Top 5-err 14.530	 Test Loss 1.555
59.41
0.5941
loss: 1.5546213340759278
(35, 0, 98) triplet: 0.18999999999999997
(35, 0): 0.03
(35, 98): 0.21999999999999997
* Epoch: [19/60]	 Top 1-err 54.734  Top 5-err 27.828	 Train Loss 2.462
* Epoch: [19/60]	 Top 1-err 42.900  Top 5-err 15.120	 Test Loss 1.612
* Epoch: [19/60]	 Top 1-err 42.900  Top 5-err 15.120	 Test Loss 1.612
57.1
0.571
loss: 1.6115861560821534
(35, 0, 98) triplet: 0.155
(35, 0): 0.025
(35, 98): 0.18
* Epoch: [20/60]	 Top 1-err 53.058  Top 5-err 25.185	 Train Loss 2.449
* Epoch: [20/60]	 Top 1-err 39.980  Top 5-err 13.580	 Test Loss 1.487
* Epoch: [20/60]	 Top 1-err 39.980  Top 5-err 13.580	 Test Loss 1.487
60.02
0.6002
loss: 1.4872484497070313
(35, 0, 98) triplet: 0.165
(35, 0): 0.01
(35, 98): 0.17500000000000002
* Epoch: [21/60]	 Top 1-err 55.706  Top 5-err 28.360	 Train Loss 2.492
* Epoch: [21/60]	 Top 1-err 42.420  Top 5-err 15.200	 Test Loss 1.610
* Epoch: [21/60]	 Top 1-err 42.420  Top 5-err 15.200	 Test Loss 1.610
57.58
0.5758
loss: 1.6095815986633302
(35, 0, 98) triplet: 0.07500000000000002
(35, 0): 0.06999999999999999
(35, 98): 0.14500000000000002
not enough sample
* Epoch: [22/60]	 Top 1-err 51.284  Top 5-err 23.906	 Train Loss 2.442
* Epoch: [22/60]	 Top 1-err 43.020  Top 5-err 15.240	 Test Loss 1.607
* Epoch: [22/60]	 Top 1-err 43.020  Top 5-err 15.240	 Test Loss 1.607
56.98
0.5698
loss: 1.606829515838623
(35, 0, 98) triplet: 0.024999999999999994
(35, 0): 0.085
(35, 98): 0.11
* Epoch: [23/60]	 Top 1-err 54.212  Top 5-err 26.473	 Train Loss 2.463
* Epoch: [23/60]	 Top 1-err 43.890  Top 5-err 16.250	 Test Loss 1.671
* Epoch: [23/60]	 Top 1-err 43.890  Top 5-err 16.250	 Test Loss 1.671
56.11
0.5611
loss: 1.6707426959991456
(35, 0, 98) triplet: 0.12
(35, 0): 0.04
(35, 98): 0.16
* Epoch: [24/60]	 Top 1-err 53.784  Top 5-err 26.485	 Train Loss 2.475
* Epoch: [24/60]	 Top 1-err 39.680  Top 5-err 13.720	 Test Loss 1.485
* Epoch: [24/60]	 Top 1-err 39.680  Top 5-err 13.720	 Test Loss 1.485
60.32
0.6032
loss: 1.4845373207092285
(35, 0, 98) triplet: 0.065
(35, 0): 0.135
(35, 98): 0.2
* Epoch: [25/60]	 Top 1-err 53.300  Top 5-err 25.247	 Train Loss 2.471
* Epoch: [25/60]	 Top 1-err 40.640  Top 5-err 13.540	 Test Loss 1.513
* Epoch: [25/60]	 Top 1-err 40.640  Top 5-err 13.540	 Test Loss 1.513
59.36
0.5936
loss: 1.512802872467041
(35, 0, 98) triplet: 0.13
(35, 0): 0.03
(35, 98): 0.16
* Epoch: [26/60]	 Top 1-err 53.353  Top 5-err 25.157	 Train Loss 2.489
* Epoch: [26/60]	 Top 1-err 41.430  Top 5-err 13.740	 Test Loss 1.547
* Epoch: [26/60]	 Top 1-err 41.430  Top 5-err 13.740	 Test Loss 1.547
58.57
0.5857
loss: 1.5467335987091064
(35, 0, 98) triplet: 0.295
(35, 0): 0.01
(35, 98): 0.305
* Epoch: [27/60]	 Top 1-err 53.464  Top 5-err 25.716	 Train Loss 2.488
* Epoch: [27/60]	 Top 1-err 41.440  Top 5-err 14.010	 Test Loss 1.552
* Epoch: [27/60]	 Top 1-err 41.440  Top 5-err 14.010	 Test Loss 1.552
58.56
0.5856
loss: 1.5516138456344604
(35, 0, 98) triplet: 0.13
(35, 0): 0.165
(35, 98): 0.035
* Epoch: [28/60]	 Top 1-err 54.584  Top 5-err 27.051	 Train Loss 2.482
* Epoch: [28/60]	 Top 1-err 40.250  Top 5-err 14.110	 Test Loss 1.536
* Epoch: [28/60]	 Top 1-err 40.250  Top 5-err 14.110	 Test Loss 1.536
59.75
0.5975
loss: 1.5361624084472656
(35, 0, 98) triplet: 0.020000000000000004
(35, 0): 0.08
(35, 98): 0.1
* Epoch: [29/60]	 Top 1-err 53.806  Top 5-err 25.689	 Train Loss 2.467
* Epoch: [29/60]	 Top 1-err 39.700  Top 5-err 12.920	 Test Loss 1.479
* Epoch: [29/60]	 Top 1-err 39.700  Top 5-err 12.920	 Test Loss 1.479
60.3
0.603
loss: 1.4790533279418945
(35, 0, 98) triplet: 0.065
(35, 0): 0.115
(35, 98): 0.05
* Epoch: [30/60]	 Top 1-err 50.208  Top 5-err 23.661	 Train Loss 2.335
* Epoch: [30/60]	 Top 1-err 32.940  Top 5-err 9.680	 Test Loss 1.221
* Epoch: [30/60]	 Top 1-err 32.940  Top 5-err 9.680	 Test Loss 1.221
67.06
0.6706
loss: 1.220730058670044
(35, 0, 98) triplet: 0.16
(35, 0): 0.005
(35, 98): 0.165
* Epoch: [31/60]	 Top 1-err 48.428  Top 5-err 22.860	 Train Loss 2.246
* Epoch: [31/60]	 Top 1-err 32.160  Top 5-err 9.420	 Test Loss 1.201
* Epoch: [31/60]	 Top 1-err 32.160  Top 5-err 9.420	 Test Loss 1.201
67.84
0.6784
loss: 1.2009402118682861
(35, 0, 98) triplet: 0.14500000000000002
(35, 0): 0.015
(35, 98): 0.16
* Epoch: [32/60]	 Top 1-err 45.610  Top 5-err 19.919	 Train Loss 2.246
* Epoch: [32/60]	 Top 1-err 31.700  Top 5-err 9.270	 Test Loss 1.208
* Epoch: [32/60]	 Top 1-err 31.700  Top 5-err 9.270	 Test Loss 1.208
68.3
0.683
loss: 1.2075787033081056
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.005
(35, 98): 0.175
not enough sample
* Epoch: [33/60]	 Top 1-err 48.413  Top 5-err 21.932	 Train Loss 2.271
* Epoch: [33/60]	 Top 1-err 31.710  Top 5-err 9.190	 Test Loss 1.199
* Epoch: [33/60]	 Top 1-err 31.710  Top 5-err 9.190	 Test Loss 1.199
68.29
0.6829
loss: 1.1991434984207154
(35, 0, 98) triplet: 0.15
(35, 0): 0.01
(35, 98): 0.16
* Epoch: [34/60]	 Top 1-err 45.236  Top 5-err 19.960	 Train Loss 2.198
* Epoch: [34/60]	 Top 1-err 31.510  Top 5-err 9.160	 Test Loss 1.207
* Epoch: [34/60]	 Top 1-err 31.510  Top 5-err 9.160	 Test Loss 1.207
68.49
0.6849
loss: 1.2072976768493653
(35, 0, 98) triplet: 0.13999999999999999
(35, 0): 0.01
(35, 98): 0.15
* Epoch: [35/60]	 Top 1-err 48.156  Top 5-err 22.327	 Train Loss 2.263
* Epoch: [35/60]	 Top 1-err 31.110  Top 5-err 9.270	 Test Loss 1.194
* Epoch: [35/60]	 Top 1-err 31.110  Top 5-err 9.270	 Test Loss 1.194
68.89
0.6889
loss: 1.1940946928024292
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.015
(35, 98): 0.185
* Epoch: [36/60]	 Top 1-err 44.698  Top 5-err 19.473	 Train Loss 2.175
* Epoch: [36/60]	 Top 1-err 31.420  Top 5-err 9.240	 Test Loss 1.208
* Epoch: [36/60]	 Top 1-err 31.420  Top 5-err 9.240	 Test Loss 1.208
68.58
0.6858
loss: 1.207538122367859
(35, 0, 98) triplet: 0.18
(35, 0): 0.005
(35, 98): 0.185
not enough sample
* Epoch: [37/60]	 Top 1-err 43.782  Top 5-err 18.855	 Train Loss 2.151
* Epoch: [37/60]	 Top 1-err 30.940  Top 5-err 8.830	 Test Loss 1.162
* Epoch: [37/60]	 Top 1-err 30.940  Top 5-err 8.830	 Test Loss 1.162
69.06
0.6906
loss: 1.1619902034759522
(35, 0, 98) triplet: 0.14
(35, 0): 0.015
(35, 98): 0.155
* Epoch: [38/60]	 Top 1-err 45.852  Top 5-err 20.297	 Train Loss 2.200
* Epoch: [38/60]	 Top 1-err 31.130  Top 5-err 8.750	 Test Loss 1.180
* Epoch: [38/60]	 Top 1-err 31.130  Top 5-err 8.750	 Test Loss 1.180
68.87
0.6887
loss: 1.1804211725234985
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.0
(35, 98): 0.16499999999999998
* Epoch: [39/60]	 Top 1-err 47.779  Top 5-err 21.471	 Train Loss 2.253
* Epoch: [39/60]	 Top 1-err 31.110  Top 5-err 9.030	 Test Loss 1.181
* Epoch: [39/60]	 Top 1-err 31.110  Top 5-err 9.030	 Test Loss 1.181
68.89
0.6889
loss: 1.1809477487564086
(35, 0, 98) triplet: 0.155
(35, 0): 0.005
(35, 98): 0.16
not enough sample
* Epoch: [40/60]	 Top 1-err 46.124  Top 5-err 20.888	 Train Loss 2.216
* Epoch: [40/60]	 Top 1-err 31.590  Top 5-err 8.940	 Test Loss 1.195
* Epoch: [40/60]	 Top 1-err 31.590  Top 5-err 8.940	 Test Loss 1.195
68.41
0.6841
loss: 1.194656706237793
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [41/60]	 Top 1-err 45.306  Top 5-err 19.809	 Train Loss 2.195
* Epoch: [41/60]	 Top 1-err 30.930  Top 5-err 8.730	 Test Loss 1.160
* Epoch: [41/60]	 Top 1-err 30.930  Top 5-err 8.730	 Test Loss 1.160
69.07
0.6907
loss: 1.1602321435928344
(35, 0, 98) triplet: 0.185
(35, 0): 0.0
(35, 98): 0.185
* Epoch: [42/60]	 Top 1-err 44.730  Top 5-err 20.266	 Train Loss 2.162
* Epoch: [42/60]	 Top 1-err 30.600  Top 5-err 8.510	 Test Loss 1.165
* Epoch: [42/60]	 Top 1-err 30.600  Top 5-err 8.510	 Test Loss 1.165
69.4
0.694
loss: 1.16492320728302
(35, 0, 98) triplet: 0.175
(35, 0): 0.015
(35, 98): 0.19
* Epoch: [43/60]	 Top 1-err 44.798  Top 5-err 19.685	 Train Loss 2.159
* Epoch: [43/60]	 Top 1-err 30.800  Top 5-err 8.970	 Test Loss 1.174
* Epoch: [43/60]	 Top 1-err 30.800  Top 5-err 8.970	 Test Loss 1.174
69.2
0.692
loss: 1.1735189840316773
(35, 0, 98) triplet: 0.185
(35, 0): 0.0
(35, 98): 0.185
* Epoch: [44/60]	 Top 1-err 46.284  Top 5-err 20.871	 Train Loss 2.167
* Epoch: [44/60]	 Top 1-err 30.490  Top 5-err 8.690	 Test Loss 1.163
* Epoch: [44/60]	 Top 1-err 30.490  Top 5-err 8.690	 Test Loss 1.163
69.51
0.6951
loss: 1.1628427467346192
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.0
(35, 98): 0.16999999999999998
not enough sample
* Epoch: [45/60]	 Top 1-err 43.506  Top 5-err 18.580	 Train Loss 2.135
* Epoch: [45/60]	 Top 1-err 30.410  Top 5-err 8.610	 Test Loss 1.170
Current best accuracy (top-1 and 5 error): 30.41 8.61
saving best model...
* Epoch: [45/60]	 Top 1-err 30.410  Top 5-err 8.610	 Test Loss 1.170
69.59
0.6959
loss: 1.1695033702850341
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.0
(35, 98): 0.16999999999999998
* Epoch: [46/60]	 Top 1-err 44.552  Top 5-err 19.902	 Train Loss 2.111
* Epoch: [46/60]	 Top 1-err 30.230  Top 5-err 8.610	 Test Loss 1.153
Current best accuracy (top-1 and 5 error): 30.23 8.61
saving best model...
* Epoch: [46/60]	 Top 1-err 30.230  Top 5-err 8.610	 Test Loss 1.153
69.77
0.6977
loss: 1.1533913793563844
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.005
(35, 98): 0.175
* Epoch: [47/60]	 Top 1-err 46.974  Top 5-err 21.432	 Train Loss 2.186
* Epoch: [47/60]	 Top 1-err 30.410  Top 5-err 8.450	 Test Loss 1.150
Current best accuracy (top-1 and 5 error): 30.23 8.61
* Epoch: [47/60]	 Top 1-err 30.410  Top 5-err 8.450	 Test Loss 1.150
69.59
0.6959
loss: 1.1495926671504975
(35, 0, 98) triplet: 0.175
(35, 0): 0.005
(35, 98): 0.18
* Epoch: [48/60]	 Top 1-err 43.653  Top 5-err 18.727	 Train Loss 2.129
* Epoch: [48/60]	 Top 1-err 30.320  Top 5-err 8.440	 Test Loss 1.152
Current best accuracy (top-1 and 5 error): 30.23 8.61
* Epoch: [48/60]	 Top 1-err 30.320  Top 5-err 8.440	 Test Loss 1.152
69.68
0.6968
loss: 1.151543828010559
(35, 0, 98) triplet: 0.18
(35, 0): 0.0
(35, 98): 0.18
* Epoch: [49/60]	 Top 1-err 44.768  Top 5-err 20.718	 Train Loss 2.093
* Epoch: [49/60]	 Top 1-err 30.440  Top 5-err 8.520	 Test Loss 1.148
Current best accuracy (top-1 and 5 error): 30.23 8.61
* Epoch: [49/60]	 Top 1-err 30.440  Top 5-err 8.520	 Test Loss 1.148
69.56
0.6956
loss: 1.14762550907135
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.005
(35, 98): 0.16999999999999998
* Epoch: [50/60]	 Top 1-err 44.076  Top 5-err 19.129	 Train Loss 2.127
* Epoch: [50/60]	 Top 1-err 30.210  Top 5-err 8.360	 Test Loss 1.145
Current best accuracy (top-1 and 5 error): 30.21 8.36
saving best model...
* Epoch: [50/60]	 Top 1-err 30.210  Top 5-err 8.360	 Test Loss 1.145
69.79
0.6979
loss: 1.1446997802734376
(35, 0, 98) triplet: 0.165
(35, 0): 0.005
(35, 98): 0.17
* Epoch: [51/60]	 Top 1-err 47.146  Top 5-err 21.462	 Train Loss 2.209
* Epoch: [51/60]	 Top 1-err 30.200  Top 5-err 8.560	 Test Loss 1.151
Current best accuracy (top-1 and 5 error): 30.2 8.56
saving best model...
* Epoch: [51/60]	 Top 1-err 30.200  Top 5-err 8.560	 Test Loss 1.151
69.8
0.698
loss: 1.150561840057373
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.005
(35, 98): 0.175
* Epoch: [52/60]	 Top 1-err 44.377  Top 5-err 18.965	 Train Loss 2.158
* Epoch: [52/60]	 Top 1-err 30.340  Top 5-err 8.550	 Test Loss 1.150
Current best accuracy (top-1 and 5 error): 30.2 8.56
* Epoch: [52/60]	 Top 1-err 30.340  Top 5-err 8.550	 Test Loss 1.150
69.66
0.6966
loss: 1.150002512550354
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.005
(35, 98): 0.16999999999999998
* Epoch: [53/60]	 Top 1-err 41.772  Top 5-err 16.997	 Train Loss 2.114
* Epoch: [53/60]	 Top 1-err 30.240  Top 5-err 8.450	 Test Loss 1.149
Current best accuracy (top-1 and 5 error): 30.2 8.56
* Epoch: [53/60]	 Top 1-err 30.240  Top 5-err 8.450	 Test Loss 1.149
69.76
0.6976
loss: 1.1485123132705688
(35, 0, 98) triplet: 0.185
(35, 0): 0.005
(35, 98): 0.19
not enough sample
* Epoch: [54/60]	 Top 1-err 43.784  Top 5-err 19.195	 Train Loss 2.156
* Epoch: [54/60]	 Top 1-err 30.720  Top 5-err 8.600	 Test Loss 1.189
Current best accuracy (top-1 and 5 error): 30.2 8.56
* Epoch: [54/60]	 Top 1-err 30.720  Top 5-err 8.600	 Test Loss 1.189
69.28
0.6928
loss: 1.1891337615966797
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.005
(35, 98): 0.175
* Epoch: [55/60]	 Top 1-err 43.774  Top 5-err 19.210	 Train Loss 2.113
* Epoch: [55/60]	 Top 1-err 30.080  Top 5-err 8.420	 Test Loss 1.135
Current best accuracy (top-1 and 5 error): 30.08 8.42
saving best model...
* Epoch: [55/60]	 Top 1-err 30.080  Top 5-err 8.420	 Test Loss 1.135
69.92
0.6992
loss: 1.1346870582580566
(35, 0, 98) triplet: 0.155
(35, 0): 0.005
(35, 98): 0.16
not enough sample
* Epoch: [56/60]	 Top 1-err 43.432  Top 5-err 18.098	 Train Loss 2.146
* Epoch: [56/60]	 Top 1-err 30.290  Top 5-err 8.600	 Test Loss 1.161
Current best accuracy (top-1 and 5 error): 30.08 8.42
* Epoch: [56/60]	 Top 1-err 30.290  Top 5-err 8.600	 Test Loss 1.161
69.71
0.6971
loss: 1.1613750680923463
(35, 0, 98) triplet: 0.15999999999999998
(35, 0): 0.005
(35, 98): 0.16499999999999998
* Epoch: [57/60]	 Top 1-err 44.635  Top 5-err 19.397	 Train Loss 2.164
* Epoch: [57/60]	 Top 1-err 30.280  Top 5-err 8.420	 Test Loss 1.133
Current best accuracy (top-1 and 5 error): 30.08 8.42
* Epoch: [57/60]	 Top 1-err 30.280  Top 5-err 8.420	 Test Loss 1.133
69.72
0.6972
loss: 1.1331907052993775
(35, 0, 98) triplet: 0.175
(35, 0): 0.0
(35, 98): 0.175
* Epoch: [58/60]	 Top 1-err 43.880  Top 5-err 18.989	 Train Loss 2.093
* Epoch: [58/60]	 Top 1-err 30.160  Top 5-err 8.330	 Test Loss 1.129
Current best accuracy (top-1 and 5 error): 30.08 8.42
* Epoch: [58/60]	 Top 1-err 30.160  Top 5-err 8.330	 Test Loss 1.129
69.84
0.6984
loss: 1.1286469123840333
(35, 0, 98) triplet: 0.16
(35, 0): 0.0
(35, 98): 0.16
* Epoch: [59/60]	 Top 1-err 42.318  Top 5-err 17.907	 Train Loss 2.083
* Epoch: [59/60]	 Top 1-err 30.310  Top 5-err 8.450	 Test Loss 1.147
Current best accuracy (top-1 and 5 error): 30.08 8.42
* Epoch: [59/60]	 Top 1-err 30.310  Top 5-err 8.450	 Test Loss 1.147
69.69
0.6969
loss: 1.1466320158004761
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.01
(35, 98): 0.175
Best accuracy (top-1 and 5 error): 30.08 8.42
=> loading checkpoint 'runs/cifar100_resnet_1/model_best.pth.tar'
* Epoch: [-1/60]	 Top 1-err 30.080  Top 5-err 8.420	 Test Loss 1.135
69.92
0.6992
loss: 1.1346870471954347
(35, 0, 98) triplet: 0.155
(35, 0): 0.005
(35, 98): 0.16
python3 cifar100_repair_bias.py --net_type resnet --dataset cifar100 --depth 50 --batch_size 256 --lr 0.1 --expname cifar100_resnet_1 --epochs 60 --beta 1.0 --cutmix_prob 1 --pretrained ./runs/cifar100_resnet_1/model_best.pth.tar --expid 0 --first 35 --second 0 --third 98 --lam 2
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
=> creating model 'resnet'
True
=> loading checkpoint './runs/cifar100_resnet_1/model_best.pth.tar'
=> loaded checkpoint './runs/cifar100_resnet_1/model_best.pth.tar'
the number of model parameters: 521716
* Epoch: [0/60]	 Top 1-err 58.665  Top 5-err 30.614	 Train Loss 2.716
* Epoch: [0/60]	 Top 1-err 48.700  Top 5-err 20.220	 Test Loss 1.885
* Epoch: [0/60]	 Top 1-err 48.700  Top 5-err 20.220	 Test Loss 1.885
51.3
0.513
loss: 1.8853176750183105
(35, 0, 98) triplet: 0.11000000000000001
(35, 0): 0.025
(35, 98): 0.135
* Epoch: [1/60]	 Top 1-err 62.134  Top 5-err 34.360	 Train Loss 2.795
* Epoch: [1/60]	 Top 1-err 51.840  Top 5-err 21.940	 Test Loss 2.017
* Epoch: [1/60]	 Top 1-err 51.840  Top 5-err 21.940	 Test Loss 2.017
48.16
0.4816
loss: 2.0173988952636717
(35, 0, 98) triplet: 0.13
(35, 0): 0.03
(35, 98): 0.16
* Epoch: [2/60]	 Top 1-err 59.896  Top 5-err 31.328	 Train Loss 2.716
* Epoch: [2/60]	 Top 1-err 44.730  Top 5-err 17.070	 Test Loss 1.691
* Epoch: [2/60]	 Top 1-err 44.730  Top 5-err 17.070	 Test Loss 1.691
55.27
0.5527
loss: 1.690725473022461
(35, 0, 98) triplet: 0.015
(35, 0): 0.095
(35, 98): 0.08
* Epoch: [3/60]	 Top 1-err 58.330  Top 5-err 30.355	 Train Loss 2.659
* Epoch: [3/60]	 Top 1-err 47.380  Top 5-err 18.550	 Test Loss 1.860
* Epoch: [3/60]	 Top 1-err 47.380  Top 5-err 18.550	 Test Loss 1.860
52.62
0.5262
loss: 1.8600420104980468
(35, 0, 98) triplet: 0.2
(35, 0): 0.0
(35, 98): 0.2
not enough sample
not enough sample
* Epoch: [4/60]	 Top 1-err 60.351  Top 5-err 31.794	 Train Loss 2.761
* Epoch: [4/60]	 Top 1-err 47.760  Top 5-err 19.060	 Test Loss 1.858
* Epoch: [4/60]	 Top 1-err 47.760  Top 5-err 19.060	 Test Loss 1.858
52.24
0.5224
loss: 1.8578109279632569
(35, 0, 98) triplet: 0.255
(35, 0): 0.005
(35, 98): 0.26
* Epoch: [5/60]	 Top 1-err 56.160  Top 5-err 27.677	 Train Loss 2.618
* Epoch: [5/60]	 Top 1-err 47.540  Top 5-err 18.760	 Test Loss 1.812
* Epoch: [5/60]	 Top 1-err 47.540  Top 5-err 18.760	 Test Loss 1.812
52.46
0.5246
loss: 1.812009277534485
(35, 0, 98) triplet: 0.1
(35, 0): 0.04
(35, 98): 0.14
* Epoch: [6/60]	 Top 1-err 58.570  Top 5-err 30.015	 Train Loss 2.705
* Epoch: [6/60]	 Top 1-err 43.370  Top 5-err 15.700	 Test Loss 1.661
* Epoch: [6/60]	 Top 1-err 43.370  Top 5-err 15.700	 Test Loss 1.661
56.63
0.5663
loss: 1.6614885334014893
(35, 0, 98) triplet: 0.15
(35, 0): 0.04
(35, 98): 0.19
* Epoch: [7/60]	 Top 1-err 57.012  Top 5-err 29.288	 Train Loss 2.593
* Epoch: [7/60]	 Top 1-err 47.260  Top 5-err 18.290	 Test Loss 1.817
* Epoch: [7/60]	 Top 1-err 47.260  Top 5-err 18.290	 Test Loss 1.817
52.74
0.5274
loss: 1.8172113677978516
(35, 0, 98) triplet: 0.10999999999999999
(35, 0): 0.06
(35, 98): 0.16999999999999998
* Epoch: [8/60]	 Top 1-err 57.684  Top 5-err 29.679	 Train Loss 2.609
* Epoch: [8/60]	 Top 1-err 47.790  Top 5-err 19.770	 Test Loss 1.842
* Epoch: [8/60]	 Top 1-err 47.790  Top 5-err 19.770	 Test Loss 1.842
52.21
0.5221
loss: 1.841989210510254
(35, 0, 98) triplet: 0.15500000000000003
(35, 0): 0.06
(35, 98): 0.21500000000000002
* Epoch: [9/60]	 Top 1-err 59.112  Top 5-err 30.929	 Train Loss 2.686
* Epoch: [9/60]	 Top 1-err 44.360  Top 5-err 16.650	 Test Loss 1.662
* Epoch: [9/60]	 Top 1-err 44.360  Top 5-err 16.650	 Test Loss 1.662
55.64
0.5564
loss: 1.6620217926025391
(35, 0, 98) triplet: 0.039999999999999994
(35, 0): 0.12
(35, 98): 0.08
* Epoch: [10/60]	 Top 1-err 59.396  Top 5-err 30.631	 Train Loss 2.708
* Epoch: [10/60]	 Top 1-err 46.560  Top 5-err 17.820	 Test Loss 1.789
* Epoch: [10/60]	 Top 1-err 46.560  Top 5-err 17.820	 Test Loss 1.789
53.44
0.5344
loss: 1.7889708518981933
(35, 0, 98) triplet: 0.05
(35, 0): 0.1
(35, 98): 0.05
* Epoch: [11/60]	 Top 1-err 59.127  Top 5-err 30.565	 Train Loss 2.715
* Epoch: [11/60]	 Top 1-err 45.760  Top 5-err 17.980	 Test Loss 1.750
* Epoch: [11/60]	 Top 1-err 45.760  Top 5-err 17.980	 Test Loss 1.750
54.24
0.5424
loss: 1.7497831363677978
(35, 0, 98) triplet: 0.020000000000000004
(35, 0): 0.09
(35, 98): 0.11
not enough sample
* Epoch: [12/60]	 Top 1-err 59.847  Top 5-err 31.721	 Train Loss 2.710
* Epoch: [12/60]	 Top 1-err 43.970  Top 5-err 15.920	 Test Loss 1.664
* Epoch: [12/60]	 Top 1-err 43.970  Top 5-err 15.920	 Test Loss 1.664
56.03
0.5603
loss: 1.6636716960906983
(35, 0, 98) triplet: 0.09999999999999999
(35, 0): 0.015
(35, 98): 0.11499999999999999
* Epoch: [13/60]	 Top 1-err 57.777  Top 5-err 29.462	 Train Loss 2.652
* Epoch: [13/60]	 Top 1-err 42.340  Top 5-err 14.560	 Test Loss 1.586
* Epoch: [13/60]	 Top 1-err 42.340  Top 5-err 14.560	 Test Loss 1.586
57.66
0.5766
loss: 1.5861299476623536
(35, 0, 98) triplet: 0.12999999999999998
(35, 0): 0.04
(35, 98): 0.16999999999999998
* Epoch: [14/60]	 Top 1-err 57.767  Top 5-err 29.490	 Train Loss 2.668
* Epoch: [14/60]	 Top 1-err 42.160  Top 5-err 14.800	 Test Loss 1.589
* Epoch: [14/60]	 Top 1-err 42.160  Top 5-err 14.800	 Test Loss 1.589
57.84
0.5784
loss: 1.589073420715332
(35, 0, 98) triplet: 0.06999999999999999
(35, 0): 0.005
(35, 98): 0.075
* Epoch: [15/60]	 Top 1-err 60.589  Top 5-err 32.541	 Train Loss 2.697
* Epoch: [15/60]	 Top 1-err 47.080  Top 5-err 18.950	 Test Loss 1.843
* Epoch: [15/60]	 Top 1-err 47.080  Top 5-err 18.950	 Test Loss 1.843
52.92
0.5292
loss: 1.8425382724761963
(35, 0, 98) triplet: 0.03
(35, 0): 0.025
(35, 98): 0.055
* Epoch: [16/60]	 Top 1-err 57.731  Top 5-err 29.267	 Train Loss 2.676
* Epoch: [16/60]	 Top 1-err 47.710  Top 5-err 18.780	 Test Loss 1.846
* Epoch: [16/60]	 Top 1-err 47.710  Top 5-err 18.780	 Test Loss 1.846
52.29
0.5229
loss: 1.8461356479644775
(35, 0, 98) triplet: 0.055
(35, 0): 0.095
(35, 98): 0.04
not enough sample
* Epoch: [17/60]	 Top 1-err 60.733  Top 5-err 32.626	 Train Loss 2.735
* Epoch: [17/60]	 Top 1-err 43.890  Top 5-err 16.750	 Test Loss 1.670
* Epoch: [17/60]	 Top 1-err 43.890  Top 5-err 16.750	 Test Loss 1.670
56.11
0.5611
loss: 1.6700814231872558
(35, 0, 98) triplet: 0.13
(35, 0): 0.015
(35, 98): 0.145
* Epoch: [18/60]	 Top 1-err 59.821  Top 5-err 31.283	 Train Loss 2.699
* Epoch: [18/60]	 Top 1-err 45.860  Top 5-err 17.860	 Test Loss 1.784
* Epoch: [18/60]	 Top 1-err 45.860  Top 5-err 17.860	 Test Loss 1.784
54.14
0.5414
loss: 1.784277131652832
(35, 0, 98) triplet: 0.045
(35, 0): 0.135
(35, 98): 0.09000000000000001
* Epoch: [19/60]	 Top 1-err 58.506  Top 5-err 30.495	 Train Loss 2.628
* Epoch: [19/60]	 Top 1-err 43.150  Top 5-err 15.190	 Test Loss 1.644
* Epoch: [19/60]	 Top 1-err 43.150  Top 5-err 15.190	 Test Loss 1.644
56.85
0.5685
loss: 1.6435990682601929
(35, 0, 98) triplet: 0.135
(35, 0): 0.065
(35, 98): 0.2
* Epoch: [20/60]	 Top 1-err 55.606  Top 5-err 27.163	 Train Loss 2.562
* Epoch: [20/60]	 Top 1-err 46.260  Top 5-err 18.360	 Test Loss 1.760
* Epoch: [20/60]	 Top 1-err 46.260  Top 5-err 18.360	 Test Loss 1.760
53.74
0.5374
loss: 1.7597057872772217
(35, 0, 98) triplet: 0.19999999999999998
(35, 0): 0.005
(35, 98): 0.205
* Epoch: [21/60]	 Top 1-err 59.777  Top 5-err 31.585	 Train Loss 2.680
* Epoch: [21/60]	 Top 1-err 46.080  Top 5-err 17.600	 Test Loss 1.735
* Epoch: [21/60]	 Top 1-err 46.080  Top 5-err 17.600	 Test Loss 1.735
53.92
0.5392
loss: 1.7350731632232665
(35, 0, 98) triplet: 0.15
(35, 0): 0.049999999999999996
(35, 98): 0.19999999999999998
not enough sample
* Epoch: [22/60]	 Top 1-err 54.709  Top 5-err 26.177	 Train Loss 2.580
* Epoch: [22/60]	 Top 1-err 44.650  Top 5-err 16.610	 Test Loss 1.700
* Epoch: [22/60]	 Top 1-err 44.650  Top 5-err 16.610	 Test Loss 1.700
55.35
0.5535
loss: 1.6999862861633301
(35, 0, 98) triplet: 0.19
(35, 0): 0.055
(35, 98): 0.245
* Epoch: [23/60]	 Top 1-err 57.554  Top 5-err 29.333	 Train Loss 2.624
* Epoch: [23/60]	 Top 1-err 46.880  Top 5-err 18.760	 Test Loss 1.804
* Epoch: [23/60]	 Top 1-err 46.880  Top 5-err 18.760	 Test Loss 1.804
53.12
0.5312
loss: 1.8038175661087037
(35, 0, 98) triplet: 0.085
(35, 0): 0.0
(35, 98): 0.085
* Epoch: [24/60]	 Top 1-err 57.201  Top 5-err 28.867	 Train Loss 2.616
* Epoch: [24/60]	 Top 1-err 42.710  Top 5-err 14.760	 Test Loss 1.585
* Epoch: [24/60]	 Top 1-err 42.710  Top 5-err 14.760	 Test Loss 1.585
57.29
0.5729
loss: 1.584620302581787
(35, 0, 98) triplet: 0.07
(35, 0): 0.034999999999999996
(35, 98): 0.10500000000000001
* Epoch: [25/60]	 Top 1-err 57.182  Top 5-err 28.417	 Train Loss 2.645
* Epoch: [25/60]	 Top 1-err 48.390  Top 5-err 19.080	 Test Loss 1.836
* Epoch: [25/60]	 Top 1-err 48.390  Top 5-err 19.080	 Test Loss 1.836
51.61
0.5161
loss: 1.8356505323410035
(35, 0, 98) triplet: 0.060000000000000026
(35, 0): 0.14500000000000002
(35, 98): 0.08499999999999999
* Epoch: [26/60]	 Top 1-err 56.220  Top 5-err 27.233	 Train Loss 2.602
* Epoch: [26/60]	 Top 1-err 46.010  Top 5-err 17.830	 Test Loss 1.773
* Epoch: [26/60]	 Top 1-err 46.010  Top 5-err 17.830	 Test Loss 1.773
53.99
0.5399
loss: 1.7728042192459106
(35, 0, 98) triplet: 0.06999999999999999
(35, 0): 0.115
(35, 98): 0.185
* Epoch: [27/60]	 Top 1-err 56.326  Top 5-err 27.890	 Train Loss 2.608
* Epoch: [27/60]	 Top 1-err 43.450  Top 5-err 15.140	 Test Loss 1.627
* Epoch: [27/60]	 Top 1-err 43.450  Top 5-err 15.140	 Test Loss 1.627
56.55
0.5655
loss: 1.626967275238037
(35, 0, 98) triplet: 0.18000000000000002
(35, 0): 0.025
(35, 98): 0.20500000000000002
* Epoch: [28/60]	 Top 1-err 57.442  Top 5-err 29.518	 Train Loss 2.620
* Epoch: [28/60]	 Top 1-err 43.010  Top 5-err 14.950	 Test Loss 1.623
* Epoch: [28/60]	 Top 1-err 43.010  Top 5-err 14.950	 Test Loss 1.623
56.99
0.5699
loss: 1.6232419689178468
(35, 0, 98) triplet: 0.045
(35, 0): 0.02
(35, 98): 0.065
* Epoch: [29/60]	 Top 1-err 56.832  Top 5-err 28.107	 Train Loss 2.609
* Epoch: [29/60]	 Top 1-err 47.020  Top 5-err 17.680	 Test Loss 1.753
* Epoch: [29/60]	 Top 1-err 47.020  Top 5-err 17.680	 Test Loss 1.753
52.98
0.5298
loss: 1.7529219646453857
(35, 0, 98) triplet: 0.395
(35, 0): 0.0
(35, 98): 0.395
* Epoch: [30/60]	 Top 1-err 54.335  Top 5-err 26.513	 Train Loss 2.506
* Epoch: [30/60]	 Top 1-err 35.010  Top 5-err 10.660	 Test Loss 1.302
* Epoch: [30/60]	 Top 1-err 35.010  Top 5-err 10.660	 Test Loss 1.302
64.99
0.6499
loss: 1.3024425760269165
(35, 0, 98) triplet: 0.19
(35, 0): 0.0
(35, 98): 0.19
* Epoch: [31/60]	 Top 1-err 52.025  Top 5-err 25.421	 Train Loss 2.396
* Epoch: [31/60]	 Top 1-err 34.060  Top 5-err 10.160	 Test Loss 1.263
* Epoch: [31/60]	 Top 1-err 34.060  Top 5-err 10.160	 Test Loss 1.263
65.94
0.6594
loss: 1.2626070400238036
(35, 0, 98) triplet: 0.105
(35, 0): 0.02
(35, 98): 0.125
* Epoch: [32/60]	 Top 1-err 49.031  Top 5-err 22.174	 Train Loss 2.381
* Epoch: [32/60]	 Top 1-err 33.750  Top 5-err 10.130	 Test Loss 1.261
* Epoch: [32/60]	 Top 1-err 33.750  Top 5-err 10.130	 Test Loss 1.261
66.25
0.6625
loss: 1.2612652896881102
(35, 0, 98) triplet: 0.11999999999999998
(35, 0): 0.02
(35, 98): 0.13999999999999999
not enough sample
* Epoch: [33/60]	 Top 1-err 51.592  Top 5-err 23.935	 Train Loss 2.399
* Epoch: [33/60]	 Top 1-err 33.440  Top 5-err 9.940	 Test Loss 1.253
* Epoch: [33/60]	 Top 1-err 33.440  Top 5-err 9.940	 Test Loss 1.253
66.56
0.6656
loss: 1.252596760559082
(35, 0, 98) triplet: 0.11
(35, 0): 0.02
(35, 98): 0.13
* Epoch: [34/60]	 Top 1-err 48.109  Top 5-err 21.679	 Train Loss 2.320
* Epoch: [34/60]	 Top 1-err 32.830  Top 5-err 9.640	 Test Loss 1.248
* Epoch: [34/60]	 Top 1-err 32.830  Top 5-err 9.640	 Test Loss 1.248
67.17
0.6717
loss: 1.24794278717041
(35, 0, 98) triplet: 0.14
(35, 0): 0.0
(35, 98): 0.14
* Epoch: [35/60]	 Top 1-err 50.878  Top 5-err 24.037	 Train Loss 2.390
* Epoch: [35/60]	 Top 1-err 32.430  Top 5-err 9.720	 Test Loss 1.224
* Epoch: [35/60]	 Top 1-err 32.430  Top 5-err 9.720	 Test Loss 1.224
67.57
0.6757
loss: 1.22446764087677
(35, 0, 98) triplet: 0.135
(35, 0): 0.015
(35, 98): 0.15
* Epoch: [36/60]	 Top 1-err 47.405  Top 5-err 21.056	 Train Loss 2.282
* Epoch: [36/60]	 Top 1-err 32.580  Top 5-err 9.200	 Test Loss 1.227
* Epoch: [36/60]	 Top 1-err 32.580  Top 5-err 9.200	 Test Loss 1.227
67.42
0.6742
loss: 1.2270373888015746
(35, 0, 98) triplet: 0.1
(35, 0): 0.03
(35, 98): 0.13
not enough sample
* Epoch: [37/60]	 Top 1-err 46.694  Top 5-err 20.650	 Train Loss 2.273
* Epoch: [37/60]	 Top 1-err 32.150  Top 5-err 9.160	 Test Loss 1.192
* Epoch: [37/60]	 Top 1-err 32.150  Top 5-err 9.160	 Test Loss 1.192
67.85
0.6785
loss: 1.192434167098999
(35, 0, 98) triplet: 0.07
(35, 0): 0.03
(35, 98): 0.1
* Epoch: [38/60]	 Top 1-err 48.336  Top 5-err 21.828	 Train Loss 2.304
* Epoch: [38/60]	 Top 1-err 32.140  Top 5-err 9.090	 Test Loss 1.219
* Epoch: [38/60]	 Top 1-err 32.140  Top 5-err 9.090	 Test Loss 1.219
67.86
0.6786
loss: 1.2186586765289307
(35, 0, 98) triplet: 0.16999999999999998
(35, 0): 0.005
(35, 98): 0.175
* Epoch: [39/60]	 Top 1-err 50.223  Top 5-err 23.270	 Train Loss 2.363
* Epoch: [39/60]	 Top 1-err 31.810  Top 5-err 9.170	 Test Loss 1.206
* Epoch: [39/60]	 Top 1-err 31.810  Top 5-err 9.170	 Test Loss 1.206
68.19
0.6819
loss: 1.2063556858062745
(35, 0, 98) triplet: 0.060000000000000005
(35, 0): 0.04
(35, 98): 0.1
not enough sample
* Epoch: [40/60]	 Top 1-err 48.593  Top 5-err 22.578	 Train Loss 2.324
* Epoch: [40/60]	 Top 1-err 32.270  Top 5-err 9.350	 Test Loss 1.226
* Epoch: [40/60]	 Top 1-err 32.270  Top 5-err 9.350	 Test Loss 1.226
67.73
0.6773
loss: 1.2257029999732971
(35, 0, 98) triplet: 0.13499999999999998
(35, 0): 0.005
(35, 98): 0.13999999999999999
* Epoch: [41/60]	 Top 1-err 47.775  Top 5-err 21.366	 Train Loss 2.298
* Epoch: [41/60]	 Top 1-err 31.960  Top 5-err 9.180	 Test Loss 1.199
* Epoch: [41/60]	 Top 1-err 31.960  Top 5-err 9.180	 Test Loss 1.199
68.04
0.6804
loss: 1.1987099353790283
(35, 0, 98) triplet: 0.115
(35, 0): 0.01
(35, 98): 0.125
* Epoch: [42/60]	 Top 1-err 47.046  Top 5-err 21.755	 Train Loss 2.256
* Epoch: [42/60]	 Top 1-err 31.650  Top 5-err 9.050	 Test Loss 1.190
* Epoch: [42/60]	 Top 1-err 31.650  Top 5-err 9.050	 Test Loss 1.190
68.35
0.6835
loss: 1.1897914617538452
(35, 0, 98) triplet: 0.155
(35, 0): 0.005
(35, 98): 0.16
* Epoch: [43/60]	 Top 1-err 47.218  Top 5-err 21.111	 Train Loss 2.262
* Epoch: [43/60]	 Top 1-err 31.820  Top 5-err 8.900	 Test Loss 1.186
* Epoch: [43/60]	 Top 1-err 31.820  Top 5-err 8.900	 Test Loss 1.186
68.18
0.6818
loss: 1.1862093158721925
(35, 0, 98) triplet: 0.09000000000000001
(35, 0): 0.02
(35, 98): 0.11000000000000001
* Epoch: [44/60]	 Top 1-err 48.340  Top 5-err 22.391	 Train Loss 2.267
* Epoch: [44/60]	 Top 1-err 31.700  Top 5-err 8.880	 Test Loss 1.194
* Epoch: [44/60]	 Top 1-err 31.700  Top 5-err 8.880	 Test Loss 1.194
68.3
0.683
loss: 1.1944814262390138
(35, 0, 98) triplet: 0.13499999999999998
(35, 0): 0.01
(35, 98): 0.145
not enough sample
* Epoch: [45/60]	 Top 1-err 45.799  Top 5-err 19.981	 Train Loss 2.237
* Epoch: [45/60]	 Top 1-err 31.470  Top 5-err 8.800	 Test Loss 1.196
Current best accuracy (top-1 and 5 error): 31.47 8.8
saving best model...
* Epoch: [45/60]	 Top 1-err 31.470  Top 5-err 8.800	 Test Loss 1.196
68.53
0.6853
loss: 1.195643054008484
(35, 0, 98) triplet: 0.15
(35, 0): 0.005
(35, 98): 0.155
* Epoch: [46/60]	 Top 1-err 46.896  Top 5-err 21.271	 Train Loss 2.211
* Epoch: [46/60]	 Top 1-err 31.290  Top 5-err 8.820	 Test Loss 1.181
Current best accuracy (top-1 and 5 error): 31.29 8.82
saving best model...
* Epoch: [46/60]	 Top 1-err 31.290  Top 5-err 8.820	 Test Loss 1.181
68.71
0.6871
loss: 1.1807445278167725
(35, 0, 98) triplet: 0.15
(35, 0): 0.005
(35, 98): 0.155
* Epoch: [47/60]	 Top 1-err 48.940  Top 5-err 22.915	 Train Loss 2.291
* Epoch: [47/60]	 Top 1-err 31.270  Top 5-err 8.590	 Test Loss 1.176
Current best accuracy (top-1 and 5 error): 31.27 8.59
saving best model...
* Epoch: [47/60]	 Top 1-err 31.270  Top 5-err 8.590	 Test Loss 1.176
68.73
0.6873
loss: 1.1755874508857727
(35, 0, 98) triplet: 0.135
(35, 0): 0.01
(35, 98): 0.14500000000000002
* Epoch: [48/60]	 Top 1-err 45.846  Top 5-err 20.076	 Train Loss 2.228
* Epoch: [48/60]	 Top 1-err 31.260  Top 5-err 8.590	 Test Loss 1.179
Current best accuracy (top-1 and 5 error): 31.26 8.59
saving best model...
* Epoch: [48/60]	 Top 1-err 31.260  Top 5-err 8.590	 Test Loss 1.179
68.74
0.6874
loss: 1.1788860321044923
(35, 0, 98) triplet: 0.15
(35, 0): 0.01
(35, 98): 0.16
* Epoch: [49/60]	 Top 1-err 46.794  Top 5-err 22.244	 Train Loss 2.193
* Epoch: [49/60]	 Top 1-err 31.210  Top 5-err 8.530	 Test Loss 1.172
Current best accuracy (top-1 and 5 error): 31.21 8.53
saving best model...
* Epoch: [49/60]	 Top 1-err 31.210  Top 5-err 8.530	 Test Loss 1.172
68.79
0.6879
loss: 1.172398157119751
(35, 0, 98) triplet: 0.13999999999999999
(35, 0): 0.01
(35, 98): 0.15
* Epoch: [50/60]	 Top 1-err 46.309  Top 5-err 20.521	 Train Loss 2.230
* Epoch: [50/60]	 Top 1-err 31.020  Top 5-err 8.520	 Test Loss 1.169
Current best accuracy (top-1 and 5 error): 31.02 8.52
saving best model...
* Epoch: [50/60]	 Top 1-err 31.020  Top 5-err 8.520	 Test Loss 1.169
68.98
0.6898
loss: 1.1691622909545898
(35, 0, 98) triplet: 0.15
(35, 0): 0.01
(35, 98): 0.16
* Epoch: [51/60]	 Top 1-err 49.348  Top 5-err 22.960	 Train Loss 2.307
* Epoch: [51/60]	 Top 1-err 31.040  Top 5-err 8.600	 Test Loss 1.176
Current best accuracy (top-1 and 5 error): 31.02 8.52
* Epoch: [51/60]	 Top 1-err 31.040  Top 5-err 8.600	 Test Loss 1.176
68.96
0.6896
loss: 1.1763072624206543
(35, 0, 98) triplet: 0.16
(35, 0): 0.005
(35, 98): 0.165
* Epoch: [52/60]	 Top 1-err 46.653  Top 5-err 20.229	 Train Loss 2.255
* Epoch: [52/60]	 Top 1-err 30.910  Top 5-err 8.520	 Test Loss 1.176
Current best accuracy (top-1 and 5 error): 30.91 8.52
saving best model...
* Epoch: [52/60]	 Top 1-err 30.910  Top 5-err 8.520	 Test Loss 1.176
69.09
0.6909
loss: 1.1759784503936768
(35, 0, 98) triplet: 0.135
(35, 0): 0.005
(35, 98): 0.14
* Epoch: [53/60]	 Top 1-err 44.175  Top 5-err 18.557	 Train Loss 2.214
* Epoch: [53/60]	 Top 1-err 30.720  Top 5-err 8.590	 Test Loss 1.171
Current best accuracy (top-1 and 5 error): 30.72 8.59
saving best model...
* Epoch: [53/60]	 Top 1-err 30.720  Top 5-err 8.590	 Test Loss 1.171
69.28
0.6928
loss: 1.1714293667793274
(35, 0, 98) triplet: 0.15
(35, 0): 0.01
(35, 98): 0.16
not enough sample
* Epoch: [54/60]	 Top 1-err 45.861  Top 5-err 20.582	 Train Loss 2.253
* Epoch: [54/60]	 Top 1-err 31.410  Top 5-err 8.680	 Test Loss 1.215
Current best accuracy (top-1 and 5 error): 30.72 8.59
* Epoch: [54/60]	 Top 1-err 31.410  Top 5-err 8.680	 Test Loss 1.215
68.59
0.6859
loss: 1.2147820051193237
(35, 0, 98) triplet: 0.16499999999999998
(35, 0): 0.005
(35, 98): 0.16999999999999998
* Epoch: [55/60]	 Top 1-err 45.797  Top 5-err 20.557	 Train Loss 2.209
* Epoch: [55/60]	 Top 1-err 30.890  Top 5-err 8.570	 Test Loss 1.163
Current best accuracy (top-1 and 5 error): 30.72 8.59
* Epoch: [55/60]	 Top 1-err 30.890  Top 5-err 8.570	 Test Loss 1.163
69.11
0.6911
loss: 1.1634062046051026
(35, 0, 98) triplet: 0.13
(35, 0): 0.01
(35, 98): 0.14
not enough sample
* Epoch: [56/60]	 Top 1-err 45.510  Top 5-err 19.601	 Train Loss 2.240
* Epoch: [56/60]	 Top 1-err 31.050  Top 5-err 8.590	 Test Loss 1.185
Current best accuracy (top-1 and 5 error): 30.72 8.59
* Epoch: [56/60]	 Top 1-err 31.050  Top 5-err 8.590	 Test Loss 1.185
68.95
0.6895
loss: 1.1853127662658691
(35, 0, 98) triplet: 0.15
(35, 0): 0.01
(35, 98): 0.16
* Epoch: [57/60]	 Top 1-err 46.683  Top 5-err 20.954	 Train Loss 2.265
* Epoch: [57/60]	 Top 1-err 30.800  Top 5-err 8.370	 Test Loss 1.158
Current best accuracy (top-1 and 5 error): 30.72 8.59
* Epoch: [57/60]	 Top 1-err 30.800  Top 5-err 8.370	 Test Loss 1.158
69.2
0.692
loss: 1.1581236255645753
(35, 0, 98) triplet: 0.13
(35, 0): 0.01
(35, 98): 0.14
* Epoch: [58/60]	 Top 1-err 46.092  Top 5-err 20.470	 Train Loss 2.197
* Epoch: [58/60]	 Top 1-err 30.990  Top 5-err 8.540	 Test Loss 1.155
Current best accuracy (top-1 and 5 error): 30.72 8.59
* Epoch: [58/60]	 Top 1-err 30.990  Top 5-err 8.540	 Test Loss 1.155
69.01
0.6901
loss: 1.1549475269317626
(35, 0, 98) triplet: 0.14
(35, 0): 0.01
(35, 98): 0.15000000000000002
* Epoch: [59/60]	 Top 1-err 44.556  Top 5-err 19.450	 Train Loss 2.182
* Epoch: [59/60]	 Top 1-err 31.030  Top 5-err 8.480	 Test Loss 1.169
Current best accuracy (top-1 and 5 error): 30.72 8.59
* Epoch: [59/60]	 Top 1-err 31.030  Top 5-err 8.480	 Test Loss 1.169
68.97
0.6897
loss: 1.169398794555664
(35, 0, 98) triplet: 0.13999999999999999
(35, 0): 0.01
(35, 98): 0.15
Best accuracy (top-1 and 5 error): 30.72 8.59
=> loading checkpoint 'runs/cifar100_resnet_1/model_best.pth.tar'
* Epoch: [-1/60]	 Top 1-err 30.720  Top 5-err 8.590	 Test Loss 1.171
69.28
0.6928
loss: 1.171429383468628
(35, 0, 98) triplet: 0.15
(35, 0): 0.01
(35, 98): 0.16
